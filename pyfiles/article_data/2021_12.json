{
    "article_0": {
        "title": "Comment: 3D Touch was one of the best technologies Apple ever created, and I still miss it",
        "body": "Some people may not think of the iPhone 6s as a device that changed everything, but it did bring some important improvements such as the first 4K camera on an iPhone, Live Photos, and 3D Touch. While today\u2019s iPhones still record 4K video and capture Live Photos, none of them have 3D Touch anymore \u2013 which is unfortunate since this was one of the best technologies ever created by Apple.\nIn fact, the story of 3D Touch began a year before the introduction of the iPhone 6s, when Apple unveiled the first Apple Watch to the world in September 2014. Since the device was so small, the company invented a new way to access some hidden buttons on the interface through touches with more pressure.\nThe Apple Watch display had pressure sensors that were able to detect the intensity of touch in order to reproduce different actions. Apple called this technology \u201cForce Touch,\u201d which was later added to the 2015 MacBook to replace the mechanical trackpad button.\nIt was only a matter of time before Force Touch was added to the iPhone. In September 2015, the iPhone 6s was the first model to have a \u201c3D Touch\u201d display. Despite the different name, 3D Touch was essentially Force Touch \u2013 a display technology based on pressure sensors to enable even more interactions on the screen.\nWhat users could do with 3D Touch\n3D Touch on iPhone was mainly used to access quick actions on app icons when the user pressed them hard. Apple also created an API for a gesture called \u201cPeek & Pop,\u201d which allowed pressing your finger on a link or conversation to see a quick preview of that content. With an even harder press, the content was fully opened.\nA similar gesture also worked for notifications, phone numbers, and more. In addition to quick actions and Peek & Pop, 3D Touch also enabled pressure-sensitive controls for iOS games.\nThanks to the Taptic Engine, which was also introduced with the first Apple Watch and added to the iPhone 6s, all 3D Touch actions were immediately responded with quick vibrations called \u201cHaptic Feedback.\u201d Here\u2019s how Apple described the feature:\nThe original iPhone introduced the world to Multi-Touch, forever changing the way people experience technology. With 3D Touch, you can do things that were never possible before. It senses how deeply you press the display, letting you do all kinds of essential things more quickly and simply. And it gives you real-time feedback in the form of subtle taps from the all-new Taptic Engine.\nWhat happened to 3D Touch?\nIn 2018, when Apple introduced three new iPhone models, only the iPhone XS and iPhone XS Max had 3D Touch, while the cheaper iPhone XR model was equipped with Haptic Touch \u2013 which is a fancy way of saying \u201clong press with vibrations.\u201d The following year, all three iPhone 11 models were released without 3D Touch.\nIt\u2019s hard to name a single factor that led to the end of 3D Touch on the iPhone (and even Force Touch on the Apple Watch). Certainly the cost and complexity of producing displays with force pressure sensors was one of the deciding factors in putting an end to this technology, but there are a few other things to keep in mind.\nMost iPhone users I know (and I\u2019m not talking about tech geeks here) had no idea what 3D Touch was. Some of them only knew how to use a few gestures, while others had no idea that their iPhones had such a feature. And honestly, I blame Apple for that.\nThe company has never made it clear to users how to take advantage of 3D Touch. There was not a single screen during the iPhone setup to introduce the feature, so basically you had to figure out on your own how to use it. Guess what? Several people didn\u2019t figure it out how to use it.\nAs a consequence, developers became less interested in supporting 3D Touch, and even Apple stopped caring about the feature at a certain point when it realized that most users wouldn\u2019t mind having the feature replaced by long presses.\nI still want 3D Touch back\nI\u2019m not saying that it has become impossible to use my iPhone without 3D Touch, but I still miss it every time I play with my old iPhones that still have the force pressure sensors on the display.\nSure, a long press might do the same thing as 3D Touch to access quick actions in iOS, but that will never be as fast and precise as accessing them instantly just by pressing harder on the screen. And things like Peek & Pop and the actions to select text using pressure on the iPhone keyboard were extremely convenient.\nNot to mention the Apple Watch, which had its interface designed with Force Touch in mind and now the technology is gone. Haptic Touch might do the job, but it doesn\u2019t feel natural or like \u201csomething only Apple would do.\u201d\nUnfortunately, I really think 3D Touch is gone forever, but it was one of the best technologies Apple ever put into the iPhone. It felt so natural to press harder in some places and instantly feel the haptic responses. It was a brilliant idea, and it\u2019s a shame it had such a short life.\nFor now, Force Touch lives on Apple\u2019s trackpads, but we are not sure for how much longer.\nDo you miss 3D Touch? Let me know in the comments below.",
        "url": "https://9to5mac.com/2021/12/29/comment-3d-touch-was-one-of-the-best-technologies-apple-ever-created-and-i-still-miss-it/",
        "labels": {
            "software": 0.2507198750972748,
            "iphone": 0.20692478120326996,
            "mac computers": 0.17076759040355682,
            "airtag": 0.0896550714969635,
            "employees": 0.05434476211667061,
            "ipad": 0.04882608726620674,
            "apple watch": 0.047523461282253265,
            "airpods": 0.03303898870944977,
            "apple tv or apple music": 0.0278934258967638,
            "data privacy": 0.026614006608724594,
            "money or stock": 0.026603490114212036,
            "leaks or rumours": 0.017088495194911957
        }
    },
    "article_1": {
        "title": "Apple brings back mask requirement to all U.S. Apple Stores",
        "body": "If you buy through our links, we may get a commission. Read our ethics policy\nArticle Hero Image\nApple is reinstating a requirement for customers to wear masks when visiting an Apple Store in the United States, due to a rise in cases of COVID-19.\nApple was in the process of reducing its social distancing measures in its stores in November, having ended its mask requirement in approximately 100 stores on November 5 before rolling out to others. In the face of pandemic changes, it's reversing course and bring masks back.\nAccording to Bloomberg's Mark Gurman on Twitter, Apple is \"reinstating its mask mandate\" at all stores in the country. Before the rollback, the mandate had been dropped at roughly half of its outlets.\nThe return of the masks isn't the only change Apple is making, as it will also start limiting store occupancy at \"several locations.\" At the pandemic's height, Apple changed the design of some stores to enable pickup and drop-off of products, as well as only allowing limited numbers of customers enter stores at a time.\nThe decision was made in the face of the fast spread of the Omicron variant, which has resulted in a surge of infections in the U.S. and other countries.",
        "url": "https://appleinsider.com/articles/21/12/14/apple-brings-back-mask-requirement-to-all-us-apple-stores",
        "labels": {
            "airtag": 0.29583728313446045,
            "software": 0.11795835196971893,
            "money or stock": 0.10187530517578125,
            "ipad": 0.0777658075094223,
            "iphone": 0.07415396720170975,
            "employees": 0.07378218322992325,
            "data privacy": 0.05744572728872299,
            "apple watch": 0.05437391996383667,
            "leaks or rumours": 0.04637599363923073,
            "mac computers": 0.04603882133960724,
            "apple tv or apple music": 0.032331231981515884,
            "airpods": 0.022061409428715706
        }
    },
    "article_2": {
        "title": "Apple made secret 5-year $275B deal with Chinese government",
        "body": "If you buy through our links, we may get a commission. Read our ethics policy\nTim Cook speaking to China Vice Premier Sun Chunlan in March 2019.\nApple's success in China is allegedly down to an agreement with the country's government to help develop its economy, with a report claiming CEO Tim Cook personally lobbied officials to get the best deal for the company.\nApple considers China to be a major market, with the iPhone maker expending considerable effort to make headway, both in generating custom and in its extensive supply chain operations. The success in China has been a turnaround years in the making, and it seems that a secret agreement may have greased the wheels.\nA report on Tuesday claims that, during a period when Apple was dealing with a rash of regulatory activity in China, Tim Cook paid a visit to the country in 2016. During that visit, he signed an agreement with the Chinese government, according to The Information.\nThe deal would have Apple working to improve China's economy and technological profile with investments, training up its workforce, and various beneficial business deals. It is alleged that the total value of the five-year agreement was worth $275 billion.\nAt the time, Apple was viewed by Chinese officials as not doing enough to help the local economy, internal documents supposedly state. With regulatory scrutiny at an all-time high, Apple executives were having trouble trying to turn the Chinese opinion of the company around.\nOver multiple visits, Cook is said to have lobbied officials on behalf of Apple, as well as signing the deal with a Chinese government agency. The lobbying against various threats hat would've affected the App Store, Apple Pay, and other products was mostly successful, if unreported at the time.\nThe deal was kept secret both by the company's own culture and by the opaque workings of the Chinese government, and was politically wise according to political economist Victor Shih. It is thought that as Apple has to appease China as both a major market and a manufacturing base, it had to keep the government happy while also not appearing to other countries as appeasing China.\n\"Apple likely wanted to avoid the optics of groveling to the Chinese government,\" said Shih.\nThe key deal was a 1,250-word agreement created by Apple's government affairs, which it thought could improve its relations with the government and potentially let Apple get the ear of senior leaders. The memorandum of understanding with the National Development and Reform Commission was signed shortly after Cook announced a $1 billion investment in Didi Chuxing in May.\nAs part of the agreement, Apple pledged to work with Chinese manufacturers to create \"the most advanced manufacturing technologies\" and \"support the training of high-quality Chinese talents.\" This was accompanied by promises to increase its use of Chinese suppliers for device components, to work with Chinese software firms and invest in tech companies, and to work with Chinese universities on new technologies.\nApple also said it would invest \"many billions of dollars more\" than its existing annual spend in the country, towards R&D centers, renewable energy projects, and retail outlets.\nThe agreement was set to run for five years, including Apple's spending pledge of more than $275 billion over the period. However, the deal had the option to be extended for an extra year, to May 2022, if neither China nor Apple objected.\nApple would also agree to \"strictly abide by Chinese laws and regulations,\" a phrase that would occasionally resurface when Apple discussed privacy issues in China, such as its move of Chinese customer data in iCloud to a Chinese company.\nWhile Apple's tasks were outlined in detail, China had more ambiguous obligations, in that it would give Apple \"necessary support and assistance.\"\nTim Cook's lobbying with high-ranking officials and other areas of the Chinese government oddly caused more concern among Apple's China-based executives. Cook's famous ability to generate rapport with leaders led some Apple executives to believe the company could be vulnerable unless local managers could form their own relationships, according to one internal document.",
        "url": "https://appleinsider.com/articles/21/12/07/apple-made-secret-5-year-275b-deal-with-chinese-government?utm_medium=rss",
        "labels": {
            "money or stock": 0.5999488234519958,
            "leaks or rumours": 0.15715481340885162,
            "airtag": 0.05340944230556488,
            "iphone": 0.0484161451458931,
            "mac computers": 0.03115989826619625,
            "software": 0.02508092112839222,
            "apple watch": 0.02022659406065941,
            "ipad": 0.018170731142163277,
            "data privacy": 0.01594465598464012,
            "apple tv or apple music": 0.012817354872822762,
            "employees": 0.012383834458887577,
            "airpods": 0.005286757834255695
        }
    },
    "article_3": {
        "title": "Report: iOS Users Who Opt-Out of App Tracking Continue to Be Tracked by Facebook and Snapchat",
        "body": "\"Loose\" interpretations of Apple's privacy policies allow apps such as Facebook and Snapchat to continue tracking users for targeted advertising even when they have asked to not be tracked, The Financial Times reports.\nIn May, Apple launched its App Tracking Transparency feature that allows users to opt-out of being tracked across apps and websites for advertising purposes. Seven months after Apple introduced the feature, companies such as Snapchat and Facebook have purportedly been allowed to continue sharing user-level signals from iPhones, providing that data is anonymized and aggregated rather than directly linked to specific user profiles.\nThe Financial Times said that Apple's position was the result of \"an unacknowledged shift that lets companies follow a much looser interpretation of its controversial privacy policy.\" Apple has instructed developers that they \"may not derive data from a device for the purpose of uniquely identifying it,\" which developers have interpreted to mean that they can still observe \"signals\" and behaviors from groups of users instead, enabling these groups to be shown tailored ads anyway.\nApple has not explicitly endorsed these techniques, but they allow third parties to track and analyze groups of users regardless of whether or not they have given consent to user-level tracking. In addition, Apple reportedly continues to trust apps to collect user-level data such as IP address, location, language, device, and screen size, even though some of this information is passed onto advertisers.\nSnapchat investors were told that the company plans to share data from its 306 million users, including those who ask the app \"not to track,\" with advertisers so that they can gain \"a more complete, real-time view\" of the success of ad campaigns. Likewise, Facebook is undertaking a \"multiyear effort\" to rebuild ad infrastructure \"using more aggregated or anonymized data,\" according to the company's operations chief.\nIn June, Apple faced pressure to tighten the rules around App Tracking Transparency after it was found that third parties were using workarounds to identify users who do not consent to be tracked, but there have been no changes around looser \"probabilistic\" methods of user identification.",
        "url": "https://www.macrumors.com/2021/12/08/users-continue-to-be-tracked-by-facebook/",
        "labels": {
            "data privacy": 0.3002786636352539,
            "iphone": 0.22151176631450653,
            "software": 0.14190998673439026,
            "ipad": 0.1031021773815155,
            "airtag": 0.09424787014722824,
            "leaks or rumours": 0.06200723350048065,
            "apple watch": 0.021764475852251053,
            "airpods": 0.018680617213249207,
            "apple tv or apple music": 0.014440749771893024,
            "money or stock": 0.009214901365339756,
            "mac computers": 0.008738544769585133,
            "employees": 0.004102933686226606
        }
    },
    "article_4": {
        "title": "Apple closes all its retail stores in New York City amidst COVID-19 spread",
        "body": "With the number of COVID-19 cases increasing around the world due to the Omicron variant, some companies are once again closing their retail stores. Now Apple has decided to close all of its stores in New York City after having temporarily shut down other stores across the US and Canada.\nAs first noted by Bloomberg\u2019s Mark Gurman, all Apple Stores in New York City were partially closed on Monday. The stores will remain open for in-store pickup of online orders, but customers can no longer visit the locations.\nIn addition to the stores located in New York City, 9to5Mac was able to confirm other Apple Stores in New York State that have also been closed. You can check out the closed retail stores in the list below:\nLast week, Apple had closed seven more stores in places like Atlanta, Houston, and New Hampshire due to the spread of COVID-19 among employees. Earlier this month, other Apple Stores were closed in Miami, Maryland, and Ottawa because of the pandemic. The company usually shuts down a store when 10% of the staff members test positive for COVID-19.\nApple has also reinstated the mandate for mask-wearing at all US retail stores. The company says it is \u201cregularly monitoring [COVID-19] conditions\u201d to determine which stores should be closed in order to \u201csupport the well-being of customers and employees.\u201d\nMore stores are expected to be temporarily shut down at any time.",
        "url": "https://9to5mac.com/2021/12/27/apple-closes-all-its-retail-stores-in-new-york-city-amidst-covid-19-spread/",
        "labels": {
            "leaks or rumours": 0.47201961278915405,
            "airtag": 0.10842420905828476,
            "iphone": 0.09444905817508698,
            "money or stock": 0.08523015677928925,
            "ipad": 0.04786701127886772,
            "mac computers": 0.03830559179186821,
            "data privacy": 0.03223316743969917,
            "apple watch": 0.030936772003769875,
            "employees": 0.02521798200905323,
            "software": 0.023320898413658142,
            "apple tv or apple music": 0.022411353886127472,
            "airpods": 0.01958424411714077
        }
    },
    "article_5": {
        "title": "Apple is rebuilding Apple Music as a full native app with macOS 12.2 beta",
        "body": "Apple on Thursday released the first beta of macOS Monterey 12.2 just a few days after the release of macOS 12.1 to all users. While the company didn\u2019t provide any release notes for today\u2019s update, it seems that Apple is finally rebuilding the Apple Music app as a full native macOS app.\nBack in 2019, when Apple introduced macOS Catalina, the well-known iTunes was replaced by the Music app to better reflect the company\u2019s strategy on iOS and tvOS. However, although under a new name, the Music app on macOS retained the iTunes backend, which was basically a bunch of web content loaded into an app.\nWhile this works for most users, having web content within apps makes the experience less fluid. Luckily Apple is finally changing this with macOS Monterey 12.2 beta, which includes some big changes to the Music app backend.\nAs first noted by Luming Yin on Twitter, Apple Music in macOS 12.2 beta now uses AppKit \u2013 which is macOS\u2019 native interface framework. 9to5Mac was able to confirm based on macOS code that the Music app is now using JET, which is a technology created by Apple to turn web content into native apps.\nSome parts of the Music app were already native, such as the music library. But now Mac users will notice that searching for new songs in Apple Music is much faster as the results pages are displayed with a native interface instead of as a webpage. Scrolling between elements has also become smoother with the beta app, and trackpad gestures are now more responsive.\nComparing both apps side by side, the beta Music app may look simpler with some interface effects missing \u2013 this is probably because Apple is rebuilding everything and it will take a while before the native version gets all these visual effects back.\nYin mentioned that the Apple TV app has also been rebuilt with a native backend. While this is indeed true, 9to5Mac found out that Apple had already updated the TV app with JET technology in macOS Monterey 12.1, which is available for everyone. Of course, more refinements are expected for both apps in the upcoming macOS 12.2 betas.\nOn a related note, macOS Monterey code suggests that Apple is still working on Universal Control, but the feature remains unavailable to users. The company has officially delayed the release of Universal Control until spring 2022.",
        "url": "https://9to5mac.com/2021/12/16/apple-is-rebuilding-apple-music-as-a-full-native-app-with-macos-12-2-beta/",
        "labels": {
            "software": 0.6326963305473328,
            "mac computers": 0.13361835479736328,
            "apple tv or apple music": 0.10577565431594849,
            "airtag": 0.028217798098921776,
            "leaks or rumours": 0.023226633667945862,
            "iphone": 0.012962388806045055,
            "employees": 0.012913402169942856,
            "data privacy": 0.012364648282527924,
            "apple watch": 0.011440626345574856,
            "ipad": 0.010168574750423431,
            "airpods": 0.010021358728408813,
            "money or stock": 0.006594353821128607
        }
    },
    "article_6": {
        "title": "Microsoft quietly told Apple it was willing to turn big Xbox-exclusive games into iPhone apps",
        "body": "Remember when Apple pretended like it would let cloud gaming services like Microsoft xCloud and Google Stadia into the App Store, while effectively tearing their business models to shreds? Know how Microsoft replied that forcing gamers to download hundreds of individual apps to play a catalog of cloud games would be a bad experience?\nIn reality, Microsoft was willing to play along with many of Apple\u2019s demands \u2014 and it even offered to bring triple-A, Xbox-exclusive games to iPhone to help sweeten the deal. That\u2019s according to a new set of private emails that The Verge unearthed in the aftermath of the Epic v. Apple trial.\nThese games would have run on Microsoft\u2019s Xbox Cloud Gaming (xCloud) platform, streaming from remote server farms filled with Xbox One and Xbox Series X processors instead of relying on the local processing power of your phone. If the deal had been made, you could have theoretically bought a copy of a game like Halo Infinite in Apple\u2019s App Store itself and launched it like any other app \u2014 instead of having to pay $14.99 a month for an Xbox Game Pass Ultimate subscription with a set catalog of games and then needing to use Microsoft\u2019s web-based App Store workaround.\nBut primarily, Microsoft was negotiating to bring its Netflix-esque catalog of xCloud games to the App Store, at a time when Apple had gotten very touchy about cloud gaming in general.\nThe emails, between Microsoft Xbox head of business development Lori Wright and several key members of Apple\u2019s App Store teams, show that Microsoft did start with a wide array of concerns about stuffing an entire service worth of Xbox games into individual App Store apps as of February 2020. Wright mentioned the \u201cComplexity & management of creating hundreds to thousands of apps,\u201d how they\u2019d have to update every one of those apps to fix any bugs, and how all those app icons could lead to cluttered iOS homescreens, among other worries.\n\u201cWe believe that the issues described here will create frustration and confusion for customers, resulting in a sub-par experience on Apple devices relative to the equivalent experience on all other platforms,\u201d she wrote.\nBut by March of last year, Microsoft was proposing that it could, actually, create those hundreds or thousands of individual apps to submit to the App Store \u2014 as long as it could make those apps a bit more like shortcuts, instead of stuffing the whole cloud gaming streaming stack into each one. She argued that\u2019s similar to how watchOS apps already worked.\n\u201cIf we have a single streaming tech app, it will be around 150 MB, but the other apps will only be roughly 30 MB and will not need to be updated when the streaming tech is updated. This will be a better experience for users,\u201d Wright wrote:\nWright explains that Microsoft is \u201cclose to finding a solve\u201d to bring Xbox games to iOS as App Store apps. Screenshot by Sean Hollister / The Verge\nThat\u2019s also when Wright pulled out the idea of bringing exclusive triple-A Xbox games to iOS, arguing that they, too, would need \u201cthe streaming tech package as a separate app to deliver the right experience.\u201d\n\u201cThis would be an incredibly exciting opportunity for iOS users to get access to these exclusive AAA titles in addition to the Game Pass games,\u201d she wrote.\nObviously, none of this happened. Microsoft rejected Apple\u2019s new App Store guidelines in September 2020 and announced the web workaround version of xCloud a month later. It arrived this April.\nWhere did negotiations break down? Microsoft now tells The Verge that Apple was actually the one that rejected its proposals \u2014 because Apple insisted on forcing each and every game to include the full streaming stack and wouldn\u2019t agree to anything else.\n\u201cForcing each game to include our streaming tech stack proved to be unrealistic\u201d\n\u201cOur proposal for bringing games through individual apps was designed to comply with App Store policies. It was denied by Apple based on our request that there be a single streaming tech app to support the individual game apps, as the initial email states. Forcing each game to include our streaming tech stack proved to be unrealistic from a support and engineering perspective and would create an incredibly negative experience for customers,\u201d reads a statement from Xbox Cloud Gaming CVP Kareem Choudhry to The Verge.\nLate last April, Apple\u2019s App Store games manager Mark Grimm suggested that might not be the only reason the companies didn\u2019t reach an understanding \u2014 it might have something to do with money as well. He told colleagues that Microsoft was actually now entertaining the idea of including the streaming code in individual Xbox games on the App Store. \u201c[Wright] was far more positive and was trying to pressure her engineering team into finding a way to put the entire streaming stack into each binary,\u201d he wrote.\nGrimm goes on to suggest that Apple\u2019s agreed role in negotiations was to \u201ctake the IAP problem back and figure something out,\u201d but it sounds like that didn\u2019t happen. Screenshot by Sean Hollister / The Verge\nBut there was also a concern that Microsoft didn\u2019t want to put in-app purchases into each game: \u201cTheir proposal for IAPs is still that they process all IAPs on their existing system and settle up with us (either in real-time or monthly),\u201d wrote Grimm, stating the opinion that Apple should let Microsoft\u2019s games bypass IAP. \u201cThey\u2019re not trying to circumvent paying us, they\u2019re trying to circumvent a large amount of redundant API work,\u201d he said.\n\u201cThey\u2019re not trying to circumvent paying us\u201d\nAnd Apple tells The Verge that money was indeed involved. \u201cUnfortunately, Microsoft proposed a version of xCloud that was not compliant with our App Store Review Guidelines, specifically the requirement to use in-app purchase to unlock additional features or functionality within an app,\u201d reads a statement via Apple spokesperson Adam Dema.\nMicrosoft\u2019s Choudhry denies that IAP came into the final decision. \u201cThe reasons for rejection were unrelated to in-app purchase capabilities; we currently provide Xbox Cloud Gaming through a singular Xbox Game Pass app in the Google Play Store without IAP enabled, for example, and we would do the same through the App Store if allowed.\u201d\nHere\u2019s Choudhry\u2019s general statement on the entire matter:\nWe explored many options to bring Cloud Gaming via Xbox Game Pass to Apple devices, always in ways that led with the customer experience first, which we believed was best through a singular app. Apple\u2019s Store policies would have forced us to launch each game as an individual app\u2014while we never favored that approach, we explored it as a possibility in the spirit of finding any solution to bring Cloud Gaming to iOS customers. However between that email in March 2020 and our statement to The Verge in September 2020, Apple rejected our proposals and we were left without the ability to release a cohesive Xbox Game Pass offering through the App Store. We shifted our engineering priorities and have now moved to a browser-based solution making Xbox Cloud Gaming available to iOS customers through web browsers, and will continue to look for viable resolutions that allow us into the App Store.\nAs for the possibility of bringing triple-A Xbox exclusives to the iPhone and iPad as individual games, he confirms:",
        "url": "https://www.theverge.com/2021/12/9/22826297/microsoft-xbox-xcloud-streaming-exclusives-iphone-ipad-gamepas",
        "labels": {
            "software": 0.39870285987854004,
            "iphone": 0.2790984511375427,
            "leaks or rumours": 0.153897225856781,
            "ipad": 0.07465150952339172,
            "employees": 0.02197292633354664,
            "airtag": 0.01757037825882435,
            "mac computers": 0.01571604423224926,
            "money or stock": 0.01365491934120655,
            "data privacy": 0.01055960450321436,
            "airpods": 0.005565421190112829,
            "apple watch": 0.004680910147726536,
            "apple tv or apple music": 0.0039296881295740604
        }
    },
    "article_7": {
        "title": "Apple Delays Corporate Return to Offices Indefinitely",
        "body": "Apple employees are no longer going to be returning to corporate offices in February as planned due to the continued spread of COVID-19 and the newly emerging omicron variant, Apple CEO Tim Cook said in a memo sent out today.\nBack in November, Apple sent out a letter telling employees that Apple would expect them to start returning to the office on February 1, but that return date has now been pushed back indefinitely.\nBREAKING: Tim Cook just sent out an email delaying Apple\u2019s return to work to a date \u201cyet to be determined.\u201d He also said the company is giving every corporate employee $1,000 to spend on home office equipment. \u2014 Zo\u00eb Schiffer (@ZoeSchiffer) December 15, 2021\nThere is no word on when employees will be expected to go back to work, and for now, those who are able to do so will continue to work from home.\nThe delay will be welcome news to Apple employees who have been dreading the return to corporate offices, but Apple does plan to have employees come back at some point. Cook said that the return date is \"yet to be determined\" at this time.\nApple executives have made it clear since the beginning of the pandemic that employees will eventually need to return work. \"Video conference calling has narrowed the distance between us, to be sure, but there are things it simply cannot replicate,\" Cook said back in June.\nWhen it is safe for employees to return to the office, Apple is planning for a hybrid work schedule. Employees will be expected to be in the office three days a week, but will have the option of working from home for two days a week. Apple also plans to allow employees to work remotely for up to one month per year, giving them more time to travel and be closer to loved ones.\nBecause employees will need to continue to work from home, Cook said that Apple is giving every corporate employee $1,000 to spend on home office equipment.",
        "url": "https://www.macrumors.com/2021/12/15/apple-delays-corporate-return-to-offices/",
        "labels": {
            "software": 0.34733861684799194,
            "employees": 0.13037161529064178,
            "airtag": 0.12496653199195862,
            "iphone": 0.1011202484369278,
            "mac computers": 0.0779711902141571,
            "money or stock": 0.07526189833879471,
            "ipad": 0.042626455426216125,
            "airpods": 0.0281401127576828,
            "leaks or rumours": 0.02375168167054653,
            "apple watch": 0.02179683931171894,
            "apple tv or apple music": 0.01402923185378313,
            "data privacy": 0.0126256188377738
        }
    },
    "article_8": {
        "title": "Apple\u2019s Frontline Employees Are Struggling To Survive",
        "body": "Nearly every shift at an Apple Store starts the same way: an employee picks a line from the Apple credo and talks about how it applies to that day\u2019s work: \u201cWe are here to enrich lives. To help dreamers become doers, to help passion expand human potential, to do the best work of our lives. At our best, we give more than we take.\u201d\nWorkers say it\u2019s easy to tell who wants to get promoted because they\u2019ll pick the line \u201cTurning dreamers into doers\u201d and discuss how Apple\u2019s devices can help customers do that. Not every retail worker buys into it, though. \u201cPeople get emotional,\u201d says a current retail employee in Pennsylvania. \u201cYou see people go really deep in that stuff when they want to get promoted. It\u2019s all just pandering.\u201d\nPart of the issue is pay. Apple\u2019s retail employees make on average between $19 and $25 an hour in the United States, according to Glassdoor. That\u2019s good for the retail industry, but can be grating for employees who want to build a career at the tech giant. Some say that after staying at the company for six years, they\u2019re making less than $21 an hour.\nThen there\u2019s the issue of how retail workers are evaluated. In the store, Apple uses something called a \u201cnet promoter score\u201d to see how stores are performing. After a customer leaves, they\u2019ll sometimes receive a survey asking them to evaluate the employee who helped them as well as the overall store experience.\nEmployees spent eight hours a day fielding inquiries from angry customers\nLow scores can often be about factors outside the employees\u2019 control \u2014 things like low inventory or wait time. But they still reflect poorly on staff. \u201cThere\u2019s never positive intent assumed,\u201d says a current employee in Pittsburgh. \u201cIt always feels like you\u2019re a kid getting in trouble and you\u2019re making an excuse.\u201d The surveys are meant to give customers a consistent way to say whether the stores are meeting Apple\u2019s standards \u2014 but they systematically place the customer over the employee, by design.\nThis can be particularly frustrating when the issue stems from Apple corporate. In 2017, Apple customers with older iPhones realized that if they replaced their phone batteries, the performance would significantly improve. This quickly became one of Apple\u2019s largest customer experience scandals, as people realized the company had intentionally slowed down older iPhone models to (supposedly) preserve their battery life.\nTo quell public outrage, the company said it would replace phone batteries, free of charge, for a year. To meet rising customer demand, Apple told some retail workers to try to complete battery swaps in under 10 minutes, according to a former employee. The result was a disaster for workers, who say they didn\u2019t have the supplies or resources necessary to meet the mandate. \u201cYou can\u2019t replace a battery in 10 minutes,\u201d a former retail manager says bluntly. \u201cNothing translates from corporate to the stores because they\u2019re not in the stores.\u201d\nSet against this backdrop, the Apple credo has become increasingly important. It\u2019s meant to inspire employees to provide exemplary customer service \u2014 to elevate even mundane parts of the job to stratospheric levels of importance.\nIn the past, some employees assumed that it applied to how Apple treated its workforce. Wasn\u2019t the company supposed to enrich the lives of its employees? To give more than it took?\nThat started to break down on March 14th, 2020, in the early days of the pandemic, when Apple announced it was temporarily closing all its stores, sending retail employees home. In Pennsylvania, one current worker was selected to be part of a new program to answer calls from customers who needed tech support over the phone. The employee was relieved \u2014 his wife was immunocompromised, and he himself suffered from asthma, making the pair particularly vulnerable to COVID-19. Being able to work from home seemed like a perfect solution to stay safe and continue earning money.\nFrom the start, the job was stressful. Employees spent eight hours a day fielding inquiries from angry customers. They were evaluated based on call time and customer satisfaction. As with many hourly roles at Apple, people with high scores knew they\u2019d eventually get better schedules, promotions, and opportunities. People with low scores could be placed on action plans to try to improve.\nThe Pennsylvania employee says he didn\u2019t hear much from his manager until July 2020, when he was told that his scores were good. So good, in fact, that he was going to start taking tier two calls. If a lower-level adviser couldn\u2019t answer a customer\u2019s questions, they could escalate the call to him. He asked if the job came with a pay bump and was told it did not.\nApple tried to make up for the increased workload for hourly employees by sending employees in the work from home program a shirt as a gift for all their hard work. When it arrived, employees realized it had a large 14 on the back (for iOS 14) and 2020 printed on the sleeve. These were leftovers from WWDC 2020 \u2014 Apple\u2019s live event that had been canceled.\nThe shirt felt like a slap in the face. Employees wanted a raise. Some had multiple roommates and worked two or three jobs to try to make ends meet. On top of that, they were dealing with the same existential angst as the rest of the population weathering a global pandemic \u2014 all while being yelled at on the phone by customers who barely treated them like people.\n\u201cIt starts to get into a game of fixing the numbers more than helping the customers.\u201d\nThe Pennsylvania employee said his mental health started to suffer. He was working in a poorly lit room, unable to leave the house because of the lockdown. When he tried to talk to his manager about how he was feeling, he was told to try opening a window or put a plant on his desk.\nOne day, the employee was doing a screen share with a customer who was having issues with her display. The device wasn\u2019t under warranty \u2014 he told her it would likely cost $500 to fix. The woman started to cry. \u201cI\u2019m a college student, can\u2019t you make an exception?\u201d she asked. He said he was sorry, but it wasn\u2019t up to him. Then the woman opened Photo Booth on her computer screen \u2014 activating the webcam \u2014 and held a razor to her wrist. \u201cThis is what the stress you\u2019re giving me is doing to me,\u201d he says she told him.\nWhen the employee explained to his boss what had happened, the man asked if he needed half an hour to decompress. The employee responded that it didn\u2019t feel like enough time. According to his recollection, his boss conceded: \u201cOkay, take a breather, but try to keep it to 30 minutes because we need to keep our call time down.\u201d\nIn early 2021, Apple Stores started to reopen, and the employee asked to go back to in-person work. \u201cI have asthma, my wife has a chronic illness, but we were forced to wager my mental health against my physical health to see if it was worth going back to the store,\u201d he says. The decision wasn\u2019t his anyway \u2014 he was told he needed to keep working from home.\nIt wasn\u2019t until he asked to go on medical leave that he was finally able to go back to the store. \u201cThey let me come back to the store to keep me off leave,\u201d he alleges.\nIn September, Apple announced that all retail and care employees who\u2019d been with the company since March 31st would receive a $1,000 bonus. To some workers, it was a nice surprise; but after years of feeling mistreated, others believe the bonus had more sinister motivations. \u201cI think it feels more like they don\u2019t want to get sued for not offering hazard pay after making some of us work in public in the last year,\u201d the Pennsylvania employee says.",
        "url": "https://www.theverge.com/c/22807871/apple-frontline-employees-retail-customer-service-pandemic",
        "labels": {
            "employees": 0.9078750610351562,
            "money or stock": 0.015910761430859566,
            "airtag": 0.015574391931295395,
            "iphone": 0.015067444182932377,
            "software": 0.010678459890186787,
            "apple watch": 0.008914039470255375,
            "mac computers": 0.007634962443262339,
            "ipad": 0.006206179969012737,
            "apple tv or apple music": 0.005186614114791155,
            "leaks or rumours": 0.002773512154817581,
            "airpods": 0.0023304768837988377,
            "data privacy": 0.0018481024308130145
        }
    },
    "article_9": {
        "title": "Apple ditched Intel, and it paid off",
        "body": "In this article INTC\nAAPL Follow your favorite stocks CREATE FREE ACCOUNT\nApple CEO Tim Cook Brooks Kraft/Apple Inc/Handout via Reuters\nApple 's decision to ditch Intel paid off this year. The pivot allowed Apple to completely rethink the Mac, which had started to grow stale with an aging design and iterative annual upgrades. Following the divorce from Intel, Apple has launched far more exciting computers which, paired with an ongoing pandemic that has forced people to work and learn from home, have sent Apple's Mac business soaring. related investing news Wall Street is bullish on Big Tech heading into earnings \u2014 here's what we think It wasn't always a given. When Apple announced its move away from Intel in 2020, it was fair to question just how well Apple could power laptops and desktop computers. Apple has used in-house chips for iPhones and iPads but had been selling Intel-powered computers for 15 years. It wasn't clear how well its macOS desktop software would work with apps designed to run on Intel chips, or whether its processors would offer any consumer benefits and keep up with intensive tasks that people turned to MacBooks to run. Those fears were quickly quelled. The first M1 Apple chip was launched in 2020 in a MacBook Air laptop. It was more powerful than Intel's chip while offering longer battery life and enabling a fanless design, which helped keep Apple's new MacBook Air even quieter. It proved to be an early success. In April 2021, CEO Tim Cook said during the company's fiscal second-quarter earnings call that the M1 chip helped fuel the 70.1% growth in Apple's Mac revenue, which hit $9.1 billion during that quarter.\nApple launches new iMac. Source: Apple Inc.\nThe growth continued in fiscal Q3, when Mac revenue was up 16% year over year. That quarter, it launched the all-new iMac, which offered a redesigned super-thin metal body that looks like a screen propped up on a stand. It's slimmer than the Intel models that came before it, while offering other benefits, like a much better webcam, great speakers and a much sharper display than the models it replaced. And Apple made the launch more exciting by offering an array of colors for the iMac, which it hadn't done since it shipped the 1999 iMac. There was a slowdown in fiscal Q4, when Mac revenue grew just 1.6%, as Apple, like all manufacturers, saw a slowdown from the burst of sales driven by the start of the pandemic and dealt with supply chain woes. But fiscal Q4 sales didn't include revenue from its most exciting new computer of the year.\nApple MacBook Pro Source: Apple\nThe 14-inch MacBook Pro, which launched in October, proved Apple's processors are capable of serving Apple's most demanding customers. The new laptop runs on Apple's latest M1 Pro and M1 Max processors, which are even more capable of handling intensive tasks such as video editing multiple high-resolution video files at the same time. It has the best screen ever on an Apple laptop that's brighter, sharper and smoother than earlier Intel models. But, more notably, the laptops still offer long battery life, thanks to the more power-efficient design of Apple's processors. Paired with plenty of ports that let you attach high-speed accessories, additional displays or more storage, it's easily the best laptop on the market. Apple's fiscal Q1 earnings in January will give an indication of how well all its new computers are selling. But it's clear the move from Intel has allowed Apple to move full speed ahead with its own chip development, much like it does for iPhones and iPads, the latter of which has yet to be matched by any other tablet on the market. It's no longer beholden to delays that plagued Intel, which started to lag behind AMD with its new 7nm chips. And Apple has full control over its \"stack,\" which means it can design new computer hardware and software together, instead of letting the power of another company's chips dictate what its computers can and can't do. Take, for example, the way M1 is able to improve the quality of the webcam on its computers, even though it's the same image sensor used in prior Intel models. There are other benefits to owning the hardware and software. Apple's new macOS Monterey software will soon let computers running on M1 share a single keyboard and mouse with an iPad, for example, allowing you to seamlessly control multiple devices at the same time. M1 Macs have other exclusive features, such as on-device voice dictation, improved Siri text-to-speech, more detailed Maps and portrait mode in FaceTime, which blurs the background behind you. And while the app selection is still limited, M1 Macs are capable of running some of the same apps that you run on your iPhone and iPad. Expect the trend to continue next year. Reports have suggested we'll see yet another new MacBook Air, updates to the most powerful iMac Pro and Mac Pro and more in 2022. And we may see M1 in totally new product categories. The chip is so good at managing power that it may even be used in Apple's augmented reality headset. A report from top Apple analyst Ming-Chi Kuo in November said Apple's upcoming AR glasses, which he said will launch at the end of 2022, will be just as powerful as its Macs. Kuo said that will help the headset stand out against the competition since the headset will be able to run intensive graphics tasks without being connected to a computer or phone.",
        "url": "https://www.cnbc.com/2021/12/29/apple-ditched-intel-and-it-paid-off.html",
        "labels": {
            "money or stock": 0.5043341517448425,
            "mac computers": 0.137473002076149,
            "iphone": 0.11839915812015533,
            "airtag": 0.06928232312202454,
            "ipad": 0.04447689652442932,
            "software": 0.03369547799229622,
            "apple watch": 0.03024013340473175,
            "apple tv or apple music": 0.01980886049568653,
            "data privacy": 0.013532107695937157,
            "employees": 0.011886202730238438,
            "airpods": 0.011245174333453178,
            "leaks or rumours": 0.005626457277685404
        }
    },
    "article_10": {
        "title": "The Popular Family Safety App Life360 Is Selling Precise Location Data on Its Tens of Millions of Users",
        "body": "The app is a major source of raw location data for a multibillion-dollar industry that buys, packages, and sells people\u2019s movements By Jon Keegan and Alfred Ng\nUPDATE: Life360 announced that it will stop sales of precise location data to the dozen or so data brokers it had been working with, and will now sell only precise location data to Arity and \u201caggregated\u201d location data to PlacerAI.\nLife360, a popular family safety app used by 33 million people worldwide, has been marketed as a great way for parents to track their children\u2019s movements using their cellphones. The Markup has learned, however, that the app is selling data on kids\u2019 and families\u2019 whereabouts to approximately a dozen data brokers who have sold data to virtually anyone who wants to buy it.\nThrough interviews with two former employees of the company, along with two individuals who formerly worked at location data brokers Cuebiq and X-Mode, The Markup discovered that the app acts as a firehose of data for a controversial industry that has operated in the shadows with few safeguards to prevent the misuse of this sensitive information. The former employees spoke with The Markup on the condition that we not use their names, as they are all still employed in the data industry. They said they agreed to talk because of concerns with the location data industry\u2019s security and privacy and a desire to shed more light on the opaque location data economy. All of them described Life360 as one of the largest sources of data for the industry.\n\u201cWe have no means to confirm or deny the accuracy\u201d of whether Life360 is among the largest sources of data for the industry, Life360 founder and CEO Chris Hulls said in an emailed response to questions from The Markup. \u201cWe see data as an important part of our business model that allows us to keep the core Life360 services free for the majority of our users, including features that have improved driver safety and saved numerous lives.\u201d\nA former X-Mode engineer said the raw location data the company received from Life360 was among X-Mode\u2019s most valuable offerings due to the sheer volume and precision of the data. A former Cuebiq employee joked that the company wouldn\u2019t be able to run its marketing campaigns without Life360\u2019s constant flow of location data.\nThe Markup was able to confirm with a former Life360 employee and a former employee of X-Mode that X-Mode\u2014in addition to Cuebiq and Allstate\u2019s Arity, which the company discloses in its privacy policy\u2014is among the companies that Life360 sells data to. The former Life360 employee also told us Safegraph was among the buyers, which was confirmed by an email from a Life360 executive that was viewed by The Markup. There are potentially more companies that benefit from Life360\u2019s data based on those partners\u2019 customers.\nHulls declined to disclose a full list of Life360\u2019s data customers and declined to confirm that Safegraph is among them, citing confidentiality clauses, which he said are in the majority of its business contracts. Data partners are only publicly disclosed when partners request transparency or there\u2019s \u201ca particular reason to do so,\u201d Hulls said. He did confirm that X-Mode buys data from Life360 and that it is one of \u201capproximately one dozen data partners.\u201d Hulls added that the company would be supportive of legislation that would require public disclosure of such partners.\nX-Mode, SafeGraph, and Cuebiq are known location data companies that supply data and insights gleaned from that data to other industry players, as well as customers like hedge funds or firms that deal in targeted advertising.\nCuebiq spokesperson Bill Daddi said in an email that the company doesn\u2019t sell raw location data but provides access to an aggregated set of data through its \u201cWorkbench\u201d tool to customers including the Centers for Disease Control and Prevention. Cuebiq, which receives raw location data from Life360, has publicly disclosed its partnership with the CDC to track \u201cmobility trends\u201d related to the COVID-19 pandemic.\n\u201cThe CDC only exports aggregate, privacy-safe analytics for research purposes, which completely anonymizes any individual user data,\u201d Daddi said. \u201cCuebiq does not sell data to law enforcement agencies or provide raw data feeds to government partners (unlike others, such as X-Mode and SafeGraph).\u201d\nX-Mode has sold location data to the U.S. Department of Defense, and SafeGraph has sold location data to the CDC, according to public records.\nX-Mode and SafeGraph didn\u2019t respond to requests for comment.\nThe Life360 CEO said that the company implemented a policy to prohibit the selling or marketing of Life360\u2019s data to any government agencies to be used for a law enforcement purpose in 2020, though the company has been selling data since at least 2016.\n\u201cFrom a philosophical standpoint, we do not believe it is appropriate for government agencies to attempt to obtain data in the commercial market as a way to bypass an individual\u2019s right to due process,\u201d Hulls said.\nFamilies would probably not like the slogan, \u2018You can watch where your kids are, and so can anyone who buys this information,\u2019 Justin Sherman, Duke Tech Policy Lab fellow\nThe policy also applies to any companies that Life360\u2019s customers share data with, he said. Hulls said the company maintains \u201can open and ongoing dialogue\u201d with its customers to ensure they comply with the policy, though he acknowledged that it was a challenge to monitor partners\u2019 activities.\nLife360 discloses in the fine print of its privacy policy that it sells the data it gleans from app users, but Justin Sherman, a cyber policy fellow at the Duke Tech Policy Lab, said people are probably not aware of how far their data can travel.\nThe company\u2019s privacy policy notes Life360 \u201cmay also share your information with third parties in a form that does not reasonably identify you directly. These third parties may use the de-identified information for any purpose.\u201d\n\u201cFamilies probably would not like the slogan, \u2018You can watch where your kids are, and so can anyone who buys this information,\u2019 \u201d Sherman said.\nTwo former Life360 employees also told The Markup that the company, while it states it anonymizes the data it sells, fails to take necessary precautions to ensure that location histories cannot be traced back to individuals. They said that while the company removed the most obvious identifying user information, it did not make efforts to \u201cfuzz,\u201d \u201chash,\u201d aggregate, or reduce the precision of the location data to preserve privacy.\nHulls said that all of Life360\u2019s contracts prohibit its customers from re-identifying individual users, along with other privacy and safety protective practices. He said that Life360 follows \u201cindustry best practices\u201d for privacy and that only certain customers like Cuebiq receive raw location data. The former X-Mode engineer said that the company also received raw data from Life360. The company relies on its customers to obfuscate that data based on their specific applications, Hulls added.\n\u21a9\ufe0e link Do you work at Life360, X-Mode or any other company that buys or sells location data? We\u2019d like to speak with you. You can reach out securely on Signal at 646-355-8306 or email keegan@themarkup.org. Read more\n\u201cSome of our data partners receive hashed data and some do not based on how the data will be used,\u201d the Life360 founder said.\nMeanwhile, selling location data has become more and more central to the company\u2019s health as it\u2019s struggled to achieve profitability. In 2016, the company made $693,000 from selling data it collected. In 2020, the company made $16 million\u2014nearly 20 percent of its revenue that year\u2014from selling location data, plus an additional $6 million from its partnership with Arity.\nWhile still reporting a loss of $16.3 million last year, the company is expanding its business to include other \u201cdigital safety\u201d products, rolling out data breach alerts, credit monitoring, and identity-theft-protection features. Publicly traded on the Australian Securities Exchange with plans to go public in the U.S., Life360 has also acquired companies that expand its tracking\u2014and potentially its data-gathering capacity. In 2019, the company purchased ZenScreen, a family screen-time monitoring app. And in April, it purchased the wearable location device company Jiobit, aimed at tracking younger children, pets, and seniors, for $37 million. Hulls said Life360 has no plans to sell data from Jiobit devices or its digital safety services.\nOn Nov. 22, Life360 also announced plans to buy Tile, a tracking device company that helps find lost items. Hulls said the company doesn\u2019t have plans to sell data from Tile devices.\n\u201cI\u2019m sure there are lots of families who do find very real comfort in an application like this, and that\u2019s valid,\u201d Sherman said. \u201cThat doesn\u2019t mean that there aren\u2019t ways that other people are harmed with this data. It also doesn\u2019t mean that the family couldn\u2019t be harmed with the data in ways that they\u2019re not aware of, such as that location data being used to target ads [or] used by insurance companies to figure out where they\u2019re traveling and increase their rates.\u201d\nHulls said that Life360 doesn\u2019t share users\u2019 private information with insurers in ways that could affect insurance rates.\n\u21a9\ufe0e link The Data Pipeline\nLife360\u2019s app allows the user to see the precise, real-time location of friends or family members, including the speed at which they are driving and the battery level on their devices.\nMarketed as a safety app, Life360 is popular among parents who want to track and supervise their kids from afar. The app offers much of the functionality of Apple\u2019s built-in location-sharing features, but it includes emergency safety features such as an SOS button and vehicle crash detection. The company says these features have saved lives.\nBut Life360\u2019s location-based features are also sources of data points for a growing, multibillion-dollar industry that trades in location data gathered from mobile phones. Advertisers, government agencies, and investors are willing to spend hundreds of thousands of dollars for location data and the insights that can be derived from it.\nReport Deeply and Fix Things Because it turns out moving fast and breaking things broke some super important things. Give Now\nWhile children can use the app (with parental consent), Life360\u2019s policy states that the company doesn\u2019t sell data on any users under 13. The Children\u2019s Online Privacy Protection Rule (better known as \u201cCOPPA\u201d) creates restrictions on digital services used by children under 13, and Life360 has detection methods like requiring a scan of a parent\u2019s ID for underage users. Life360 does \u201cdisclose\u201d younger children\u2019s information to third parties \u201cas needed to analyze and detect driving behavior data, perform analytics or otherwise ,[sic] support the features and functionality of our Service,\u201d according to its privacy policy, but not \u201cfor marketing or advertising purposes.\u201d\nMarketers use location data to target ads to people near businesses, while investors buy data to determine popularity based on foot traffic. Government agencies have bought location data to track movement patterns and in one case to support \u201cSpecial Operations Forces mission requirements overseas.\u201d\n\u201cIt sounds like the company\u2019s pointing to a couple of cases where, sure, they helped somebody, they were able to do something good,\u201d Sherman said. \u201cBut then they will not talk about all of the other cases where the buying and selling of this data is potentially very harmful.\u201d\nIn July, a high-ranking Catholic priest resigned after a Catholic news outlet outed him by using location data from the gay dating app Grindr linked to his device. The data was obtained by an unknown vendor, and the report claimed to show that the priest frequented gay bars. There is no indication that Life360 was involved in this incident.\nGrindr, like other apps that feed data into this industry, is required to ask for location permissions when a user first opens the app.\n\u201cWe are not aware of any instance where our data has been traced back to individuals via our data partners,\u201d Hulls said. \u201cFurthermore, our contracts contain language specifically prohibiting any reidentification, and we would aggressively take action against any breach of this term.\u201d\nIn Life360\u2019s case, because of how the app works, it asks for the broadest location permissions possible for functional purposes. Many apps that use location data allow users to grant access only while it\u2019s in use. Because Life360 is for tracking whereabouts in real time, the app asks for location data at all times\u2014and does not function unless that permission is turned on.\nA disclaimer appears in smaller print at the bottom of the permissions screen: \u201cYour location data may be shared with Partners for the purposes of crash detection, research, analytics, attribution and tailored advertising.\u201d Users can disable the sale of their location data in the privacy settings, though that setting is not disclosed in or part of the prompt.\nLife360\u2019s Hulls said that millions of its users have used this feature to opt out of their data being sold.\n\u21a9\ufe0e link How to Disable the Sale of Your Location Data in the Life360 App Tap on the gear icon for \"Settings\" Tap on \u201cPrivacy & Security\u201d Tap on \u201cDo Not Sell My Personal Information\u201d Toggle the button next to \u201cPersonal Information Sales\u201d to the off position Source: Life360 app\nFor those who have not opted out, their Life360 data may be shared with the company\u2019s partners within 20 minutes of being recorded, a former Life360 employee said.\nHulls said this description was \u201cdirectionally accurate,\u201d saying it only applied to certain partners and use cases.\n\u201cFor example, some use cases, like road traffic probing, which powers travel time estimates in automotive navigation systems and GPS apps, require very fresh data,\u201d he said.\nPrivacy researchers and app store operators often look for data brokers\u2019 code in apps for signs of an app sending data off to third parties. But Life360 collects its data directly from the app and provides it to data brokers through its own servers.\nApple\u2019s and Google\u2019s app stores have no way of detecting this transfer of location data to a third party. \u201cIt makes sense to send this data directly from the server side from the app vendor so it can never be traced or observed by anyone,\u201d said Wolfie Christl, a researcher who investigates digital tracking.\nHulls said Life360\u2019s method of providing data through its own servers wasn\u2019t an intentional effort to evade detection from researchers and app stores.\n\u201cThis is completely unrelated. We have our own proprietary sensor technology, which we started building in 2008 well before the emergence of the data industry, and we avoid using SDKs that could have a negative battery impact or other interplay with our own sensor technology,\u201d he said.\nGoogle didn\u2019t comment on why Life360 was able to sell data this way despite its policy against selling location data. Apple spokesperson Adam Dema responded with a link to Life360\u2019s privacy policy but didn\u2019t comment about the company\u2019s data sales to companies like SafeGraph and X-Mode.\nHulls said Life360 de-identifies the data it sells, which can include a device\u2019s mobile advertising ID, IP address, and latitude and longitude coordinates collected by Life360\u2019s app.\nHulls clarified that \u201cde-identification\u201d involves removing usernames, emails, phone numbers, and other types of identifiable user information before the data is shared with Life360\u2019s customers. The data sold still includes a device\u2019s mobile advertising ID and latitude and longitude coordinates.\nEven without names or phone numbers, researchers have repeatedly demonstrated how \u201canonymized\u201d location data can easily be connected to the people from whom it came.\nAnd privacy experts note that mobile advertising IDs are more valuable than identifiers like names.\n\u201cThis code can be used to track and follow you across many life situations,\u201d Christl said. \u201cAs such, it is a much better identifier than a name.\u201d\n\u21a9\ufe0e link Controversial Partners\nThe location data industry operates largely out of public view and with little oversight or regulation. Some of Life360\u2019s partners have faced controversy in the past over how they handle data and privacy.\nStarted in 2013 as Drunk Mode, a novelty app that \u201cprevents users from drunk dialing,\u201d X-Mode was reportedly banned from the big app stores after Vice\u2019s Motherboard reported that the company was selling location data from Muslim prayer apps like Muslim Pro to U.S. government contractors associated with national security, raising concerns about unconstitutional government surveillance.\nPublic records show that X-Mode received at least $423,000 from the U.S. Air Force and the Defense Intelligence Agency for location data between 2019 and 2020. The company also sold data on Americans in profiled sets, like people who were drivers or likely to shop at department stores, according to Motherboard.\nIn August, X-Mode was purchased by intellectual property intelligence firm Digital Envoy and rebranded as Outlogic.\nIn response to the backlash over X-Mode\u2019s selling location data to defense contractors, its new owners said the company would stop selling U.S. location data to such companies.\nReport Deeply and Fix Things Because it turns out moving fast and breaking things broke some super important things. Give Now\n\u201cWe cannot comment on the practices of another company or what that company does with data it receives from other sources,\u201d Hulls said. \u201cHowever, Life360 has worked closely with X-Mode to ensure that X-Mode and all of its data customers do not sell data originating from Life360 to law enforcement agencies or to any government agency to be used for a law enforcement purpose.\u201d\nSafeGraph is one of the biggest firms in the location data business, and its investors include venture capitalist Peter Thiel; Prince Turki Al Faisal Al Saud, former head of Saudi intelligence; and Life360\u2019s chief business officer, Itamar Novick.\nThe company specializes in data that associates places of interest with raw coordinates, adding a layer of meaning to the raw location data that the company ingests. SafeGraph was identified as not just a customer of Life360\u2019s data but also a major partner in an email from a Life360 executive that was viewed by The Markup.\nIn April, as first reported by Motherboard, SafeGraph was awarded a $420,000 contract to sell data to the Centers for Disease Control described as \u201cData Gathering and Reporting.\u201d The Washington Post also reported that SafeGraph shared billions of phone location records with the D.C. Department of Health through its spinoff company Veraset.\nThe company openly sells location data on Amazon\u2019s data marketplace, including a $240,000 yearly subscription to data on people across the U.S. Veraset has boasted of selling location data for purposes including marketing, real estate, investing, and city planning.\nSen. Ron Wyden has flagged SafeGraph as a \u201cdata broker of concern\u201d to Google, Wyden\u2019s chief communications officer, Keith Chu, said in an email. The Democrat from Oregon has made multiple attempts to speak with SafeGraph to learn more about how the company obtains, sells, and shares Americans\u2019 location data, but the company never responded, Chu said.\nCuebiq also worked with the Centers for Disease Control, with a $208,000 contract awarded in June for aggregated location data, according to public records.\nThe CDC didn\u2019t respond to requests for comment.\nDuring the beginning of the coronavirus pandemic, Cuebiq became a main source of location data for news outlets looking to report on people\u2019s movements after cities and states issued stay-at-home orders. Outlets including The New York Times and NBC News received location data from Cuebiq for their analyses.\nIt\u2019s been suggested that location data brokers like Cuebiq are using the pandemic to improve their public reputation by presenting themselves as tools for public health rather than as mechanisms for surveillance.\nCuebiq\u2019s Daddi said the company\u2019s data has helped in the aftermath of natural disasters and public health crises.\n\u21a9\ufe0e link Safety vs. Privacy\nLife360 has positioned itself as \u201cthe leading digital safety brand for families.\u201d But experts say families who use it are not necessarily thinking about their digital security.\n\u201cAn app that claims to be a family safety service selling exact location data to several other companies, this is a total disaster,\u201d Christl said. \u201cIt would be a problem if it\u2019s any other app, and it\u2019s even more a problem when it\u2019s an app that claims to be a family safety service.\u201d\nAn app that claims to be a family safety service selling exact location data to several other companies, this is a total disaster. Wolfie Christl, researcher\nLife360 has faced concerns over privacy in the past. In mid-2020, teens, displeased at the privacy invasion of an app that allowed their parents to minutely track their movements, took to TikTok to encourage their peers to bomb the app with negative reviews. Over the course of a month, the app received more than a million one-star reviews, driving the average rating down from 4.6 to 2.7 stars.\nHulls responded by adding a \u201cbubbles\u201d feature that shows parents a more vague location of their child (but still allows parents to see exact locations with an additional step). He also recruited and paid teens to hawk the app on TikTok, resulting in a \u201cviral surge in downloads,\u201d according to the company.\nThose teens, however, were likely not aware that their parents were hardly the only ones privy to data on their movements.\nSamira Madi, an 18-year-old student in Texas, started using Life360 when she was 15. She didn\u2019t have a problem with the company sharing her location data for marketing and advertising purposes, which the company readily disclosed.\nAfter learning about who Life360 was selling data to, and the scale it was sold at, Madi felt that the company crossed a line.\n\u201cI had no idea it would be passed around this way,\u201d Madi said in an email. \u201cThis concerns me because I would not want my location data to possibly be sold to people with ill intentions.\u201d",
        "url": "https://themarkup.org/privacy/2021/12/06/the-popular-family-safety-app-life360-is-selling-precise-location-data-on-its-tens-of-millions-of-user",
        "labels": {
            "software": 0.7445347905158997,
            "data privacy": 0.07889522612094879,
            "iphone": 0.03682065010070801,
            "ipad": 0.028407447040081024,
            "airtag": 0.026445681229233742,
            "airpods": 0.02359197847545147,
            "apple watch": 0.014272185042500496,
            "leaks or rumours": 0.014217634685337543,
            "apple tv or apple music": 0.010234110057353973,
            "mac computers": 0.010055401362478733,
            "employees": 0.006633237469941378,
            "money or stock": 0.00589162390679121
        }
    },
    "article_11": {
        "title": "25 Years Ago, Apple Acquired NeXT and Brought Back Steve Jobs",
        "body": "This week marks the 25th anniversary of Apple announcing that it had agreed to acquire NeXT for $400 million. The stunning move brought Steve Jobs back to Apple over a decade after he left the company following an internal power struggle.\n\"The acquisition of NeXT is the start of a new chapter in Apple's history and represents a milestone in our transformation as a corporation,\" said former Apple CEO Gil Amelio, when the deal was announced December 20, 1996. \"Today Apple welcomed back its most talented visionary Steve Jobs, someone who can inspire a new generation of customers and software developers and show that Apple remains the industry home for innovation and excitement.\"\nA key reason Apple acquired NeXT was to gain access to its Unix-based NeXTSTEP operating system, which served as the foundation of Mac OS X.\n\"With this merger, the advanced software from NeXT will be married with Apple's very high-volume hardware platforms and marketing channels to create another breakthrough, leapfrogging existing platforms, and fueling Apple and the industry copy cats for the next ten years and beyond,\" said Jobs. \"I still have very deep feelings for Apple, and it gives me great joy to play a role in architecting Apple's future.\"\nJobs initially returned to Apple as an advisor, making his first appearance at Macworld San Francisco in January 1997 to announce details of how Apple planned to incorporate NeXT technology into future releases of Mac OS. Jobs went on to replace Amelio as the company's interim CEO just nine months later, and he ended up dropping the interim title and remaining CEO until stepping down in August 2011 due to health issues.\nUnder the leadership of Jobs, Apple went from near bankruptcy to becoming the world's most valuable company, introducing iconic products like the iPod, iPhone, and iPad along the way. Without acquiring NeXT, Apple's fate may have been far different.\nSteve Hayman, a longtime Apple and NeXT employee, reflected on the 25th anniversary of the two companies merging in a blog post earlier this week.",
        "url": "https://www.macrumors.com/2021/12/22/apple-acquiring-next-25th-anniversary/",
        "labels": {
            "software": 0.2264547199010849,
            "airtag": 0.16181042790412903,
            "money or stock": 0.11976157128810883,
            "employees": 0.09309004992246628,
            "mac computers": 0.0808267593383789,
            "apple tv or apple music": 0.07871591299772263,
            "apple watch": 0.07102064043283463,
            "data privacy": 0.040628597140312195,
            "airpods": 0.03941153734922409,
            "iphone": 0.03776188939809799,
            "ipad": 0.028763750568032265,
            "leaks or rumours": 0.021754123270511627
        }
    },
    "article_12": {
        "title": "TDIL: Apple has a site to show you exactly how siri works",
        "body": "The \"Hey Siri\" feature allows users to invoke Siri hands-free. A very small speech recognizer runs all the time and listens for just those two words. When it detects \"Hey Siri\", the rest of Siri parses the following speech as a command or query. The \"Hey Siri\" detector uses a Deep Neural Network (DNN) to convert the acoustic pattern of your voice at each instant into a probability distribution over speech sounds. It then uses a temporal integration process to compute a confidence score that the phrase you uttered was \"Hey Siri\". If the score is high enough, Siri wakes up. This article takes a look at the underlying technology. It is aimed primarily at readers who know something of machine learning but less about speech recognition.\nHands-Free Access to Siri\nTo get Siri\u2019s help, say \"Hey Siri\". No need to press a button as \"Hey Siri\" makes Siri hands-free. It seems simple, but quite a lot goes on behind the scenes to wake up Siri quickly and efficiently. Hardware, software, and Internet services work seamlessly together to provide a great experience.\nFigure 1. The Hey Siri flow on iPhone\nBeing able to use Siri without pressing buttons is particularly useful when hands are busy, such as when cooking or driving, or when using the Apple Watch. As Figure 1 shows, the whole system has several parts. Most of the implementation of Siri is \"in the Cloud\", including the main automatic speech recognition, the natural language interpretation and the various information services. There are also servers that can provide updates to the acoustic models used by the detector. This article concentrates on the part that runs on your local device, such as an iPhone or Apple Watch. In particular, it focusses on the detector: a specialized speech recognizer which is always listening just for its wake-up phrase (on a recent iPhone with the \"Hey Siri\" feature enabled).\nThe Detector: Listening for \"Hey Siri\"\nThe microphone in an iPhone or Apple Watch turns your voice into a stream of instantaneous waveform samples, at a rate of 16000 per second. A spectrum analysis stage converts the waveform sample stream to a sequence of frames, each describing the sound spectrum of approximately 0.01 sec. About twenty of these frames at a time (0.2 sec of audio) are fed to the acoustic model, a Deep Neural Network (DNN) which converts each of these acoustic patterns into a probability distribution over a set of speech sound classes: those used in the \"Hey Siri\" phrase, plus silence and other speech, for a total of about 20 sound classes. See Figure 2.\nThe DNN consists mostly of matrix multiplications and logistic nonlinearities. Each \"hidden\" layer is an intermediate representation discovered by the DNN during its training to convert the filter bank inputs to sound classes. The final nonlinearity is essentially a Softmax function (a.k.a. a general logistic or normalized exponential), but since we want log probabilities the actual math is somewhat simpler.\nFigure 2. The Deep Neural Network used to detect \"Hey Siri.\" The hidden layers are actually fully connected. The top layer performs temporal integration. The actual DNN is indicated by the dashed box.\nWe choose the number of units in each hidden layer of the DNN to fit the computational resources available when the \"Hey Siri\" detector runs. Networks we use typically have five hidden layers, all the same size: 32, 128, or 192 units depending on the memory and power constraints. On iPhone we use two networks\u2014one for initial detection and another as a secondary checker. The initial detector uses fewer units than the secondary checker.\nThe output of the acoustic model provides a distribution of scores over phonetic classes for every frame. A phonetic class is typically something like \"the first part of an /s/ preceded by a high front vowel and followed by a front vowel.\"\nWe want to detect \"Hey Siri\" if the outputs of the acoustic model are high in the right sequence for the target phrase. To produce a single score for each frame we accumulate those local values in a valid sequence over time. This is indicated in the final (top) layer of Figure 2 as a recurrent network with connections to the same unit and the next in sequence. Inside each unit there is a maximum operation and an add:\nwhere\nF i,t is the accumulated score for state i of the model\nis the accumulated score for state i of the model q i,t is the output of the acoustic model\u2014the log score for the phonetic class associated with the ith state given the acoustic pattern around time t\nis the output of the acoustic model\u2014the log score for the phonetic class associated with the ith state given the acoustic pattern around time t s i is a cost associated with staying in state i\nis a cost associated with staying in state i m i is a cost for moving on from state i\nBoth s i and m i are based on analysis of durations of segments with the relevant labels in the training data. (This procedure is an application of dynamic programming, and can be derived based on ideas about Hidden Markov Models\u2014HMMs.)\nFigure 3. Visual depiction of the equation\nEach accumulated score F i,t is associated with a labelling of previous frames with states, as given by the sequence of decisions by the maximum operation. The final score at each frame is F i,t , where the last state of the phrase is state I and there are N frames in the sequence of frames leading to that score. (N could be found by tracing back through the sequence of max decisions, but is actually done by propagating forwards the number of frames since the path entered the first state of the phrase.)\nAlmost all the computation in the \"Hey Siri\" detector is in the acoustic model. The temporal integration computation is relatively cheap, so we disregard it when assessing size or computational resources.\nYou may get a better idea of how the detector works by looking at Figure 4, which shows the acoustic signal at various stages, assuming that we are using the smallest DNN. At the very bottom is a spectrogram of the waveform from the microphone. In this case, someone is saying \"Hey Siri what \u2026\" The brighter parts are the loudest parts of the phrase. The Hey Siri pattern is between the vertical blue lines.\nFigure 4. The acoustic pattern as it moves through the detector\nThe second horizontal strip up from the bottom shows the result of analyzing the same waveform with a mel filter bank, which gives weight to frequencies based on perceptual measurements. This conversion also smooths out the detail that is visible in the spectrogram and due to the fine-structure of the excitation of the vocal tract: either random, as in the /s/, or periodic, seen here as vertical striations.\nThe alternating green and blue horizontal strips labelled H1 to H5 show the numerical values (activations) of the units in each of the five hidden layers. The 32 hidden units in each layer have been arranged for this figure so as to put units with similar outputs together.\nThe next strip up (with the yellow diagonal) shows the output of the acoustic model. At each frame there is one output for each position in the phrase, plus others for silence and other speech sounds. The final score, shown at the top, is obtained by adding up the local scores along the bright diagonal according to Equation 1. Note that the score rises to a peak just after the whole phrase enters the system.\nWe compare the score with a threshold to decide whether to activate Siri. In fact the threshold is not a fixed value. We built in some flexibility to make it easier to activate Siri in difficult conditions while not significantly increasing the number of false activations. There is a primary, or normal threshold, and a lower threshold that does not normally trigger Siri. If the score exceeds the lower threshold but not the upper threshold, then it may be that we missed a genuine \"Hey Siri\" event. When the score is in this range, the system enters a more sensitive state for a few seconds, so that if the user repeats the phrase, even without making more effort, then Siri triggers. This second-chance mechanism improves the usability of the system significantly, without increasing the false alarm rate too much because it is only in this extra-sensitive state for a short time. (We discuss testing and tuning for accuracy later.)\nResponsiveness and Power: Two Pass Detection\nThe \"Hey Siri\" detector not only has to be accurate, but it needs to be fast and not have a significant effect on battery life. We also need to minimize memory use and processor demand\u2014particularly peak processor demand.\nTo avoid running the main processor all day just to listen for the trigger phrase, the iPhone\u2019s Always On Processor (AOP) (a small, low-power auxiliary processor, that is, the embedded Motion Coprocessor) has access to the microphone signal (on 6S and later). We use a small proportion of the AOP\u2019s limited processing power to run a detector with a small version of the acoustic model (DNN). When the score exceeds a threshold the motion coprocessor wakes up the main processor, which analyzes the signal using a larger DNN. In the first versions with AOP support, the first detector used a DNN with 5 layers of 32 hidden units and the second detector had 5 layers of 192 hidden units.\nFigure 5. Two-pass detection\nApple Watch presents some special challenges because of the much smaller battery. Apple Watch uses a single-pass \"Hey Siri\" detector with an acoustic model intermediate in size between those used for the first and second passes on other iOS devices. The \"Hey Siri\" detector runs only when the watch motion coprocessor detects a wrist raise gesture, which turns the screen on. At that point there is a lot for WatchOS to do\u2014power up, prepare the screen, etc.\u2014so the system allocates \"Hey Siri\" only a small proportion (~5%) of the rather limited compute budget. It is a challenge to start audio capture in time to catch the start of the trigger phrase, so we make allowances for possible truncation in the way that we initialize the detector.\n\"Hey Siri\" Personalized\nWe designed the always-on \"Hey Siri\" detector to respond whenever anyone in the vicinity says the trigger phrase. To reduce the annoyance of false triggers, we invite the user to go through a short enrollment session. During enrollment, the user says five phrases that each begin with \"Hey Siri.\" We save these examples on the device.\nWe compare any possible new \"Hey Siri\" utterance with the stored examples as follows. The (second-pass) detector produces timing information that is used to convert the acoustic pattern into a fixed-length vector, by taking the average over the frames aligned to each state. A separate, specially trained DNN transforms this vector into a \"speaker space\" where, by design, patterns from the same speaker tend to be close, whereas patterns from different speakers tend to be further apart. We compare the distances to the reference patterns created during enrollment with another threshold to decide whether the sound that triggered the detector is likely to be \"Hey Siri\" spoken by the enrolled user.\nThis process not only reduces the probability that \"Hey Siri\" spoken by another person will trigger the iPhone, but also reduces the rate at which other, similar-sounding phrases trigger Siri.\nFurther Checks\nIf the various stages on the iPhone pass it on, the waveform arrives at the Siri server. If the main speech recognizer hears it as something other than \"Hey Siri\" (for example \"Hey Seriously\") then the server sends a cancellation signal to the phone to put it back to sleep, as indicated in Fig 1. On some systems we run a cut-down version of the main recognizer on the device to provide an extra check earlier.\nThe Acoustic Model: Training\nThe DNN acoustic model is at the heart of the \"Hey Siri\" detector. So let\u2019s take a look at how we trained it. Well before there was a Hey Siri feature, a small proportion of users would say \"Hey Siri\" at the start of a request, having started by pressing the button. We used such \"Hey Siri\" utterances for the initial training set for the US English detector model. We also included general speech examples, as used for training the main speech recognizer. In both cases, we used automatic transcription on the training phrases. Siri team members checked a subset of the transcriptions for accuracy.\nWe created a language-specific phonetic specification of the \"Hey Siri\" phrase. In US English, we had two variants, with different first vowels in \"Siri\"\u2014one as in \"serious\" and the other as in \"Syria.\" We also tried to cope with a short break between the two words, especially as the phrase is often written with a comma: \"Hey, Siri.\" Each phonetic symbol results in three speech sound classes (beginning, middle and end) each of which has its own output from the acoustic model.\nWe used a corpus of speech to train the DNN for which the main Siri recognizer provided a sound class label for each frame. There are thousands of sound classes used by the main recognizer, but only about twenty are needed to account for the target phrase (including an initial silence), and one large class class for everything else. The training process attempts to produce DNN outputs approaching 1 for frames that are labelled with the relevant states and phones, based only on the local sound pattern. The training process adjusts the weights using standard back-propagation and stochastic gradient descent. We have used a variety of neural network training software toolkits, including Theano, Tensorflow, and Kaldi.\nThis training process produces estimates of the probabilities of the phones and states given the local acoustic observations, but those estimates include the frequencies of the phones in the training set (the priors), which may be very uneven, and have little to do with the circumstances in which the detector will be used, so we compensate for the priors before the acoustic model outputs are used.\nTraining one model takes about a day, and there are usually a few models in training at any one time. We generally train three versions: a small model for the first pass on the motion coprocessor, a larger-size model for the second pass, and a medium-size model for Apple Watch.\n\"Hey Siri\" works in all languages that Siri supports, but \"Hey Siri\" isn\u2019t necessarily the phrase that starts Siri listening. For instance, French-speaking users need to say \"Dis Siri\" while Korean-speaking users say \"Siri \uc57c\" (Sounds like \"Siri Ya.\") In Russian it is \"\u043f\u0440\u0438\u0432\u0435\u0442 Siri \" (Sounds like \"Privet Siri\"), and in Thai \"\u0e2b\u0e27\u0e31\u0e14\u0e14\u0e35 Siri\". (Sounds like \"Wadi Siri\".)\nTesting and Tuning\nAn ideal detector would fire whenever the user says \"Hey Siri,\" and not fire at other times. We describe the accuracy of the detector in terms of two kinds of error: firing at the wrong time, and failing to fire at the right time. The false-accept rate (FAR or false-alarm rate), is the number of false activations per hour (or mean hours between activations) and the false-reject rate (FRR) is the proportion of attempted activations that fail. (Note that the units we use to measure FAR are not the same as those we use for FRR. Even the dimensions are different. So there is no notion of an equal error rate.)\nFor a given model we can change the balance between the two kinds of error by changing the activation threshold. Figure 6 shows examples of this trade-off, for two sizes of early-development models. Changing the threshold moves along the curve.\nDuring development we try to estimate the accuracy of the system by using a large test set, which is quite expensive to collect and prepare, but essential. There is \"positive\" data and \"negative\" data. The \"positive\" data does contain the target phrase. You might think that we could use utterances picked up by the \"Hey Siri\" system, but the system doesn\u2019t capture the attempts that failed to trigger, and we want to improve the system to include as many of such failed attempts as possible.\nAt first we used the utterances of \"Hey Siri\" that some users said as they pressed the Home button, but these users are not attempting to catch Siri\u2019s attention, (the button does that) and the microphone is bound to be within arm\u2019s reach, whereas we also want \"Hey Siri\" to work across a room. We made recordings specially in various conditions, such as in the kitchen (both close and far), car, bedroom, and restaurant, by native speakers of each language.\nWe use the \"negative\" data to test for false activations (and false wakes). The data represent thousands of hours of recordings, from various sources, including podcasts and non-\"Hey Siri\" inputs to Siri in many languages, to represent both background sounds (especially speech) and the kinds of phrases that a user might say to another person. We need such a lot of data because we are trying to estimate false-alarm rates as low as one per week. (If there are any occurrences of the target phrase in the negative data we label them as such, so that we do not count responses to them as errors.)\nFigure 6. Detector accuracy. Trade-offs against detection threshold for small and larger DNNs\nTuning is largely a matter of deciding what thresholds to use. In Figure 6, the two dots on the lower trade-off curve for the larger model show possible normal and second-chance thresholds. The operating point for the smaller (first-pass) model would be is at the right-hand side. These curves are just for the two stages of the detector, and do not include the personalized stage or subsequent checks.\nWhile we are confident that models that appear to perform better on the test set probably are really better, it is quite difficult to convert offline test results into useful predictions of the experience of users. So in addition to the offline measurements described previously, we estimate false-alarm rates (when Siri turns on without the user saying \"Hey Siri\") and imposter-accept rates (when Siri turns on when someone other than the user who trained the detector says \"Hey Siri\") weekly by sampling from production data, on the latest iOS devices and Apple Watch. This does not give us rejection rates (when the system fails to respond to a valid \"Hey Siri\") but we can estimate rejection rates from the proportion of activations just above the threshold that are valid, and a sampling of just-below threshold events on devices carried by development staff.\nWe continually evaluate and improve \"Hey Siri,\" and the model that powers it, by training and testing using variations of the approach described here. We train in many different languages and test under a wide range of conditions.\nNext time you say \"Hey Siri\" you may think of all that goes on to make responding to that phrase happen, but we hope that it \"just works!\"",
        "url": "https://machinelearning.apple.com/research/hey-siri",
        "labels": {
            "software": 0.6013696193695068,
            "iphone": 0.07859403640031815,
            "airtag": 0.06335627287626266,
            "data privacy": 0.04637131839990616,
            "apple tv or apple music": 0.041310880333185196,
            "apple watch": 0.036408741027116776,
            "mac computers": 0.033932559192180634,
            "ipad": 0.02900347113609314,
            "employees": 0.026965809985995293,
            "money or stock": 0.014483590610325336,
            "leaks or rumours": 0.014258500188589096,
            "airpods": 0.013945220969617367
        }
    },
    "article_13": {
        "title": "Rumor: iOS 16 to drop support for iPhone 6s, iPhone 6s Plus and first-gen iPhone SE",
        "body": "As the new year begins and with iOS 15.2 released, Apple will start internally focusing on the next major release, iOS 16. According to iPhonesoft, based on knowledge of someone running an internal seed, Apple plans to drop support for the iPhone 6s, iPhone 6s Plus, and the original iPhone SE.\nOn the iPad side, iPadOS 16 is said to be incompatible with iPad mini 4, iPad Air 2, iPad (fifth-generation), and the 2015 iPad Pro models.\nOf course, nothing is official until Apple announces iOS 16 at WWDC 2022. However, iPhonesoft has correctly reported the device matrix ahead of time for iOS 13 and iOS 14. The publication was incorrect last year when it claimed that the iPhone 6s and original SE would not be able to update to iOS 15. iOS 15 would go on to support all the same device models as iOS 14.\nAssuming that iPhonesoft is correct with today\u2019s reporting, that would mean that iOS 16 would require an A10 chip as the minimum spec to be able to get the latest features.\nNevertheless, this still represents an incredibly long lifespan of new features and bug fixes for Apple\u2019s phones. The iPhone 6s and iPhone SE supposedly being dropped were first released in late 2015 and early 2016 respectively, meaning they will have received more than six years of regular software updates. The newest device set to be dropped from the software update roadmap is the iPad mini 4, and even that product will be more than five years old by the time iOS 16 ships.\nApple has also laid out plans to continue to support both the current and previous major versions of iOS with critical security patches, further extending the usable life of older models. For example, right now, customers can choose to stick to iOS 14 rather than updating to iOS 15. Assuming Apple sticks to its promises, iPhone 6s and iPhone SE customers will continue to receive iOS 15 security updates for several years after iOS 16 is released in the fall of 2022.\nIt\u2019s still too early in the rumor cycle though to have any idea about what new features to expect from iOS 16. Credible rumors around forthcoming iOS software features usually begin to surface in the spring.",
        "url": "https://9to5mac.com/2021/12/20/ios-16-device-support-rumor/",
        "labels": {
            "leaks or rumours": 0.6921595335006714,
            "software": 0.21063609421253204,
            "iphone": 0.052545998245477676,
            "airtag": 0.01606902666389942,
            "money or stock": 0.007890564389526844,
            "employees": 0.005707502365112305,
            "data privacy": 0.0050506978295743465,
            "airpods": 0.0030471431091427803,
            "apple tv or apple music": 0.0021772750187665224,
            "ipad": 0.0018991592805832624,
            "mac computers": 0.0015809375327080488,
            "apple watch": 0.0012361550470814109
        }
    },
    "article_14": {
        "title": "Apple Allegedly Preparing for iPhones Without SIM Card Slot by September 2022",
        "body": "Earlier this week, a rumor from Brazilian website Blog do iPhone claimed that iPhone 15 Pro models might not have a physical SIM card slot in at least some countries and regions, but the change might happen even sooner.\nImage: iFixit\nAn anonymous tipster informed MacRumors that Apple has advised major U.S. carriers to prepare for the launch of eSIM-only smartphones by September 2022. The tipster shared a seemingly legitimate document outlining the timeframe for this initiative, although the document does not specifically mention Apple or the iPhone.\nAs part of the transition, some U.S. carriers will allegedly start offering select iPhone 13 models without a nano-SIM card in the box in the second quarter of 2022. iPhone 13 models sold at Apple Stores or on Apple.com already lack a nano-SIM card in the box, with users typically able to activate a cellular plan via eSIM by turning on the iPhone, connecting to a Wi-Fi network, and following the on-screen instructions.\nGiven the alleged September 2022 deadline, it is possible that Apple might remove the physical SIM card slot starting with some iPhone 14 models, rather than some iPhone 15 models as originally rumored, but nothing is definitive at this point.\nAn eSIM is a digital SIM that allows users to activate a cellular plan without having to use a nano-SIM card. It's worth noting that eSIM service is not available in all countries, so iPhones with a SIM card slot may remain available in some markets. Adoption is expanding rapidly, though, with over 100 carriers offering eSIM service worldwide and more planning to roll out support in 2022, including Three in the UK and Vodafone in New Zealand.\niPhone 13 models already support multiple eSIM profiles, allowing users to subscribe to several cellular plans digitally and switch between them, and this functionality could pave the way for the SIM card slot's removal in select countries.\nApple's former design chief Jony Ive once envisioned the iPhone as becoming a \"single slab of glass,\" and the SIM card slot's removal would be another step towards a seamless design and improve water resistance in the process. Taking out the slot would also free up some valuable internal space in the iPhone \u2014 every bit counts.\nUpdate: Leaker @dylandkt, who has a respectable track record with Apple rumors, tweeted that they are \"in agreement with recent rumors regarding the removal of the physical SIM card tray.\"",
        "url": "https://www.macrumors.com/2021/12/26/iphones-without-sim-card-slot-2022-rumor/",
        "labels": {
            "leaks or rumours": 0.5582497715950012,
            "iphone": 0.16367290914058685,
            "software": 0.09358131140470505,
            "airtag": 0.042022012174129486,
            "data privacy": 0.036683276295661926,
            "money or stock": 0.029339073225855827,
            "employees": 0.020813480019569397,
            "mac computers": 0.017209794372320175,
            "apple tv or apple music": 0.015718083828687668,
            "ipad": 0.01053866557776928,
            "airpods": 0.006147206295281649,
            "apple watch": 0.00602438859641552
        }
    },
    "article_15": {
        "title": "How to make a QR code on your iPhone to connect guests to your Wi-Fi",
        "body": "How to make a QR code on your iPhone to connect guests to your Wi-Fi\nIf you buy through our links, we may get a commission. Read our ethics policy\nGive your guests a QR code and they can instantly log on to your Wi-Fi network\nRather than have a guest select your Wi-Fi network and then enter a fiddly password, just make them a QR code their iPhone can read. Here's how to do it.\nYou wouldn't want to put a QR code for accessing your network up on street corners like a lost cat photo. Yet when someone you trust comes to your house, giving them your Wi-Fi password is as important as showing them where the restroom is \u2014 if fiddlier.\nIf your Wi-Fi password is any good at all, they are guaranteed to have to ask you to spell - it - out - slow - ly. Or if your password isn't any good, they'll first have to figure out which of the many networks in reach is yours.\nSo instead of that, you can make them a QR code. Perhaps you could email it to them, more likely you need to find a way to show it when they arrive.\nThat could mean just waving your iPhone at theirs, or you could even frame it and have the QR code on the wall in your kitchen. As of iOS 11, iPhones have been able to read QR codes without a third-party app, so the odds are that your guest has all they need.\nIf they don't know how to scan a QR code \u2014 or how to make the scanning much faster by putting a button for it in Control Center \u2014 show them this.\nThen it's up to you to make one. There are third-party apps that will do it, but you don't need that, you just need Shortcuts.\nHow to make a QR code in a Shortcut\nGrab this Shortcut written by Stephen Robles for AppleInsider Run that once Save a copy of the result by Sharing it to yourself over AirDrop, Messages, Mail or more\nIf you'd rather try working out the Shortcut itself, Stephen's one really does two distinct jobs. It prompts for some information, and then it uses that information to create the QR code.\nSo at a minimum, you could use the Ask for Input step in Shortcuts and have a user prompted to enter the Wi-Fi network's name and password. That's really all you need, that Ask for Input, and then Generate QR Code From step.\nYou can download the Shortcut directly, but for interest here are the steps it goes through\nStephen's Shortcut does go further, though. Before you type in the network's name, it asks if you want to use the current Wi-Fi network you're on. And if you do, it then goes out and finds that network's name.\nYou still have to enter the password, but it's a lot handier to tap on a Wi-Fi network name than it is to type that too.\nWhat you have to do next\nStephen's Shortcut ends by displaying the QR code and giving you the option to share it. You could add a step that automatically saves it somewhere.\nOr even that automatically emails a copy to anyone you choose.\nWhat your guest has to do\nWhen the guest is within range of your network, they can scan the QR code. Assuming you've entered the network name and password correctly when you were making the code, that's all they have to do.\nL-R your guest looks for a QR code, the iPhone registers it, and it unlocks the network\nPoint the phone's cameras at a QR code, tap to confirm when prompted whether they want to join this network, and it's done.\nFor the sake of a few minutes work in advance by you, they get straight on your network in moments.",
        "url": "https://appleinsider.com/articles/21/05/14/how-to-make-a-qr-code-to-connect-guests-to-your-wi-fi",
        "labels": {
            "iphone": 0.8108558654785156,
            "airtag": 0.08679001033306122,
            "software": 0.02678047865629196,
            "airpods": 0.012816361151635647,
            "apple tv or apple music": 0.011444605886936188,
            "ipad": 0.011003244668245316,
            "data privacy": 0.00917318556457758,
            "employees": 0.007337728049606085,
            "leaks or rumours": 0.006270404905080795,
            "apple watch": 0.005967026110738516,
            "mac computers": 0.005959601607173681,
            "money or stock": 0.005601463373750448
        }
    },
    "article_16": {
        "title": "This Safari extension replaces YouTube\u2019s video player with one that supports PiP",
        "body": "It\u2019s no secret that using YouTube in the browser on your iPhone or iPad is a rather unpleasant experience. And in many ways, YouTube\u2019s \u201cnative\u201d app isn\u2019t much better. This is where a new Safari extension called \u201cVinegar\u201d comes in to save the day.\nUpdate December 4, 2021: Vinegar has received a nice update today with a host of new features:\nOn videos (and devices) that support 4K, you should see 4K video options in the quality switcher toolbar.\nFor people with expensive data plans, you can also switch to 144p and 240p.\nYou can toggle Theater mode with the keyboard shortcut \u201cT\u201d or click the \u201cTheater\u201d text below the video.\nSupports keyboard shortcuts like \u201cJ\u201d to rewind, \u201cK\u201d to pause, and \u201cL\u201d to forward.\nVideos should be playable if you\u2019re signed in with YouTube Premium.\nVideos should be added to your watching history if you\u2019re signed in.\nFixed a bunch of 3rd-party website embeds.\nSound no longer plays when you go back to the homepage.\nIt should preserve timestamp when you switch video quality.\nTimestamped URLs (i.e. URLs with &t=123) should now work.\nClicking the chapter links in the description should bring you to the correct time.\nYou can now select quality for live videos as well.\nPrivacy-wise, it\u2019s \u201csafer\u201d because now it only has access to web content on youtube.com.\n\u201cVinegar\u201d is a Safari extension for iPhone, iPad, and Mac that was released earlier this month with the goal of replacing the YouTube player with a minimal HTML player (via Daring Fireball). This is similar to the days of YouTube5, which was a similar Safari extension back in the days when the YouTube player was written in Adobe Flash.\nDeveloped by Zhenyi Tan, Vinegar replaces the YouTube player in Safari with a native HTML video tag, bringing a host of associated improvements:\nThe removal of in-video ads.\nPrevent YouTube from tracking your play/pause/seek activities.\nRestore picture-in-picture functionality.\nThe videos don\u2019t stop playing if you switch to another browser tab.\nYou can choose the audio-only stream to keep the music playing when Safari is in the background.\nThe app works just as you would expect it to, and it makes the experience of using YouTube in your browser significantly more enjoyable. It works both directly on YouTube\u2019s website and on videos embedded on third-party websites.\nYou can download Vinegar from the App Store for iPhone, iPad, and Mac. It\u2019s a one-time universal purchase of $1.99.",
        "url": "https://9to5mac.com/2021/12/04/safari-native-youtube-app-player/",
        "labels": {
            "software": 0.8308021426200867,
            "airtag": 0.05627117305994034,
            "airpods": 0.02987506613135338,
            "employees": 0.015121911652386189,
            "ipad": 0.012109804898500443,
            "iphone": 0.01191534474492073,
            "mac computers": 0.011110816150903702,
            "apple watch": 0.010408297181129456,
            "data privacy": 0.009145567193627357,
            "apple tv or apple music": 0.005926967598497868,
            "leaks or rumours": 0.004104377701878548,
            "money or stock": 0.0032084432896226645
        }
    },
    "article_17": {
        "title": "TSA to begin rolling out support for Apple Wallet IDs in February 2022",
        "body": "At WWDC in June, Apple unveiled its ambitious plans to allow users in participating states to store their IDs in the Wallet application. This feature was initially slated to launch sometime this year, but it ended up being delayed until 2022. Now, the Transportation Security Administration \u2013 or TSA \u2013 has announced that it plans to begin accepting digital licenses starting in 2022\u2026\nWhen support for storing IDs in Apple Wallet was first announced at WWDC in June, Apple Pay VP Jennifer Bailey touted that the TSA was \u201cworking to enable\u201d airport security checkpoints as \u201cthe first place you can use your digital ID.\u201d This is what now seems to be coming to fruition.\nThis was announced by the Secure Technology Alliance last week. According to the announcement, the TSA revealed its plans to begin accepting mobile driver\u2019s licenses in airports during a recent industry panel. The TSA plans to launch a pilot program at two state airports starting in February of 2022, then expand it to two more locations in March of 2022.\nThe process will work by requiring a traveler to tap their iPhone on an NFC reader or use a QR scanner:\nSurveys show that travel is the number one use case people are interested in for mDL. The Transportation Security Administration (TSA) is in agreement and it fully supports the mDL movement for travel authentication. TSA is preparing to begin its phased rollout, with mDL Apple Wallet integration being its first step, which will allow select security checkpoints in participating airports to be the first locations people can use their mDL in the Wallet app. During a panel about early mDL adoption, TSA shared its plans to begin accepting mDL use in airports at two state locations starting February of next year. The pilot program will add two additional states around March of 2022. TSA says standards-based digital ID\u2019s, such as state-issued mDL will help streamline and secure the identity verification process. Instead of TSA staff examining a physical ID card, manually comparing a traveler\u2019s ID photo to their face, and verifying flight information, a machine will automate the process. Travelers will simply tap an NFC reader or use a QR scanner to initiate the data exchange. A TSA staff member will be present to oversee and validate the verification process.\nAt this point, it\u2019s unclear which four airports in the United States will participate in this pilot program. Apple has promised that the feature will launch first in Arizona, Connecticut, Georgia, Iowa, Kentucky, Maryland, Oklahoma, and Utah.\nThanks, Joshua!",
        "url": "https://9to5mac.com/2021/12/21/tsa-apple-wallet-id-support/",
        "labels": {
            "iphone": 0.23187865316867828,
            "software": 0.21050667762756348,
            "airtag": 0.10717058181762695,
            "mac computers": 0.09287631511688232,
            "data privacy": 0.0738198310136795,
            "ipad": 0.07088436186313629,
            "apple watch": 0.048907846212387085,
            "leaks or rumours": 0.044547807425260544,
            "money or stock": 0.039629966020584106,
            "apple tv or apple music": 0.03884927183389664,
            "employees": 0.02306152880191803,
            "airpods": 0.017867164686322212
        }
    },
    "article_18": {
        "title": "Gurman: Apple planning redesigned iPad Pro, high-end iMac, new Mac Pro, more for 2022",
        "body": "Apple had a busy year in 2021, announcing the iPhone 13 lineup, new Apple Silicon-powered Macs, and more. Looking ahead to 2022, there\u2019s still a lot to expect from the company. In the latest edition of his Power On newsletter, Bloomberg\u2019s Mark Gurman breaks down everything he expects Apple to unveil in the new year.\nIn today\u2019s newsletter, Gurman corroborates much of what has previously been reported about what to expect from Apple in 2022. Notably, he still says that Apple is planning a new iPad Pro design with support for wireless charging, as well as updates to the lower-end iPad Air and entry-level iPad.\nGurman also says that Apple has a redesigned MacBook Air in the works for 2022, alongside a \u201crevamped, high-end iMac with Apple Silicon.\u201d There\u2019s also a new entry-level MacBook Pro, a new Mac Pro, and a new Mac mini in the works for next year.\nFor the Apple Watch, Gurman expects three new models, including an updated Apple Watch SE, an updated standard model, and a \u201cruggedized version aimed at extreme sports athletes.\u201d\n\u201c2021 was a modest year for Apple product updates, but I\u2019d expect a wider-range of new products in 2022. Let\u2019s go down the list of what I expect to come next year,\u201d Gurman says. Here\u2019s the full breakdown of everything to expect from Apple in 2022, according to Gurman:\nA new iPad Pro design with wireless charging, plus updates to the iPad Air and entry-level iPad.\nA revamped, high-end iMac with Apple silicon to sit above the new 24-inch model.\nAn iPhone SE with 5G.\nNew AirPods Pro earbuds.\nThe biggest MacBook Air revamp in the product\u2019s history, adding the M2 chip and a new design.\nNew versions of the Mac mini, entry-level MacBook Pro and a revamped Mac Pro with Apple silicon.\nOf course, the iPhone 14 lineup.\nThree fresh Apple Watches, including a new Apple Watch SE, an updated standard model, and a ruggedized version aimed at extreme sports athletes.\nAnd, probably most significantly, the introduction of Apple\u2019s first mixed augmented and virtual reality headset.\nIf all of this ends up panning out, it\u2019s clear that Apple has quite a bit in the works for 2022. What are you most excited to see? Let us know down in the comments!",
        "url": "https://9to5mac.com/2021/12/05/apple-new-products-to-expect-in-2022/",
        "labels": {
            "mac computers": 0.5480034947395325,
            "leaks or rumours": 0.2252945899963379,
            "ipad": 0.11198275536298752,
            "airtag": 0.028441598638892174,
            "money or stock": 0.026643510907888412,
            "software": 0.017233731225132942,
            "employees": 0.010591398924589157,
            "apple tv or apple music": 0.00887559074908495,
            "data privacy": 0.008556606248021126,
            "airpods": 0.00573420524597168,
            "iphone": 0.004920653533190489,
            "apple watch": 0.003721913555637002
        }
    },
    "article_19": {
        "title": "'Notchmeister' Lets You Decorate the Notch on Your MacBook Pro",
        "body": "While the inclusion of a display notch on the latest MacBook Pro was derided by many from its first appearance as a rumor ahead of the machine's unveiling, it's now a fact of life for those who have purchased Apple's latest pro notebook.\nVarious apps and wallpaper designs to hide the notch have popped up, but The Iconfactory has gone in the opposite direction, embracing the notch with a simple new app called \"Notchmeister\" that lets you add effects to the notch.\nSo what does Notchmeister do? Think of it as a fun way to spruce up your notch. Or as a screen saver for something you can\u2019t see. Or, maybe, just a useless waste of time.\nThe free app offers a handful of effects you can apply to the area of your screen around the notch, including festive holiday lights dangling from the notch, a pop-up radar screen, and a glow effect that tracks your pointer when it moves behind the notch.\nYes, it's gimmicky, but it's a free download from the Mac App Store, an interesting proof of concept, and worth a few minutes to play with.",
        "url": "https://www.macrumors.com/2021/12/17/notchmeister-decorate-macbook-pro-notch/",
        "labels": {
            "software": 0.5348817110061646,
            "mac computers": 0.43514522910118103,
            "airtag": 0.007752169389277697,
            "employees": 0.006513418164104223,
            "data privacy": 0.004833882674574852,
            "apple tv or apple music": 0.002061198465526104,
            "money or stock": 0.0017592280637472868,
            "airpods": 0.0017067382577806711,
            "ipad": 0.0016744437161833048,
            "apple watch": 0.0015997010050341487,
            "iphone": 0.0011552266078069806,
            "leaks or rumours": 0.0009170136181637645
        }
    },
    "article_20": {
        "title": "After US ban and Apple action, Pegasus spyware maker NSO running out of cash",
        "body": "Pegasus spyware maker NSO Group is reportedly running out of cash following actions by both the US government and Apple. This has led the company to explore options to put itself up for sale.\nTwo US funds have expressed an interest, claiming that they would change the company\u2019s mission from offensive to defensive, though skepticism has been expressed about this \u2026\nBackground\nOur brief guide explains NSO\u2019s current business model.\nNSO Group makes spyware called Pegasus, which is sold to government and law enforcement agencies. The company purchases so-called zero-day vulnerabilities (ones that are unknown to Apple) from hackers, and its software is said to be capable of mounting zero-click exploits \u2013 where no user interaction is required by the target [\u2026] NSO sells Pegasus only to governments, but its customers include countries with extremely poor human rights records \u2013 with political opponents and others targeted.\nThe company has come under increasing scrutiny since an Amnesty International expos\u00e9. The US government subsequently named the company as a national security risk, banning the import and sale of its spyware.\nApple followed this by suing the company for breaching the privacy of iPhone users, and began checking iPhones for signs of compromise by Pegasus, notifying owners that they appeared to have been targeted,\nPegasus spyware maker NSO in financial trouble\nBloomberg reports that the company is now running out of cash, and is exploring exit options.\nNSO Group Ltd., the scandal-plagued spyware company that\u2019s in danger of defaulting on its debts, is exploring options that include shutting its controversial Pegasus unit and selling the entire company, according to people familiar with the matter. Talks have been held with several investment funds about moves that include a refinancing or outright sale, said the people, who asked not to be identified as the discussions are private.\nThe report claims that Pegasus would be repurposed, from hacking smartphones to protecting them.\nThe prospective new owners include two American funds that have discussed taking control and closing Pegasus, one of the people said. Under that scenario, the funds would then inject about $200 million in fresh capital to turn the know-how behind Pegasus into strictly defensive cyber security services.\nHowever, TNW notes that this reported plan to develop a security product is being met with skepticism.\nMajor tech companies, including Meta and Apple, have sued the Israeli company for exploiting their platforms and snooping on people. So it\u2019s hard to trust this kind of product in a defensive avatar. Plus, there will likely be some suspicion of the presence of backdoors being built into its new products. Ronald Deibert, the director of Canada-based research firm Citizen Labs, warned that we should look out for corporate rebranding of the product. \u2018Warning: don\u2019t believe the hype about \u201cdefensive\u201d products. Who\u2019d trust that company with defence? Watch out for corporate rebranding too.\u2019\nIt\u2019s not often we\u2019d hope for a business to fail, but we\u2019ll make an exception here.\nPhoto: Mikhail Fesenko/Unsplash",
        "url": "https://9to5mac.com/2021/12/15/pegasus-spyware-maker-nso-running-out-of-cash/",
        "labels": {
            "software": 0.38068804144859314,
            "money or stock": 0.18046173453330994,
            "iphone": 0.11591175943613052,
            "mac computers": 0.0789756178855896,
            "ipad": 0.05186725780367851,
            "apple watch": 0.047793444246053696,
            "data privacy": 0.03946615010499954,
            "airtag": 0.03446797654032707,
            "apple tv or apple music": 0.027601545676589012,
            "leaks or rumours": 0.020246395841240883,
            "airpods": 0.012933289632201195,
            "employees": 0.00958678126335144
        }
    },
    "article_21": {
        "title": "Apple AirTags Being Used by Thieves to Steal Cars in York Region, Say Police",
        "body": "This was bound to happen at some point, right? According to York Regional Police, thieves have been using Apple\u2019s AirTag tracker to steal cars across the York Region.\nPolice say since September 2021, there have been five incidents investigated where suspects have hidden an AirTag on a high-end car, to be able to track the vehicle\u2019s location and later steal the latter. AirTags are being placed on vehicles found at malls or parking lots, then thieves move in later to steal the car when tracked to the victim\u2019s home, with cars stolen from driveways.\n\u201cOnce inside, an electronic device, typically used by mechanics to reprogram the factory setting, is connected to the onboard diagnostics port below the dashboard and programs the vehicle to accept a key the thieves have brought with them. Once the new key is programmed, the vehicle will start and the thieves drive it away,\u201d explained a York Regional Police press release (via CTV News).\nPolice shared an image below of where an AirTag can be hidden on vehicles.\nAirTags leverage Bluetooth from any iPhone passing within its vicinity, then sending its location data using the latter to iCloud, allowing the owner of the tracker to see its approximate location. Precision Finding down to the exact metre is possible with an iPhone 11 or newer, leveraging Ultra Wideband technology.\nApple\u2019s AirTag will warn iPhone users if an \u201cunknown\u201d AirTag has been following them, and also provide instructions on how to disable it. But this feature is not available for Android phones yet. AirTags can operate down to -20 C and up to 60 C and operates with a CR2032 coin cell battery that lasts one year in the device.\nYork Regional Police are informing people to park their cars in the garage or use camera security to protect their vehicles. More tips can be seen in the video above.",
        "url": "https://www.iphoneincanada.ca/news/apple-airtags-being-used-by-thieves-to-steal-cars-in-york-region-say-police/?fbclid=iwar2uezraerpfmyxsm9ame2x2qrupnpn6efopnvmyukyv07gmgu4sj8a6xou",
        "labels": {
            "airtag": 0.3939432203769684,
            "airpods": 0.2717505395412445,
            "mac computers": 0.10160008072853088,
            "iphone": 0.08794137835502625,
            "ipad": 0.08073682337999344,
            "software": 0.020535040646791458,
            "apple tv or apple music": 0.01165381632745266,
            "leaks or rumours": 0.010980058461427689,
            "data privacy": 0.007886101491749287,
            "apple watch": 0.0056174458004534245,
            "money or stock": 0.004634523764252663,
            "employees": 0.0027209969703108072
        }
    },
    "article_22": {
        "title": "Is Bluetooth holding back Apple's AirPods? We asked the man who made them",
        "body": "While there's every chance that you're not familiar with Gary Geaves, millions of people across the world are familiar with his work. That's because he's the VP of Acoustics at Apple, and he leads the team that's largely responsible for all of Apple's audio products \u2013 and, for that matter, the audio components in products such as iPhones and iPads.\nIt's always a pleasure to speak to the fabulously enthusiastic Geaves, so I jumped at the opportunity to take a deep dive into the development of the AirPods 3 with him and Eric Treski from the Product Marketing Team.\nIt quickly became clear that much of the work in designing the new AirPods 3 revolved around trying to solve problems inherent in the brief to come up with a true-wireless in-ear headphone that crams into its tiny form next-gen technology such as Spatial Audio and ups the sound quality ante without resorting to a burrowing or noise-isolating design.\nAs you'll discover from this long chat, Geaves and his team went to great lengths to overcome the obstacles presented, but there's one problem that still persists, and that we got to towards the end of our conversation. But first...\nAirPods Pro 2 vs AirPods 3: what are the differences?\nWho is Gary Geaves?\nIt may be slightly surprising to learn that the man leading the team behind such a tiny, envelope-pushing device as the AirPods has a background in the British hi-fi industry, most notably at Bowers & WIlkins, but before that Geaves studied computational acoustics. He joined Apple about ten years ago to, as he puts it, \u201cget more focus on audio\u201d.\nI\u2019ve actually met Geaves before, when I was given the opportunity to tour his team\u2019s labs in Cupertino right around the launch of the first (now sadly discontinued) HomePod in 2018. I got a clear sense of the obsessive nature of Apple\u2019s Acoustics Team during that visit. It\u2019s of course no surprise that Apple has loads of money to throw around, but the people I met and the facilities I saw suggested a serious appetite for acoustic problem-solving. That\u2019s a very good thing considering the problems presented by the AirPods 3.\nThe challenges of non-burrowing earphones\n(Image credit: Apple)\nNow I\u2019m going to be upfront and say that I love an open, non-isolating headphone. There are plenty of scenarios that call for noise isolation and/or cancellation, most obviously those that involve planes, trains, buses, children or chit-chatting colleagues, but I do much of my listening while walking or jogging, and at those times I actually want some background noise to reach my ears.\nThe so-called 'Transparency' modes of the AirPods Pro and AirPods Max are very impressive (as are many similar features offered by rival noise-cancelling headphones) but they can never sound as natural as a true open design. What\u2019s more, an in-ear headphone that sits outside the ear canal is always going to be more comfortable than one that\u2019s wedged into it. Personally speaking, I'm therefore glad that Apple has stuck with the non-burrowing design for the AirPods 3, even though doing so presented lots of challenges for Geaves and his team.\n\u201cWe started with looking very closely at the strengths of the original AirPods\u201d, Geaves explains, \u201cand we know many people really like the effortless open fit that doesn\u2019t stick into your ear canal and rests comfortably on your ear. That doesn\u2019t create a seal, which is what people like, but it creates challenges for the audio team\u201d.\nThe biggest challenge, says Geaves, is that \u201cno two ears are alike \u2013 each person\u2019s are a unique fit, and what that means is the sound that people experience will be significantly different, especially the bass\u201d.\nThis is where Adaptive EQ, which was first introduced with the AirPods Pro, comes in: \u201cwe\u2019ve added an inward-facing microphone\u201d, says Geaves, \u201cwhich continuously monitors what\u2019s being played by the speaker and tunes the bass and, to some extent, midrange frequencies as well, to deliver a really consistent frequency response regardless of the level of fit that each person gets\u201d. The idea is that everyone hears the music the same way, and the way the artist intended.\nThe intersection between liberal arts and technology\n(Image credit: Apple)\nThat raises the question, of course, of how Apple gets to the point of thinking it knows what the artist intended. Geaves explains that the approach \u201cis to really try and navigate the intersection between liberal arts and technology\u201d.\nThe technology part \u201cstarts with a strong analytic foundation\u201d, as he puts it. \u201cOver the years we\u2019ve conducted really extensive measurements and we\u2019ve done deep statistical research in order to inform a kind of internal acoustic analytic target response, and we use that to design hardware around\u201d.\nOne suspects that many companies would stop there, particularly as Apple\u2019s data is likely to be far more advanced than that available to the average headphones manufacturer, but Apple is determined to not rely on analytics alone.\n\u201cWe really understand that listening to music is an emotional experience which people connect with on a very deep level\u201d, Geaves says, \u201cso from the analytic tuning we work closely with an expert team of critical listeners and tuners. Many of these are folks from the pro audio industry, and really what they try to do is intentionally refine the sound signature for each product, AirPods in this case, so that it\u2019s accurate, but it\u2019s also exciting and moving\u201d.\nI imagine that someone with a background in computational acoustics, someone clearly so data-driven, would find it hard to hand a device built by science over to subjective human beings, but I get the sense that Geaves is genuinely appreciative of this part of the design process: \u201cwe respect music and the emotional impact that it can make and we want to deliver this natural experience\u201d.\nThe added complications of Spatial Audio\n(Image credit: Apple)\nThe task of delivering the sound as intended apparently becomes much harder when a technology such as Spatial Audio is added into the mix, but Geaves explains that Apple leverages its position in the industry to get opinions from the people creating the music: \u201cwe have whole teams of people who deal with the kind of interaction between engineering and artistic folks \u2013 they don\u2019t let me near artistic folks because I\u2019m too much of a fan to be honest with you \u2013 but we do take feedback from people delivering the content\u201d.\nEric Treski jumps in here to explain how \u201cfull circle\u201d this process is, encompassing not only the Acoustics Team, but also the Apple Music Team and the team behind GarageBand and Logic, who have access to music producers and artists and from whom they take feedback.\nBut the differences between listeners are also even more of a factor with Spatial Audio than with stereo. \u201cThe shape of your ears, the width of your head, to some extent the placement of features on your face, the shape of your head even, mean that everybody hears sound differently\u201d, says Geaves.\n\u201cThese differences can be mathematically categorised by something that\u2019s called head related transfer function. You\u2019ll see this referred to as HRTF frequently. In developing this feature we captured thousands of HRTFs on different people. And these are quite complex, difficult measurements to do \u2013 we\u2019re measuring the sound, the response or your ear to a speaker in multiple different directions \u2013 and we really did that so that we could come up with the best HRTF that works for everyone, which is again very easy to say and not easy to do. It\u2019s not just creating an average, it\u2019s creating the HRTF that\u2019s kind of closest to everybody\u2019s perceptual response\u201d.\nThe approach taken to tuning Spatial Audio, specifically for music, is complex, too. As Geaves puts it, \u201cit\u2019s very easy to come up with a convincing demo that\u2019s very whizzbang, but it\u2019s not quite so easy to develop a feature that works across multiple people and multiple different products and multiple genres.\n\u201cWe could really turn all of these nobs up to 11 and it would sound \u201coh, well that\u2019s big\u201d and then after about a minute you go \u201cyou know what, this is weird, I don\u2019t like this\u201d.\nThe aim is for Spatial Audio to sound natural and convincing, but that apparently isn\u2019t straightforward. Given free, three-dimensional rein, \u201cyou have to take into account the position of your virtual speakers in space, so how far are they are from you, and the angle that they are in space, things like reverberation time \u2013 so you need to choose whether you want your thing to sound like a cathedral or a very small room with lots of carpet and curtains in and things like that. You need to think about decay times of the sound, channel gains and so on. And that\u2019s a really artistic choice\u201d.\nSomething I hadn\u2019t realised is that Spatial Audio is tuned differently depending on the source device: \u201cwhen watching a movie on Apple TV the virtual speakers are placed further away from you than when you\u2019re watching on an iPhone\u201d, Geaves says. Apparently, it would just sound weird to combine a fairly small screen that\u2019s quite close to you with virtual speakers that seem a long way away.\nThe Bluetooth problem\n(Image credit: Apple Music)\nThe AirPods 3 were essentially built from the ground up, using custom-made components. \u201cNothing\u2019s off the shelf\u201d, as Geaves puts it. That includes the \u201cvery low distortion\u201d loudspeaker, which is integrated into \u201cquite a complicated acoustic system\u201d that regulates the movement of the speaker, minimises pressure in the ear canal and features a \u201ccarefully tuned bass port\u201d. The AirPods 3 also feature a \u201cbrand new, custom amplifier\u201d that apparently combines high dynamic range with very low latency (which is particularly crucial for head-tracked Spatial Audio) and serious power-efficiency.\nEverything points to Apple taking sound quality very seriously with the AirPods 3, and all of its modern-day audio products for that matter, and the recent launches of Lossless, Hi-Res Lossless and (in a slightly different way) Spatial Audio point to a real push towards higher audio quality. But there\u2019s a catch, as far as I can see it \u2013 a bottle-neck that\u2019s been preventing real qualitative leaps in the sound of wireless headphones essentially since wireless headphones came into being. I\u2019m talking about Bluetooth, of course, which almost all wireless headphones, including AirPods, rely upon and which doesn\u2019t have the data rate for hi-res or even lossless audio. I ask Geaves whether the use of Bluetooth is holding back his hardware and stifling sound quality.\n\u201cObviously the wireless technology is critical for the content delivery that you talk about\u201d, he says, \u201cbut also things like the amount of latency you get when you move your head, and if that\u2019s too long, between you moving your head and the sound changing or remaining static, it will make you feel quite ill, so we have to concentrate very hard on squeezing the most that we can out of the Bluetooth technology, and there\u2019s a number of tricks we can play to maximise or get around some of the limits of Bluetooth. But it\u2019s fair to say that we would like more bandwidth and\u2026 I\u2019ll stop right there. We would like more bandwidth\u201d, he smiles.\nReading between the lines, I reckon Apple has a plan for overcoming Bluetooth\u2019s current limitations. That could be as simple as switching to Qualcomm\u2019s recently announced aptX Lossless format, but I wonder if Apple might have its own alternative to Bluetooth up its sleeve.\nWhile it's clear that Geaves would love to go deeper into this topic right now, it's obvious that he's not allowed to do so. It seems that we'll just have to wait for the next generation of AirPods to find out whether he and his team have managed to solve the Bluetooth problem and develop a pair of genuinely hi-res wireless headphones.\nMORE:\nOur verdict: Apple AirPods 3 review\nHow HomePod was made: a tale of obsession from inside Apple\u2019s audio labs\nHere's our list of the best wireless headphones you can buy",
        "url": "https://www.whathifi.com/features/is-bluetooth-holding-back-apples-airpods-we-asked-the-man-who-made-them",
        "labels": {
            "airpods": 0.7255691289901733,
            "airtag": 0.067379891872406,
            "leaks or rumours": 0.06455067545175552,
            "employees": 0.03563409671187401,
            "apple tv or apple music": 0.0236976258456707,
            "money or stock": 0.021816641092300415,
            "iphone": 0.01582089439034462,
            "data privacy": 0.011868941597640514,
            "mac computers": 0.010121714323759079,
            "ipad": 0.008951038122177124,
            "apple watch": 0.008102691732347012,
            "software": 0.006486699916422367
        }
    },
    "article_23": {
        "title": "Apple reportedly hires Meta's AR communications lead ahead of headset launch in 2022",
        "body": "As rumors about Apple\u2019s rumored AR headset only grow, the company is said to have hired Meta\u2019s augmented reality communications lead. As reported by Bloomberg\u2019s Mark Gurman in his Power On newsletter, he was told that Apple hired Andrea Schubert, Meta\u2019s communications and public relations head for its augmented reality efforts.\nAhead of that launch, Apple is starting to get its ducks in a row. I\u2019m told the company has hired Andrea Schubert, Meta Platforms Inc.\u2019s communications and public relations head for its augmented reality efforts.\nSo far Bloomberg has reported that Apple\u2019s AR headset can show text, emails, maps, games, and other things through holographic displays built into the lens. Last year, Mark Gurman revealed that the company has two strategies for its augmented reality devices, one being the AR glasses and the other a more robust AR/VR headset.\nThe latest rumors suggest that the device will be targeted at advanced users as it will feature two 8K displays to show ultra-high resolution images. Ming-Chi Kuo believes that Apple\u2019s AR/VR headset will be equipped with advanced 3D sensors capable of not only detecting objects in a scene but also identifying gestures made by the user\u2019s hands.\nWhile he first argued that \u201cApple Glasses\u201d were expected to heavily rely on iPhone, the analyst now says that the device will have an advanced chip to operate without a phone nearby. Another report from Kuo claims that the headset will weigh about 350 grams.\nWhile multiple sources believed that Apple\u2019s AR/VR headset would be introduced in 2020, it now seems that the company may have plans to announce the new product in 2022.\nRight now, the most likely scenario is that Apple\u2019s headset will hit stores in late 2022. Of course, everything can change until Apple officially announces the new product but with Meta\u2019s PR being hired by Apple, it shows that the company is really pushing forward into this AR headset direction.\nRelated:",
        "url": "https://9to5mac.com/2021/12/26/apple-reportedly-hires-metas-ar-communications-lead-ahead-of-headset-launch-in-2022/",
        "labels": {
            "leaks or rumours": 0.48202624917030334,
            "employees": 0.17514820396900177,
            "software": 0.10160790383815765,
            "iphone": 0.062035076320171356,
            "airtag": 0.036020778119564056,
            "mac computers": 0.03006126917898655,
            "money or stock": 0.02580929920077324,
            "apple watch": 0.02027696929872036,
            "data privacy": 0.020095346495509148,
            "apple tv or apple music": 0.019000742584466934,
            "ipad": 0.01796392910182476,
            "airpods": 0.009954248555004597
        }
    },
    "article_24": {
        "title": "Project Zero researchers: Pegasus zero-click iMessage exploit 'one of the most technically sophisticated exploits ever'",
        "body": "Apple announced last month that it had filed a lawsuit against NSO Group, the company behind the advanced \u201cPegasus\u201d spyware that targets iPhone and Android devices. Now, researchers at Google\u2019s Project Zero have gone an in-depth on Pegasus, calling it \u201cone of the most technically sophisticated exploits we\u2019ve ever seen.\u201d\nPegasus spyware is capable of allowing hackers to access the microphone, camera, and other sensitive data from iPhone users. While earlier versions of the spyware required users to click on a link sent via iMessage, that newest version is a zero-click exploit. This means that targeted users do not have to click or interact with the attack at all in order for it to be effective.\nIn an interview with Wired, Project Zeros\u2019s Ian Beer and Samuel Gross explained:\nWe haven\u2019t seen an in-the-wild exploit build an equivalent capability from such a limited starting point, no interaction with the attacker\u2019s server possible, no JavaScript or similar scripting engine loaded, etc. There are many within the security community who consider this type of exploitation \u2014 single-shot remote code execution \u2014 a solved problem. They believe that the sheer weight of mitigations provided by mobile devices is too high for a reliable single-shot exploit to be built. This demonstrates that not only is it possible, it\u2019s being used in the wild reliably against people.\nThe Project Zero researchers say that this represents one of the most sophisticated exploits ever seen:\nBased on our research and findings, we assess this to be one of the most technically sophisticated exploits we\u2019ve ever seen, further demonstrating that the capabilities NSO provides rival those previously thought to be accessible to only a handful of nation states.\nYou can read more from the Project Zero team in a blog post on the Google Project Zero website as well as in this interview with Wired.",
        "url": "https://9to5mac.com/2021/12/16/project-zero-researchers-pegasus-zero-click-imessage-exploit-one-of-the-most-technically-sophisticated-exploits-ever/",
        "labels": {
            "software": 0.38133561611175537,
            "airtag": 0.23580187559127808,
            "iphone": 0.11244925111532211,
            "airpods": 0.07740069180727005,
            "data privacy": 0.041865523904561996,
            "ipad": 0.02809319645166397,
            "leaks or rumours": 0.026281509548425674,
            "apple tv or apple music": 0.02601063810288906,
            "employees": 0.021749982610344887,
            "apple watch": 0.01955140195786953,
            "mac computers": 0.014961420558393002,
            "money or stock": 0.01449879352003336
        }
    }
}