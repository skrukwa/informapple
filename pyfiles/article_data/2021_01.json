{
    "article_0": {
        "title": "Parler app and website go offline; CEO blames Apple and Google for destroying the company",
        "body": "The Parler app is non-functional, and parler.com has gone offline, as Amazon Web Services discontinued service to the company. Both the website and the app relied on AWS for content distribution.\nThe social network was set up after Twitter started clamping down on disinformation and hate speech. However, both Apple and Google dropped the apps after it was revealed that Parler was used to help plan the attempted coup at the Capitol, and was being used to plan a second armed attack in DC ahead of the inauguration \u2026\nBackground\nApple first gave Parler 24 hours to begin properly moderating posts, to remove harmful content.\nWe have received numerous complaints regarding objectionable content in your Parler service, accusations that the Parler app was used to plan, coordinate, and facilitate the illegal activities in Washington D.C. on January 6, 2021 that led (among other things) to loss of life, numerous injuries, and the destruction of property. The app also appears to continue to be used to plan and facilitate yet further illegal and dangerous activities.\nThis included calls to execute US vice-president Mike Pence as part of a second, armed attack in DC.\nWhen the social network\u2019s response proved weak, Apple removed Parler from the App Store.\nWe have determined that the measures you describe are inadequate to address the proliferation of dangerous and objectionable content on your app. Parler has not upheld its commitment to moderate and remove harmful or dangerous content encouraging violence and illegal activity, and is not in compliance with the App Store Review Guidelines.\nGoogle also removed the Android version from the Play store.\nParler app and website go offline\nCNET reports that Amazon followed Apple\u2019s example in pulling the service, following an earlier report that it would do so.\nParler Chief Executive John Matze posted on his service late Saturday that Amazon had informed him it would no longer host his service on its Amazon Web Services platform [\u2026] \u201cThis was a coordinated attack by the tech giants to kill competition in the marketplace,\u201d Matze wrote, adding that his service had become \u201ctoo successful too fast.\u201d He didn\u2019t initially address his platform\u2019s comparatively lax moderation rules or its use by extremists ahead of the Capitol Hill riot. He also didn\u2019t mention increasing concerns that social media apps, including Parler, were being used to organize another attack in the coming weeks.\nWhile Matze had earlier been dismissive of Amazon\u2019s decision, claiming that other companies would be lining up to take their money, Deadline reports that he was forced to backtrack after learning that nobody else wanted to touch Parler either.\nParler CEO John Matze said today that his social media company has been dropped by virtually all of its business alliances after Amazon, Apple and Google ended their agreements with the social media service. \u201cEvery vendor from text message services to email providers to our lawyers all ditched us too on the same day,\u201d Matze said today on Fox News. Matze conceded that the bans could put the company out of business while raising free speech issues, calling it \u201can assault on everybody.\u201d \u201cThey all work together to make sure at the same time we would lose access to not only our apps, but they\u2019re actually shutting all of our servers off tonight, off the internet,\u201d Matze said. \u201cThey made an attempt to not only kill the app, but to actually destroy the entire company. And it\u2019s not just these three companies. Every vendor from text message services to email providers to our lawyers all ditched us too on the same day\u201d [\u2026] He added: \u201cWe\u2019re going to try our best to get back online as quickly as possible. But we\u2019re having a lot of trouble because every vendor we talk to says they won\u2019t work with us. Because if Apple doesn\u2019t approve and Google doesn\u2019t approve, they won\u2019t.\u201d\nSeparately, the WSJ reports that Stripe has stopped processing payments for the Trump campaign website.\nStripe Inc. will no longer process payments for President Trump\u2019s campaign website following last week\u2019s riot at the Capitol, according to people familiar with the matter. The financial-technology company handles card payments for millions of online businesses and e-commerce platforms, including Mr. Trump\u2019s campaign website and online fundraising apparatus. Stripe is cutting off the president\u2019s campaign account for violating its policies against encouraging violence, the people said.\nFive people died in the Capitol attack, including a police officer.\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.6818213632213883
    },
    "article_1": {
        "title": "Apple says it will kick Parler off the App Store in 24 hours unless content is moderated",
        "body": "Apple has informed the social media network Parler that it has 24 hours to rid the platform of inappropriate content, or else face removal from the App Store. The news was first reported by Input Mag, who obtained a copy of the email sent from Apple to Parler.\nApple wrote in the email that it disagrees with Parler\u2019s hands-off approach to moderation, emphasizing that it will not distribute apps that include the types of content found on Parler. Apple says that Parler is responsible for all user-generated content, as per the App Store Review Guidelines.\n\u201cWe want to be clear that Parler is in fact responsible for all the user generated content present on your service and for ensuring that this content meets App Store requirements for the safety and protection of our users,\u201d the company said. \u201cWe won\u2019t distribute apps that present dangerous and harmful content.\u201d\nApple specifically cites the \u201cillegal activities\u201d that took place in Washington D.C. on January 6 as an example. Apple writes that Parler was used to \u201cplan, coordinate, and facilitate\u201d what happened.\n\u201cWe have received numerous complaints regarding objectionable content in your Parler service, accusations that the Parler app was used to plan, coordinate, and facilitate the illegal activities in Washington D.C. on January 6, 2021 that led (among other things) to loss of life, numerous injuries, and the destruction of property. The app also appears to continue to be used to plan and facilitate yet further illegal and dangerous activities.\u201d\nApple shared links to multiple examples of Parler posts inciting violence in the email, not limited to but including:\nHonest question for @AppStore and @GooglePlay. If Parler continues to allow incitement and calls for violence, doesn\u2019t that break your Terms of Service for apps? pic.twitter.com/CkXg99Trl7 \u2014 Sleeping Giants (@slpng_giants) January 7, 2021\nApple\u2019s ultimatum\nIf the content cited by Apple, and all similar content, is not removed within the next 24 hours, then the Parler app will be kicked out of the App Store. Parler, which has publicly taken a hands-off approach to moderation, must also prove to Apple that it will adopt systems and practices to avoid this kind of content from appearing on the social network in the future.\n\u201cPlease remove all objectionable content from your app and submit your revised binary for review. Such content includes any content similar to the examples attached to this message, as well as any content referring to harm to people or attacks on government facilities now or at any future date. In addition, you must respond to this message with detailed information about how you intend to moderate and filter this content from your app, and what you will do to improve moderation and content filtering your service for this kind of objectionable content going forward.\u201d\nApple specifically cites guideline 1.2 of its App Store Review Guidelines, which says that apps with user-generated content must also have precautions in place to manage objections content:\nGuideline 1.2 \u2013 Safety \u2013 User Generated Content Your app enables the display of user-generated content but does not have sufficient precautions in place to effectively manage objectionable content present in your app.\nFinally, Apple also cited comments by Parler CEO John Matze in which said he does not \u201cfeel responsible for any of this and neither should the platform,\u201d in reference to the riots at the Capitol in Washington D.C. this week.\nYour CEO was quoted recently saying \u201cBut I don\u2019t feel responsible for any of this and neither should the platform, considering we\u2019re a neutral town square that just adheres to the law.\u201d We want to be clear that Parler is in fact responsible for all the user generated content present on your service and for ensuring that this content meets App Store requirements for the safety and protection of our users. We won\u2019t distribute apps that present dangerous and harmful content.\nIn response to Apple\u2019s threat to ban Parler from the App Store, Parler co-owner Dan Bongino posted on the platform that this is \u201cclearly an idealogical decision, not a principled one.\u201d He went on to call on Parler users to \u201cspread the word about this destructive war on civil liberties.\u201d\nGiven that Parler has publicly touted that it will forever take a hands-off approach to moderation, it remains to be seen whether it will cave to Apple\u2019s pressure. With Apple having sent its email this morning, we should know the answer as soon as the 24-hour clock expires sometime on Saturday morning.\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.336012649671014
    },
    "article_2": {
        "title": "Apple kicks Parler off the App Store citing 'threats of violence and illegal activity'",
        "body": "After giving Parler 24 hours to introduce new content moderation policies, Apple announced today that Parler has officially been kicked off the App Store. This means the app has been removed from the App Store for new downloads.\nApple bans Parler from the App Store\nIn a statement to 9to5Mac, Apple said:\nWe have always supported diverse points of view being represented on the App Store, but there is no place on our platform for threats of violence and illegal activity. Parler has not taken adequate measures to address the proliferation of these threats to people\u2019s safety. We have suspended Parler from the App Store until they resolve these issues.\nThis comes after Apple threatened to remove Parler yesterday evening. At the time, Apple said that Parler had 24 hours to implement measures to address the objectionable content on its platform. Apple now says that Parler failed to address these concerns in an adequate manner.\nThe removal of Parler from the App Store will stop new users from downloading it, but the app will likely continue working for those who have already installed. However, this does mean that Parler will be unable to release updates to the app, and that future iOS updates could render it obsolete.\nBelow is the communication that Apple sent to the Parler developers today informing them of its intentions to kick Parler off the App Store. Apple says that Parler\u2019s proposed solutions to address the \u201cdangerous and harmful content\u201d on the platform is not sufficient.\nThank you for your response regarding dangerous and harmful content on Parler. We have determined that the measures you describe are inadequate to address the proliferation of dangerous and objectionable content on your app. Parler has not upheld its commitment to moderate and remove harmful or dangerous content encouraging violence and illegal activity, and is not in compliance with the App Store Review Guidelines. In your response, you referenced that Parler has been taking this content \u201cvery seriously for weeks.\u201d However, the processes Parler has put in place to moderate or prevent the spread of dangerous and illegal content have proved insufficient. Specifically, we have continued to find direct threats of violence and calls to incite lawless action in violation of Guideline 1.1 \u2013 Safety \u2013 Objectionable Content. Your response also references a moderation plan \u201cfor the time being,\u201d which does not meet the ongoing requirements in Guideline 1.2 \u2013 Safety \u2013 User Generated content. While there is no perfect system to prevent all dangerous or hateful user content, apps are required to have robust content moderation plans in place to proactively and effectively address these issues. A temporary \u201ctask force\u201d is not a sufficient response given the widespread proliferation of harmful content. For these reasons, your app will be removed from the App Store until we receive an update that is compliant with the App Store Review Guidelines and you have demonstrated your ability to effectively moderate and filter the dangerous and harmful content on your service.\nNote that Apple\u2019s communication to Parler says that the app is banned \u201cuntil we receive an update that is compliant with the App Store Review Guidelines,\u201d which could mean the app returns in the future. Nonetheless, Parler has publicly touted that it will forever take a hands-off approach to moderation.\nParler was also removed from the Google Play Store on Android yesterday, as our colleagues over at 9to5Google reported at the time.\nApple kill-switching Parler no longer necessary for the app to stop working unless it finds a new web host. https://t.co/7RFfIP0r6e \u2014 Mark Gurman (@markgurman) January 10, 2021\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.5598669903963034
    },
    "article_3": {
        "title": "Facebook accuses Apple of killing its business, causing significant drop in revenue. Apple CEO has replied",
        "body": "Listen to this article Listen to this article\nFacebook CEO Mark Zuckerberg is significantly apprehensive about Apple. He had lengthy labeled the telephone manufacturing firm as \u201cone of [Facebook\u2019s] biggest competitors\u201d. Facebook depends on the iPhone and iOS as half of its cell platform within the United States. It is of little surprise the Social Media big carefully displays any replace by Apple.\nThe rift between Facebook and Apple intensified on Wednesday throughout Facebook\u2019s quarterly earnings name with buyers. Last month, Facebook took out full-page newspaper attack ads against Apple.\nZuckerberg continued the assault on Wednesday by accusing Apple\u2019s new modifications round IDFA (IDentifier For Advertisers) as truly being meant to hurt Facebook\u2019s business pursuits and not shield buyer privateness.\nApple delayed releasing a privateness characteristic as half of its iOS 14 replace after builders, together with Facebook, complained it will lower their advert income. The replace will ask customers permission for apps to trace them for promoting functions.\nBut on Wednesday, Apple introduced that the characteristic, initially deliberate to be rolled out in September, would now be launched early this spring. Facebook is apprehensive that the characteristic will lower Facebook\u2019s income. The Social Media big\u2019s Chief Financial Officer Dave Wehner stated the corporate would face \u201csignificant ad targeting headwinds\u201d within the first quarter of 2021 as a result of of Apple\u2019s privateness modifications.\n\u201cApple may say they\u2019re doing this to help people, but the moves clearly track with their competitive interests,\u201d Facebook CEO, Mark Zuckerberg stated on Wednesday.\nHe criticized Apple of making \u201cmisleading\u201d privateness guarantees to folks whereas providing a messaging service, iMessage, that has much less privateness than Facebook\u2019s WhatsApp.\n\u201cWe have a lot of competitors who make claims about privacy that are often misleading,\u201d Zuckerberg stated.\n\u201cNow Apple recently released so-called nutrition labels, which focused largely on metadata that apps collect rather than the privacy and security of people\u2019s actual messages, but iMessage stores non-end-to-end encrypted backups of your messages by default unless you disable iCloud,\u201d he stated.\nZuckerberg went additional to accuse Apple of anticompetitive behavior.\n\u201cWe are also seeing Apple\u2019s business depend more and more on gaining share in apps and services against us and other developers. So Apple has every incentive to use their dominant platform position to interfere with how our apps and other apps work, which they regularly do to preference their own. And this impacts the growth of millions of businesses around the world,\u201d he stated.\nZuckerberg stated that whereas Apple \u201cmay say that they\u2019re doing this to help people,\u201d the modifications \u201cclearly track their competitive interests. And I think that this dynamic is important for people to understand because we and others are going to be up against this for the foreseeable future.\u201d\nIn an announcement on Thursday apparently directed to Facebook however with out naming the corporate, Apple CEO Tim Cook leveled a blistering condemnation of \u201cconspiracy theories juiced by algorithms\u201d as he mentioned the brand new privateness instrument the iPhone maker plans to roll out. Cook cited latest social unrest to a broader argument that app-tracking instruments are turning customers into promoting merchandise.\nTired of Facebook? Check out this alternative and create an account",
        "sentiment": -0.30902937181293966
    },
    "article_4": {
        "title": "Apple launches major new Racial Equity and Justice Initiative projects to challenge systemic racism, advance racial equity nationwide",
        "body": "Apple\u2019s First US Developer Academy to Open in Downtown Detroit\nLater this year, Apple will open an Apple Developer Academy in Detroit \u2014 the first of its kind in the US. Detroit has a vibrant Black entrepreneur and developer community, with over 50,000 Black-owned businesses, according to US Census data. The academy is designed to empower young Black entrepreneurs, creators, and coders, helping them cultivate the skills necessary for jobs in the rapidly growing iOS app economy. Launched in collaboration with Michigan State University, Apple Developer Academy courses will be open to all learners across Detroit, regardless of their academic background or whether they have any previous coding experience.\nThe Apple Developer Academy will offer two programs in Detroit. A 30-day introductory program is designed for learners who are considering app economy careers and looking to better understand what it means to be a developer. The full academy program is an intensive 10- to 12-month program that will help aspiring developers build the skills needed to participate in the iOS app economy, and even start their own businesses. Apple expects the academy\u2019s programming to reach close to 1,000 students each year with a curriculum that covers coding, design, marketing, and professional skills.\nAnd next month, Apple will host the inaugural cohort of its Entrepreneur Camp for Black Founders and Developers for a virtual experience, offering one-on-one code-level guidance from Apple experts and engineers, as well as mentorship, inspiration, and insights from top Apple leaders.\nEmpowering Entrepreneurs Through New Funding Partnerships\nTo address systemic barriers to access and funding faced by Black and Brown entrepreneurs, Apple is today announcing two new investments in the venture capital and banking spaces, with both projects designed to provide capital to minority-owned businesses. The company will invest $10 million with Harlem Capital \u2014 an early-stage venture capital firm based in New York \u2014 to support its investments in 1,000 companies with diverse founders over the next 20 years. In addition to providing capital to entrepreneurs of color, Harlem Capital will also lend its expertise to Apple\u2019s broader efforts to advance access to economic opportunity. The firm will offer guidance and mentorship to students at the Detroit Developer Academy and participants in Apple\u2019s Entrepreneur Camp for Black Founders and Developers. Apple will also support Harlem Capital\u2019s internship program, focused on opening doors for aspiring women and minority investors.\nThe company will also invest $25 million in Siebert Williams Shank\u2019s Clear Vision Impact Fund, which provides capital to small and medium-size businesses, with an emphasis on minority-owned companies. The fund looks to support businesses that operate in or serve underserved markets, and that foster inclusive growth initiatives.\nLifting up Community Organizations\nAs part of its REJI work, Apple continues to build on its contributions toward community colleges, nonprofit advocates, and local organizations working to empower and expand opportunity for the next generation.\nApple is making a contribution to The King Center, a living memorial to the legacy of Dr. Martin Luther King Jr. to share his teachings and inspire new generations to carry forward his unfinished work. Next week, Dr. King\u2019s daughter and the CEO of The King Center, Dr. Bernice A. King, will issue a call to action encouraging young people to give back to their communities as part of Apple\u2019s \u201cChallenge for Change\u201d series \u2014 a set of conversation guides and learning-based challenges on issues related to race and inequality.\nApple\u2019s contribution to The King Center joins the company\u2019s previous donations to nonprofit organizations that advance equity and justice, including the Birmingham Civil Rights Institute and the Equal Justice Initiative in Montgomery, Alabama.",
        "sentiment": 0.6816560509847477
    },
    "article_5": {
        "title": "Tim Cook Implies That Facebook's Business Model of Maximizing Engagement Leads to Polarization and Violence",
        "body": "Apple CEO Tim Cook today spoke at the virtual Computers, Privacy, and Data Protection conference, condemning the business model of companies like Facebook and emphasizing Apple's commitment to advancing user privacy.\n\"At a moment of rampant disinformation and conspiracy theories juiced by algorithms, we can no longer turn a blind eye to a theory of technology that says all engagement is good engagement \u2014 the longer the better \u2014 and all with the goal of collecting as much data as possible,\" said Cook. \"It is long past time to stop pretending that this approach doesn't come with a cost \u2014 of polarization, of lost trust and, yes, of violence,\" he added.\nCook highlighted two recent privacy measures that Apple has taken, including privacy labels in the App Store and App Tracking Transparency, which will require apps to request permission to track users starting with the next iOS 14, iPadOS 14, and tvOS 14 betas. Apple says the software updates will be released in the early spring.\nOn an earnings call yesterday, Facebook CEO Mark Zuckerberg said Apple's privacy claims are often misleading and self serving:\nIncluding -- with the upcoming iOS 14 changes, many small businesses will no longer be able to reach their customers with targeted ads. Now, Apple may say that they're doing this to help people, but the moves clearly track their competitive interests.\nApple has every incentive to use their dominant platform position to interfere with how our apps and other apps work, which they regularly do to preference their own. And this impacts the growth of millions of businesses around the world.\nToday is Data Privacy Day, and Apple has marked the occasion by sharing \"A Day in the Life of Your Data,\" an easy-to-understand PDF report that explains how third-party companies track user data across websites and apps, highlights Apple's privacy principles, and provides more details about App Tracking Transparency.\nCook's remarks can be listened to in this YouTube video starting at the 3:50 mark:\nGood afternoon.\nJohn, thank you for the generous introduction and for hosting us today.\nIt's a privilege to join you \u2014 and to learn from this knowledgeable panel \u2014 on this fitting occasion of Data Privacy Day.\nA little more than two years ago, joined by my good friend, the much-missed Giovanni Buttarelli, and Data Protection regulators from around the world, I spoke in Brussels about the emergence of a data-industrial complex.\nAt that gathering we asked ourselves: \u201cwhat kind of world do we want to live in?\"\nTwo years later, we should now take a hard look at how we've answered that question.\nThe fact is that an interconnected ecosystem of companies and data brokers, of purveyors of fake news and peddlers of division, of trackers and hucksters just looking to make a quick buck, is more present in our lives than it has ever been.\nAnd it has never been so clear how it degrades our fundamental right to privacy first, and our social fabric by consequence.\nAs I've said before, \u201cif we accept as normal and unavoidable that everything in our lives can be aggregated and sold, then we lose so much more than data. We lose the freedom to be human.\"\nAnd yet this is a hopeful new season. A time of thoughtfulness and reform. And the most concrete progress of all is thanks to many of you.\nProving cynics and doomsayers wrong, the GDPR has provided an important foundation for privacy rights around the world, and its implementation and enforcement must continue.\nBut we can't stop there. We must do more. And we're already seeing hopeful steps forward worldwide, including a successful ballot initiative strengthening consumer protections right here in California.\nTogether, we must send a universal, humanistic response to those who claim a right to users' private information about what should not and will not be tolerated.\nAs I said in Brussels two years ago, it is certainly time, not only for a comprehensive privacy law here in the United States, but also for worldwide laws and new international agreements that enshrine the principles of data minimization, user knowledge, user access and data security across the globe.\nAt Apple, spurred on by the leadership of many of you in the privacy community, these have been two years of unceasing action.\nWe have worked to not only deepen our own core privacy principles, but to create ripples of positive change across the industry as a whole.\nWe've spoken out, time and again, for strong encryption without backdoors, recognizing that security is the foundation of privacy.\nWe've set new industry standards for data minimization, user control and on-device processing for everything from location data to your contacts and photos.\nAt the same time that we've led the way in features that keep you healthy and well, we've made sure that technologies like a blood-oxygen sensor and an ECG come with peace of mind that your health data stays yours.\nAnd, last but not least, we are deploying powerful, new requirements to advance user privacy throughout the App Store ecosystem.\nThe first is a simple but revolutionary idea that we call the privacy nutrition label.\nEvery app \u2014 including our own \u2014 must share their data collection and privacy practices, information that the App Store presents in a way every user can understand and act on.\nThe second is called App Tracking Transparency. At its foundation, ATT is about returning control to users \u2014 about giving them a say over how their data is handled.\nUsers have asked for this feature for a long time. We have worked closely with developers to give them the time and resources to implement it. And we're passionate about it because we think it has the great potential to make things better for everybody.\nBecause ATT responds to a very real issue.\nEarlier today, we released a new paper called \u201cA Day in the Life of Your Data.\" It tells the story of how apps that we use every day contain an average of six trackers. This code often exists to surveil and identify users across apps, watching and recording their behavior.\nIn this case, what the user sees is not always what they get.\nRight now, users may not know whether the apps they use to pass the time, to check in with their friends, or to find a place to eat, may in fact be passing on information about the photos they've taken, the people in their contact list, or location data that reflects where they eat, sleep or pray.\nAs the paper shows, it seems that no piece of information is too private or personal to be surveilled, monetized, and aggregated into a 360-degree view of your life. The end result of all of this is that you are no longer the customer, you're the product.\nWhen ATT is in full effect, users will have a say over this kind of tracking.\nSome may well think that sharing this degree of information is worth it for more targeted ads. Many others, I suspect, will not, just as most appreciated it when we built a similar functionality into Safari limiting web trackers several years ago.\nWe see developing these kinds of privacy-centric features and innovations as a core responsibility of our work. We always have, we always will.\nThe fact is that the debate over ATT is a microcosm of a debate we have been having for a long time \u2014 one where our point of view is very clear.\nTechnology does not need vast troves of personal data, stitched together across dozens of websites and apps, in order to succeed. Advertising existed and thrived for decades without it. And we're here today because the path of least resistance is rarely the path of wisdom.\nIf a business is built on misleading users, on data exploitation, on choices that are no choices at all, then it does not deserve our praise. It deserves reform.\nWe should not look away from the bigger picture.\nAt a moment of rampant disinformation and conspiracy theories juiced by algorithms, we can no longer turn a blind eye to a theory of technology that says all engagement is good engagement \u2014 the longer the better \u2014 and all with the goal of collecting as much data as possible.\nToo many are still asking the question, \u201chow much can we get away with?,\" when they need to be asking, \u201cwhat are the consequences?\"\nWhat are the consequences of prioritizing conspiracy theories and violent incitement simply because of their high rates of engagement?\nWhat are the consequences of not just tolerating, but rewarding content that undermines public trust in life-saving vaccinations?\nWhat are the consequences of seeing thousands of users join extremist groups, and then perpetuating an algorithm that recommends even more?\nIt is long past time to stop pretending that this approach doesn't come with a cost \u2014 of polarization, of lost trust and, yes, of violence.\nA social dilemma cannot be allowed to become a social catastrophe.\nI think the past year, and certainly recent events, have brought home the risk of this for all of us \u2014 as a society, and as individuals as much as anything else.\nLong hours spent cooped up at home, the challenge of keeping kids learning when schools are closed, the worry and uncertainty about what the future would hold, all of these things threw into sharp relief how technology can help \u2014 and how it can be used to harm.\nWill the future belong to the innovations that make our lives better, more fulfilled and more human?\nOr will it belong to those tools that prize our attention to the exclusion of everything else, compounding our fears and aggregating extremism, to serve ever-more-invasively-targeted ads over all other ambitions?\nAt Apple, we made our choice a long time ago.\nWe believe that ethical technology is technology that works for you. It's technology that helps you sleep, not keeps you up. That tells you when you've had enough, that gives you space to create, or draw, or write or learn, not refresh just one more time. It's technology that can fade into the background when you're on a hike or going for a swim, but is there to warn you when your heart rate spikes or help you when you've had a nasty fall. And that all of this, always, puts privacy and security first, because no one needs to trade away the rights of their users to deliver a great product.\nCall us naive. But we still believe that technology made by people, for people, and with people's well-being in mind, is too valuable a tool to abandon. We still believe that the best measure of technology is the lives it improves.\nWe are not perfect. We will make mistakes. That's what makes us human. But our commitment to you, now and always, is that we will keep faith with the values that have inspired our products from the very beginning. Because what we share with the world is nothing without the trust our users have in it.\nTo all of you who have joined us today, please keep pushing us all forward. Keep setting high standards that put privacy first. And take new and necessary steps to reform what is broken.\nWe've made progress together, and we must make more. Because the time is always right to be bold and brave in service of a world where, as Giovanni Buttarelli put it, technology serves people, and not the other way around.\nThank you very much.",
        "sentiment": 0.3697366307271605
    },
    "article_6": {
        "title": "Kuo: New MacBook Pro Models to Feature Flat-Edged Design, MagSafe, No Touch Bar and More Ports",
        "body": "Apple is working on two new MacBook Pro models that will feature significant design changes, well-respected Apple analyst Ming-Chi Kuo said today in a note to investors that was obtained by MacRumors.\nAccording to Kuo, Apple is developing two models in 14 and 16-inch size options. The new MacBook Pro machines will feature a flat-edged design, which Kuo describes as \"similar to the iPhone 12\" with no curves like current models. It will be the most significant design update to the MacBook Pro in the last five years.\nThere will be no OLED Touch Bar included, with Apple instead returning to physical function keys. Kuo says the MagSafe charging connector design will be restored, though it's not quite clear what that means as Apple has transitioned to USB-C. The refreshed MacBook Pro models will have additional ports, and Kuo says that Most people may not need to purchase dongles to supplement the available ports on the new machines. Since 2016, Apple's MacBook Pro models have been limited to USB-C ports with no other ports available.\nAll of the new MacBook Pro models will feature Apple silicon chips, and there will be no Intel chip options included.\n1. The two new models are equipped with about 14-inch and 16-inch displays, respectively.\n2. In terms of casing design, the new models cancel the curvy design of existing models\u2019 top and bottom parts and adopt a flat-edged form factor design similar to the iPhone 12.\n3. The MagSafe charging connector design is restored.\n4. The OLED touch bar is removed, and the physical function buttons are restored.\n5. There is no Intel CPU option for the new models.\n6. They are equipped with more types of I/O, and most users may not need to purchase additional dongles.\nThe MacBook Pro models will use the same heat pipe design used by the current 16-inch MacBook Pro model, which Kuo says is much better than the current 13-inch MacBook Pro and MacBook Air because it will allow for increased computing power.\nKuo says that we can expect to see the new MacBook Pro models released in the third quarter of 2021. Due to the revamped design and strong replacement demand, Kuo expects total MacBook shipments to grow significantly by 25 to 30 percent year over year to 20 million units.\nKuo also said that high-end iPhone models coming in 2022 are likely to adopt a vapor chamber thermal system, which Apple is \"aggressively testing.\" The VC thermal system will be required for the high-end iPhones due to their stronger computing power and faster 5G connection speeds. There are already smartphones from companies like Samsung, Razer, and LG that use vapor chamber cooling technology, which is used to keep a device cooler when it is under heavy stress.\nIt is unclear if the vapor chamber thermal system will meet Apple's high requirements, according to Kuo, but he is optimistic about the reliability improvement schedule and expects at least high-end models to adopt it in the near future.",
        "sentiment": 0.35748203890398145
    },
    "article_7": {
        "title": "Adobe Flash rides off into the sunset",
        "body": "Adobe scheduled the end of support for its famous Flash software on December 31st, 2020, and today is the day. While Adobe won\u2019t start blocking Flash content until January 12th, major browsers will shut it all down tomorrow and Microsoft will block it in most versions of Windows. It\u2019s over.\nFlash enjoyed huge cultural relevance and looms large in web history, which might be why its funeral procession has lasted for years. Browsers started showing Flash the door early in the last decade, and in 2015 Adobe asked developers to move on to HTML5. Things became official in 2017, when Adobe announced it would end support.\nWhile Adobe is finally (mercifully) letting Flash go, it will live on in many historical artifacts. The Internet Archive is preserving Flash games and animations, including well-known hits like \u201cPeanut Butter Jelly Time.\u201d",
        "sentiment": -0.24386641383171082
    },
    "article_8": {
        "title": "Report speculates that Google hasn\u2019t updated its iOS apps in weeks to avoid providing privacy details",
        "body": "Apple officially rolled out its new App Privacy labels on the App Store last month, requiring developers to provide detailed privacy information about what data is collected from users. Interestingly, Fast Company has spotted that Google has not updated any of its iOS applications since the new App Privacy details became mandatory.\nUpdate: Google says it will add these privacy labels as soon as this week.\nGoogle privacy details\nAs background, Apple began requiring developers to submit their new privacy information to the App Store in order to update their apps on December 8. The App Privacy labels themselves became visible to users on December 14, coinciding with the release of iOS 14.3.\nWhat this means is that in order to release an update to their applications, developers also have to provide this privacy information. The only way to get around providing the privacy details is to not update an application, and that appears to be exactly the strategy Google has taken.\nThe report from Fast Company notes that the last time Google updated any of its iOS applications was December 7, one day before Apple mandated that developers provide privacy details for their applications. This means that when you visit a Google application in the App Store, you simply see a message that says \u201cNo Details Provided.\u201d\nAt the same time, Google has rolled out updates to its apps on Android since December 7, including multiple updates to the same apps in some instances.\nBy getting in all its existing apps\u2019 updates on or before December 7, Google has managed to avoid filling out a privacy label for any of their apps so far. You can verify this yourself by launching the App Store app on your iPhone, selecting any Google-owned app in the store, and then checking its privacy label on the app\u2019s listing. As of the time of this writing, you\u2019ll see every Google app\u2019s privacy label still reads, \u201cNo Details Provided. The developer will be required to provide privacy details when they submit their next app update.\u201d\nWhat\u2019s important to remember, however, is that Google will have to update its iOS applications at some point. One would hope that this current lapse in updates is due to Google adjusting its privacy practices to avoid the bad publicity of having its App Privacy labels raise questions among users. Whether that\u2019s actually what\u2019s happening here remains to be seen.\nA variety of companies have faced criticism for their App Privacy labels so far. For instance, the new privacy labels have raised awareness among the different messaging applications, while Facebook\u2019s App Privacy label is humorously long.\nAs for what data Google will be required to provide, Apple has emphasized that there are several different pieces of important information that developers should remember while preparing the App Privacy \u201cnutrition labels\u201d for their applications:\nDevelopers should identify all possible data collections and uses, even if certain data will be collected and used only in limited situations.\nDevelopers\u2019 answers should follow the App Store Review Guidelines and any applicable laws.\nDevelopers are responsible for keeping your responses accurate and up-to-date. If your practices change, update your responses in App Store Connect.\nRead more:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.33125608377158644
    },
    "article_9": {
        "title": "Apple overtakes Amazon to become world\u2019s most valuable brand, while Tesla is the fastest-growing",
        "body": "Apple has overtaken Amazon to become the world\u2019s most valuable brand for the first time in five years, according to a global report.\nThe value of the technology giant\u2019s brand has climbed 87% in the past year to $263.4 billion, the Brand Finance Global 500 2021 Index found. The rise was down to Apple\u2019s diversification strategy, which has seen the company expand into digital and subscription services and potentially into electric cars in the future, said London-based brand valuation consulting firm Brand Finance.\nApple AAPL, +1.01% became the first U.S. company to reach a $2 trillion market cap in August last year. It is also expected to post its largest ever quarterly revenue total and its first ever total above $100 billion when it reports earnings on Wednesday.\n\u201cAs Apple reclaims the title of the world\u2019s most valuable brand from Amazon five years since it last held the top spot, we are witnessing it Think Different once again. From Mac to iPod, to iPhone, to iPad, to Apple Watch, to subscription services, to infinity and beyond,\u201d said Brand Finance Chief Executive David Haigh.\nAmazon\u2019s AMZN, +2.99% brand value grew 15% to $254.2 billion, in a year that has seen the e-commerce giant benefit from stay-at-home orders around the world and surging demand. The company has also innovated and expanded in recent months, launching an online pharmacy store as well as the Halo fitness tracker. Earlier this month, Amazon bought its first fleet of planes, from airlines Delta and WestJet, to expand its airfreight network. However, the company lost its top spot to Apple.\nTech giant Google GOOG, +0.97% has the third-highest brand value, edging 1.4% higher to $191.2 billion, while rival Microsoft MSFT, +0.30% stayed in fourth with a brand value of $140.4 billion, a 20% rise, and conglomerate Samsung 005930, +0.50% kept hold of fifth spot. Retailer Walmart WMT, +0.33% jumped two places to sixth, while social media company Facebook FB stayed in seventh.\nIn a year dominated by COVID-19, technology and innovation helped drive the value of the world\u2019s largest brands, with ride-hailing app Uber UBER, +1.41% and Chinese e-commerce platform Meituan 3690, -1.04% enjoying strong growth.\nBut Tesla TSLA, -0.94% was the fastest-growing global brand in terms of value last year, jumping 158% to $32 billion. Brand Finance said record sales numbers, a production ramp-up and expansion into new markets helped boost the electric-car maker\u2019s brand.\nWhile some sectors have performed well, others have endured a difficult year. Airline and aerospace brands accounted for six of the 10 fastest-falling brands: Boeing BA, -0.09% , American Airlines AAL, +1.13% , United Airlines UAL, +0.68% , Delta DAL, -3.54% , Airbus AIR, +0.52% and Safran SAF, +1.04% . Hotels also had a tough 2020 due to travel restrictions and lockdown measures, with Hilton\u2019s HLT, +1.39% brand value falling 30% to $7.6 billion, while Marriott MAR, +1.12% dropped out of the top 500 altogether, along with Airbnb ABNB, +1.91% .\nRead: What will happen to airfares in 2021?\nA brand\u2019s value is the economic benefit it would achieve if it was licensed in the open market, according to the consulting firm.\nThe combined value of the 21 U.K. brands among the world\u2019s top 500 fell 11% year-over-year, making it the worst-performing country, as Brexit uncertainty and the COVID-19 pandemic hit the economy. Oil major Royal Dutch Shell UK:RDSA kept its place as the U.K.\u2019s most valuable brand but dropped 11% to $42.2 billion, while second-placed rival BP BP, -0.13% fell 8% to $21.4 billion.\n\u201cAs the vaccine rollout advances it will be interesting to see which British brands thrive and which will flounder while navigating a future outside the European Union,\u201d Haigh said.\nThe index also includes brand strength, which is calculated as the efficacy of a brand\u2019s performance relative to its peers.",
        "sentiment": 0.2929792118817568
    },
    "article_10": {
        "title": "VLC media player for macOS updated with full support for M1 Macs",
        "body": "VLC is one of the most popular multi-platform media players, and its macOS version is getting a major update today with full support for M1 Macs. Users can now enjoy VLC running at maximum performance on Apple Silicon Macs.\nHaving a Mac app compatible with the Apple Silicon platform means that the software can take full advantage of the new hardware with faster performance and also better energy efficiency, which is great for MacBook users.\nVLC 3.0.12 comes with a version ready for M1 Macs (and any future Macs with Apple Silicon chips). In addition, the upgrade also comes with enhancements to work properly on macOS Big Sur, a fix for audio distortions and adaptive streaming resolution, and security improvements.\nUnfortunately, VLC is not yet offered with a universal binary, which means that the app now has two different versions: one for Intel Macs and one for M1 Macs. Once you update the VLC app for macOS to version 3.0.12, you must check for updates again and install version 3.0.12.1 \u2014 which is compiled for ARM machines.\nVLC 3.0.12 is now out! Support for Apple Silicon (Mac M1) and Big Sur, improvements for DASH, RIST, Bluray support, fixes for macOS audio, Windows GPU, crashes and security issues.https://t.co/3zAr8VgzbL pic.twitter.com/TAU8ayKEBU \u2014 VideoLAN (@videolan) January 18, 2021\nVLC is available for free and you can get it through the official VideoLAN website. The mobile version of VLC for iOS is available for free on the App Store.\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": 0.18074067446746325
    },
    "article_11": {
        "title": "Apple Watch Series 7 Rumored to Feature Blood Glucose Monitoring",
        "body": "The Apple Watch Series 7 will reportedly feature blood glucose monitoring via an optical sensor, according to ETNews.\nThe report, which mainly focuses on the blood glucose capabilities of the Samsung Galaxy Watch 4, explains that Apple is intending to bring blood glucose monitoring to the upcoming Apple Watch Series 7 using a non-invasive optical sensor.\nMeasuring blood glucose levels, also known as blood sugar levels, is vital to managing conditions such as diabetes. Normally, measuring blood glucose requires testing a drop of blood in a blood sugar meter or using an implanted continuous glucose monitor (CGM). The ability to observe any major increases or decreases in blood glucose may raise awareness of a potential health condition or simply help to improve a user's diet.\nApple is said to have secured patents around blood glucose monitoring, and the company is now purportedly \"focusing on securing reliability and stability prior to commercialization of the technology.\" The Apple-designed optical sensor is believed to be a skin-top continuous monitoring solution that does not require an implant.\nRumors suggest that Apple has been interested in adding blood glucose monitoring to the Apple Watch for some time. The company reportedly established a team of biomedical engineers and consultants specifically working on sensors for non-invasively monitoring blood sugar levels in 2017, and work on the sensor reportedly progressed to trials at clinical sites in the San Francisco Bay Area. Apple CEO Tim Cook has even been spotted testing what was believed to be a prototype glucose monitor connected to his Apple Watch.\nApple has added new health-oriented features to the Apple Watch in recent years, such as the ability to measure blood oxygen levels or take an ECG. Late last year, \u200cTim Cook\u200c teased the future of the Apple Watch, saying that the device is still \"in the early innings,\" with Apple testing \"mind blowing\" capabilities in its labs. \"Think about the amount of sensors in your car,\" said Cook, adding \"and arguably, your body is much more important than your car.\"\nThe Apple Watch Series 7 is expected to arrive later this year, but there have been few rumors around what the new models may feature. While there have been reports of microLED displays and solid-state buttons with haptic feedback for the Apple Watch, these are not directly expected for the Apple Watch Series 7.",
        "sentiment": 0.0535595233241717
    },
    "article_12": {
        "title": "App privacy labels show stark contrasts among messaging apps",
        "body": "Apple\u2019s new app privacy labels went live in the App Store last month, giving users the chance to see what data is collected by each. We then explained how to view them.\nAll apps are required to show what data is used to track you, and what data is linked to your identity. Looking at that more comprehensive category reveals some stark differences between four popular messaging apps\u2026\nApp privacy labels for messaging apps\nForbes compared Signal, Apple\u2019s own iMessage, WhatsApp and Facebook Messenger.\nSignal\nNone. (The only personal data Signal stores is your phone number, and it makes no attempt to link that to your identity.)\niMessage\nEmail address\nPhone number\nSearch history\nDevice ID\nWhatsApp\nDevice ID\nUser ID\nAdvertising Data\nPurchase History\nCoarse Location\nPhone Number\nEmail Address\nContacts\nProduct Interaction\nCrash Data\nPerformance Data\nOther Diagnostic Data\nPayment Info\nCustomer Support\nProduct Interaction\nOther User Content\nFacebook Messenger\nApps have to show what data is used in what way \u2014 categorized by such things as third-party advertising and developer marketing. Some data is shown in more than one category, but it was easy enough to de-dupe them in the above apps. With Facebook Messenger, in contrast, the list is so long I have to list it in full.\nThird-Party Advertising\nPurchases\nPurchase History\nFinancial Info\nOther Financial Info\nLocation\nPrecise Location\nCoarse Location\nContact Info\nPhysical Address\nEmail Address\nName\nPhone Number\nOther User Contact Info\nContacts\nContacts\nUser Content\nPhotos or Videos\nGameplay Content\nOther User Content\nSearch History\nSearch History\nBrowsing History\nBrowsing History\nIdentifiers\nUser ID\nDevice ID\nUsage Data\nProduct Interaction\nAdvertising Data\nOther Usage Data\nDiagnostics\nCrash Data\nPerformance Data\nOther Diagnostic Data\nOther Data\nOther Data Types\nDeveloper\u2019s Advertising or Marketing\nPurchases\nPurchase History\nFinancial Info\nOther Financial Info\nLocation\nPrecise Location\nCoarse Location\nContact Info\nPhysical Address\nEmail Address\nName\nPhone Number\nOther User Contact Info\nContacts\nContacts\nUser Content\nPhotos or Videos\nGameplay Content\nOther User Content\nSearch History\nSearch History\nBrowsing History\nBrowsing History\nIdentifiers\nUser ID\nDevice ID\nUsage Data\nProduct Interaction\nAdvertising Data\nOther Usage Data\nDiagnostics\nCrash Data\nPerformance Data\nOther Diagnostic Data\nOther Data\nOther Data Types\nAnalytics\nHealth & Fitness\nHealth\nFitness\nPurchases\nPurchase History\nFinancial Info\nPayment Info\nOther Financial Info\nLocation\nPrecise Location\nCoarse Location\nContact Info\nPhysical Address\nEmail Address\nName\nPhone Number\nOther User Contact Info\nContacts\nContacts\nUser Content\nPhotos or Videos\nAudio Data\nGameplay Content\nCustomer Support\nOther User Content\nSearch History\nSearch History\nBrowsing History\nBrowsing History\nIdentifiers\nUser ID\nDevice ID\nUsage Data\nProduct Interaction\nAdvertising Data\nOther Usage Data\nSensitive Info\nSensitive Info\nDiagnostics\nCrash Data\nPerformance Data\nOther Diagnostic Data\nOther Data\nOther Data Types\nProduct Personalization\nPurchases\nPurchase History\nFinancial Info\nOther Financial Info\nLocation\nPrecise Location\nCoarse Location\nContact Info\nPhysical Address\nEmail Address\nName\nPhone Number\nOther User Contact Info\nContacts\nContacts\nUser Content\nPhotos or Videos\nGameplay Content\nOther User Content\nSearch History\nSearch History\nBrowsing History\nBrowsing History\nIdentifiers\nUser ID\nDevice ID\nUsage Data\nProduct Interaction\nAdvertising Data\nOther Usage Data\nSensitive Info\nSensitive Info\nDiagnostics\nCrash Data\nPerformance Data\nOther Diagnostic Data\nOther Data\nOther Data Types\nApp Functionality\nHealth & Fitness\nHealth\nFitness\nPurchases\nPurchase History\nFinancial Info\nPayment Info\nCredit Info\nOther Financial Info\nLocation\nPrecise Location\nCoarse Location\nContact Info\nPhysical Address\nEmail Address\nName\nPhone Number\nOther User Contact Info\nContacts\nContacts\nUser Content\nEmails or Text Messages\nPhotos or Videos\nAudio Data\nGameplay Content\nCustomer Support\nOther User Content\nSearch History\nSearch History\nBrowsing History\nBrowsing History\nIdentifiers\nUser ID\nDevice ID\nUsage Data\nProduct Interaction\nAdvertising Data\nOther Usage Data\nSensitive Info\nSensitive Info\nDiagnostics\nCrash Data\nPerformance Data\nOther Diagnostic Data\nOther Data\nOther Data Types\nOther Purposes\nPurchases\nPurchase History\nFinancial Info\nOther Financial Info\nLocation\nPrecise Location\nCoarse Location\nContact Info\nPhysical Address\nEmail Address\nName\nPhone Number\nOther User Contact Info\nContacts\nContacts\nUser Content\nPhotos or Videos\nGameplay Content\nCustomer Support\nOther User Content\nSearch History\nSearch History\nBrowsing History\nBrowsing History\nIdentifiers\nUser ID\nDevice ID\nUsage Data\nProduct Interaction\nAdvertising Data\nOther Usage Data\nDiagnostics\nCrash Data\nPerformance Data\nOther Diagnostic Data\nOther Data\nOther Data Types\nSome have suggested that Apple respond to Facebook\u2019s full-page newspaper ads with a response consisting solely of the above list.\nHow many people will actually read the app privacy labels, let alone have that influence their choice of messaging app, remains to be seen.\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.04175660186089002
    },
    "article_13": {
        "title": "Apple Watch credited with helping police locate kidnapped Texas woman",
        "body": "Police in Selma, Texas are crediting the Apple Watch with helping them locate a kidnapped woman. As reported by Fox San Antonio, the kidnapped woman used her Apple Watch to call for help, and police were able to use a cellular ping to track her.\nThe report describes the situation:\nWhen they arrived, officers spoke to a girl who told them her mother had been kidnapped. She said her mother and Adalberto Longoria were outside of an apartment arguing when the girl heard her mother scream. The girl told police she heard the screaming from the parking lot, but did not know where she was taken. Around 10 to 15 minutes later, the woman called the girl through her mobile watch, telling her Longoria had kidnapped her and wanted to hurt her. As she was speaking to her mom, the mobile watch was suddenly disconnected.\nOfficers were able to then use an \u201cemergency cellular ping\u201d to track the victim, the report explains. The Apple Watch was able to accurately show the victim\u2019s location, so when the police arrived, they found her in a car in a parking lot, with Longoria having fled on foot.\nThe report goes on:\nThe victim told police she and Longoria were fighting, and Longoria had refused to give up the vehicle. He told her to get her things out of the bed of the truck. When she went to do this, Longoria allegedly got in the driver\u2019s seat and drove away with her still in the bed of the truck. The victim told police he was allegedly drunk at the time.\nThis is certainly one of the more interesting Apple Watch stories we\u2019ve seen. The device\u2019s health features are often credited with helping save lives, but the always-on wearable clearly has other benefits as well. For example, the Apple Watch Emergency SOS feature has also been credited with helping locate missing and stranded people in the past.\nThe full report can be found at Fox 29 San Antonio.\nVia MacMagazine\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.3556444092506641
    },
    "article_14": {
        "title": "Apple's MacBook revival plan is stupid smart: Bring back old features",
        "body": "In his relentless quest to make MacBooks thinner and lighter and fully wireless, former Apple chief design officer Jony Ive reached into his soul and summoned up perhaps too much of the company\u2019s infamous \u201ccourage\u201d when he redesigned the MacBooks a half-decade ago.\nStarting with the 12-inch MacBook in 2015, Apple made some very questionable design choices when it walked back on everything that made MacBooks delightful, setting the notebooks on a painful march toward unprecedented criticism and failure.\nThe 12-inch MacBook was a harbinger of the troubles the MacBook lineup would face. Though svelte, the 2-pound laptop ditched the beloved MagSafe charger that was a staple on every MacBook before it and replaced it with USB-C charging. The laptop also brought with it Apple\u2019s universally hated butterfly keyboards.\nWhen it came time for the MacBook Pros the following year in 2016, not only did Apple bring these two \u201cfeatures\u201d over, it also replaced the perfectly fine function row/media keys with an unnecessary OLED Touch Bar that has proven to be less useful than originally advertised. More egregious to creative professionals like myself: Apple removed the SD card slot, which upset just about everyone. Why?\nAt the time, former (noticing a pattern yet?) senior vice president of worldwide marketing Phil Schiller (now Apple Fellow) told The Independent this bundle of spin:\nBecause of a couple of things. One, it\u2019s a bit of a cumbersome slot. You've got this thing sticking halfway out. Then there are very fine and fast USB card readers, and then you can use CompactFlash as well as SD. So we could never really resolve this \u2013 we picked SD because more consumer cameras have SD but you can only pick one. So, that was a bit of a trade-off. And then more and more cameras are starting to build wireless transfer into the camera. That\u2019s proving very useful. So we think there\u2019s a path forward where you can use a physical adaptor if you want, or do wireless transfer.\nI have no doubt Apple strongly believed wireless transfer on cameras would be the future. That or Apple drank its Kool-Aid so hard and truly thought there would be more people shooting with iPhones and iPads and AirDropping their footage over to MacBooks. (A great workflow for real-time news coverage that I think is greatly underutilized whenever I talk to journalism students who are fixated on expensive gear and antiquated workflows.)\nApple was ultimately wrong. As camera sensors have increased to accommodate higher resolutions and bitrates (8K, HDR, etc.), creatives like myself have found ourselves cornered by the lack of SD card slots on MacBooks. Nobody wants a dongle. They\u2019re also easy to forget to pack in a bag and use up a USB-C port.\nBut things are looking up for the MacBook\u2019s future. Reports from Bloomberg and well-connected TF International Securities analyst Ming-Chi Kuo (via MacRumors) both indicate that Apple will be correcting all of these backsteps on new 14 and 16-inch MacBook Pros launching later this year.\nBoth reports corroborate the same features on the upcoming Apple MacBook Pros powered by next-gen Apple Silicon:\nMagSafe charging is returning\nThere will be additional I/O ports\nThe Touch Bar will be replaced by a row of function keys\nOther details include a flat edge design (goodbye tapers) similar to the iPhone 12 (or more likely the iPad Pro / iPad Air 4) and brighter displays with more contrast likely using microLED technology.\nBut let\u2019s back up to those three bullet points. Holy shit. Apple is going to return to the very features it removed five years ago? That is seismic. Apple hates to admit any wrongdoing. But with Ive long gone and Schiller no longer leading marketing, Apple no longer has these old balls and chains weighing it down.\nThese new MacBook Pros look to be a much-needed return to function. Shutterstock\nSchiller had squashed all hope of the SD card returning when YouTuber Jonathan Morrisson asked him \u201cDo you think there\u2019s ever a chance that the SD card slot makes a comeback?\u201d following the launch of the 16-inch MacBook Pro, which was the first MacBook to ditch the butterfly keyboard for good ol\u2019 scissor-switch keys.\nSchiller said \u201cprobably not\u201d but added that \u201cnothing about the future is in stone and things can always change.\u201d\n\u201c[Apple] did a lot of soul searching on it and asking ourselves about a lot of connectors about USB-A about SD card readers, about HDMI and requestioning everything,\u201d he said. \u201cMore and more customers are taking advantage of USB-C and Thunderbolt, love the incredible headroom in performance there is there, the higher power there is, the charging ability there is, and so we think having on the highest-end notebook four USB-C/Thunderbolt ports gives the most headroom for the things you will be doing in the years ahead.\u201d\nThese new MacBook Pros look to be a much-needed return to function.\nSchiller admitted dongles were the tradeoff but still stuck to his guns that \u201cafter a lot of soul searching, we think we\u2019ve done the best thing for customers with the I/O we have, meaning we don\u2019t have every port type in the world on it.\u201d\nClearly, actual creatives and professionals disagree with Apple\u2019s soul-searching because if all of these rumors come to fruition, Apple will be returning to what was already considered the MacBook Pro\u2019s zenith. Coupled with Apple Silicon and Apple could experience Mac growth that it ceded to PC laptops during these past years of stumbling.\nAs someone totally in love with my M1 MacBook Air \u2014 the performance and battery life still astounds me every day \u2014 I am beyond excited for these new MacBook Pros. I\u2019ve missed MagSafe dearly, pleaded for Apple to kill the Touch Bar more times than I can count, and curse under my breath every time I can\u2019t find my SD card dongle for my MacBook.\nThese new MacBook Pros look to be a much-needed return to function. Now, add a touchscreen and don\u2019t forget Face ID, and then Windows laptops are really in trouble. Bring back the glowing Apple logo, too. That was hot.",
        "sentiment": -0.03190840466641912
    },
    "article_15": {
        "title": "Netflix reportedly testing AirPods spatial audio support - 9to5Mac",
        "body": "Netflix is reportedly quietly testing support for iOS 14\u2019s new spatial audio functionality. According to a report from iPhoneSoft, citing an anonymous Netflix developer, the company has been testing spatial audio support on iPhone and iPad since December.\nUpdate February 22, 2021: Netflix has given a statement to MacRumors saying it is not testing spatial audio:\nIn a statement to MacRumors, a Netflix spokesperson says that it\u2019s not currently testing spatial audio support, and has no plans to make public at this point in time. Netflix instead says it was testing multi-channel support for built-in speakers, as part of its mission to \u201cimprove\u201d its service, and evaluate \u201cnew experiences\u201d for users.\nThere is actually not much work that would be required on Netflix\u2019s end to support spatial audio. The service already offers a variety of surround sound content, which is what spatial audio converts into virtual surround sound. Many streaming services, including Apple TV+, Hulu, and Disney+ already support spatial audio.\nThe iPhoneSoft report doesn\u2019t include specific information on when Netflix will roll out spatial audio report, instead only vaguely suggesting a spring release with a \u201climited\u201d catalog.\nWhat is spatial audio?\nThe spatial audio feature is exclusive to AirPods Pro and AirPods Max, and the toggle is accessible via Control Center when your AirPods are connected.\nApple says that spatial audio is an immersive experience using directional audio filters to \u201cplay sounds virtually anywhere in space, creating an immersive sound experience.\u201d This will put surround channels exactly in the right spot, even as you turn your head or move your device.\nHere\u2019s how Apple describes the spatial audio feature for AirPods Pro and AirPods Max users:\nSpatial audio with dynamic head tracking brings the movie theater experience right to your AirPods Pro. By applying directional audio filters and subtly adjusting the frequencies each ear receives, spatial audio can place sounds virtually anywhere in space, creating an immersive surround sound experience. Using the gyroscope and accelerometer in your AirPods Pro and your iPhone, spatial audio tracks the motion of your head as well as your device, compares the motion data, and then remaps the sound field so that it stays anchored to your device even as your head moves.\nOne important thing to keep in mind is that only Netflix\u2019s highest-end Premium tier supports Dolby Atmos surround sound. It\u2019s possible that Netflix also limits Spatial Audio to the Premium subscribers too, but we don\u2019t yet know the details.\nIf you notice support for spatial audio with Netflix on your iPhone or iPad, be sure to let us know down in the comments or on Twitter @9to5Mac.\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.016933853505179286
    },
    "article_16": {
        "title": "Apple will let Amphetamine app stay in the App Store after wrongly telling developer it violated App Store rules",
        "body": "The developer of Amphetamine, an app that prevents Macs from going into sleep mode, says Apple told him it violated App Store guidelines, even though it\u2019s been in the App Store since 2014, and has nothing to do with drug use. Not long after The Verge reached out to Apple for comment on Saturday however, the company reversed its decision, and the app will be able to stay up with its current name and logo.\nWilliam C. Gustafson said in January 1st posts on Reddit and Github that Apple had informed him he had two weeks to \u201cremove all references to the word \u2018amphetamine\u2019 and remove the pill from the icon.\u201d If he failed to do so, Gustafson wrote, Apple said it would remove the app from the App Store on January 12th. The logo features a cartoon image of a pill.\nGustafson told The Verge he got a call Saturday from Apple granting his appeal\u2014 but he didn\u2019t have insight into how the app was flagged in the first place. \u201cI specifically asked Apple on the phone if this was a result of customer complaints and Apple\u2019s response was \u2018I don\u2019t think so,\u2019\u201d he said. \u201cI found it odd that this issue came up out of nowhere. I wasn\u2019t in the middle of trying to update Amphetamine or anything. Just sitting at home with my kids enjoying our holiday and got the violation/rejection email from Apple.\u201d\nGustafson says Apple contacted him on December 29th and told him Amphetamine \u201cappears to promote inappropriate use of controlled substances. Specifically, your app name and icon include references to controlled substances, pills.\u201d\nThe free macOS app has been downloaded more than 432,000 times, with a 4.8 rating, Gustafson said, noting that Apple even featured Amphetamine in an Mac App Store story. He said he had numerous interactions with Apple employees for updates to the app since its launch, with no one objecting to the name or logo until now.\nThe specific App Store guideline Gustafson was accused of violating is this one, which states \u201cApps that encourage consumption of tobacco or vape products, illegal drugs, or excessive amounts of alcohol are not permitted on the App Store. Apps that encourage minors to consume any of these substances will be rejected. Facilitating the sale of marijuana, tobacco, or controlled substances (except for licensed pharmacies) isn\u2019t allowed.\u201d\nGustafson says Amphetamine does none of these things, and said changing the name of the app would have wrecked its brand recognition and potentially made it harder for users to find future updates.\nGustafson initially said he didn\u2019t expect his appeal to be successful, and indeed, Apple typically hews pretty closely to its App Store rules in most cases. The company has faced pushback from developers on several fronts in recent months, with big industry companies including Spotify, Tile, and Epic Games forming a group called the Coalition for App Fairness. It says Apple\u2019s rules create an uneven playing field in its App Stores.",
        "sentiment": -0.544987824279815
    },
    "article_17": {
        "title": "Second report says iOS 15 to drop support for iPhone 6s, original iPhone SE, more",
        "body": "A new report today from iPhoneSoft claims to have details on what devices Apple plans to drop support with iOS 15. Notably, the information lines up with a preview report we saw at the end of last year suggesting the iPhone 6s, 6s Plus, original iPhone SE will be on the chopping block.\nFrench publication iPhoneSoft says that the information is coming from within Apple (via Apple Translate):\nHere is a first list of apple devices that will host iOS 15 beta next June, once again gleaned from our developer friend at Apple and who notably officiates on the Plans app.\nAt this point, it looks like Apple is planning to drop support for iPhones with A9 chips and earlier. While that might not sound like a big deal for many, the iPhone 6s has continued to be very popular in resale markets.\nThe iPhone 6s and iPhone 6s Plus originally launched in September 2015, with the iPhone SE landing in March 2016. These devices have been very popular and received over four years of software updates from Apple.\nNot compatible with iOS 15:\niPhone 6s an 6s Plus\niPhone SE 2016\nThat lines up with the report we saw last November from The Verifier. But going beyond that information, iPhoneSoft says that iOS 15 will also \u201cprobably not be available\u201d on:\niPad mini 4\niPad Air 2\niPad 5\nHere\u2019s a look at what devices could work with the iOS/iPadOS 15.\nLikely compatible with iOS 15:\n2021 iPhone lineup\niPhone 12 Pro Max\niPhone 12 Pro\niPhone 12 mini\niPhone 12\niPhone 11\niPhone 11 Pro\niPhone 11 Pro Max\niPhone XS\niPhone XS Max\niPhone XR\niPhone X\niPhone 8\niPhone 8 Plus\niPhone 7\niPhone 7 Plus\niPhone SE (2nd generation)\niPod touch (7th generation)\nand for iPad:\n2021 iPad lineup\n12.9-inch iPad Pro\n9.7, 10.5, and 11-inch iPad Pro\niPad Air 3\niPad Air 4\niPad 6/7/8\niPad mini 5\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.10516282662071964
    },
    "article_18": {
        "title": "Apple Car team adds more ex-Tesla executives, release at least 5-7 years away",
        "body": "A new report from Bloomberg today offers new details on Apple\u2019s ongoing efforts to build a self-driving electric car. According to the report, Apple has a \u201csmall team\u201d working on Apple Car, but a release is likely at least five to seven years away. The Apple Car team has also added even more former Tesla executives, the report says.\nThe report explains:\nThe Cupertino, California-based technology giant has a small team of hardware engineers developing drive systems, vehicle interior and external car body designs with the goal of eventually shipping a vehicle. That\u2019s a more ambitious goal than in previous years when the project mostly focused on creating an underlying self-driving system. The company has also added more ex-Tesla Inc. executives to the project.\nWork on the Apple Car project has been delayed this year because of the COVID-19 pandemic, the report says. Currently, \u201cthe majority of the team is currently either working from home or at the office for limited time\u201d because of the pandemic. This has caused Apple\u2019s work on the project to slow down.\nEngineers working on the project believe that a product could be released \u201cin five to seven years if Apple goes ahead with its plans.\u201d Ultimately, however, the car is said to be \u201cnowhere near production stage\u201d and \u201ctimelines could change.\u201d\nApple Car and Tesla\nBloomberg also has details on some recent hires for the Apple Car team, including former Tesla executives. Jonathan Sive, formerly a vehicle engineer for Tesla, BMW, and Waymo, serves as a senior manager on the Apple Car team. And Apple hired former Tesla vice president Stuart Bowers in late 2020.\nThe report says that all-in-all, Apple\u2019s electric car team is \u201cfilled with dozens\u201d of ex-Tesla hardware and software engineers. Apple has \u201cseveral hundred\u201d engineers currently working on the project altogether.\nApple also recently hired Jonathan Sive, a vehicle engineer from BMW AG, Tesla and Alphabet Inc.\u2019s Waymo, as a senior manager on the car project. In 2019, Apple tapped Michael Schwekutsch, Tesla\u2019s former vice president in charge of drive systems, adding to a growing list of former Tesla employees working on the vehicle effort. Late in 2020, Apple also hired another former Tesla vice president, Stuart Bowers, according to a person familiar with the move. He led Tesla\u2019s self-driving technology team until mid-2019 and was an executive-in-residence at venture capital firm Greylock Partners until July, according to his LinkedIn profile.\nAdditionally, Michael Schwekutsch joined Apple\u2019s engineering team in 2019 after previously serving as Tesla\u2019s VP of engineering. Apple has made a habit of hiring former Tesla employees for its self-driving car team. This has led to Tesla CEO Elon Musk referring to Apple as the \u201cTesla graveyard.\u201d\nMusk also recently revealed that he looked to sell Tesla to Apple for a fraction of its current value. Musk claims to have reached out directly to Apple CEO Tim Cook, but Cook wasn\u2019t interested in taking a meeting.\nFinally, the report says that Apple has a small hardware team working on \u201cvehicle dynamics, drive trains, safety mechanics and battery technology.\u201d The goal is to \u201cre-imagine a car\u2019s interior for a future in which people ride passively rather than steer.\u201d Apple\u2019s chip team is also working on processors to power the car\u2019s self-driving system.\nToday\u2019s report from Bloomberg follows earlier reporting from Reuters, who said Apple Car could enter production by 2024. Reliable Apple analyst Ming-Chi Kuo, however, has said that the Apple Car launch could be 2028 or later depending on a variety of factors.\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": 0.08088880762807094
    },
    "article_19": {
        "title": "iOS 14.4 Will Introduce Warning on iPhones With Non-Genuine Cameras",
        "body": "In the second beta of iOS 14.4 seeded to developers and public testers this week, MacRumors contributor Steve Moser has discovered code indicating that Apple will be introducing a new warning on iPhones that have had their camera repaired or replaced with aftermarket components rather than genuine Apple components.\n\"Unable to verify this iPhone has a genuine Apple camera,\" the message will read. As with a similar warning for non-genuine iPhone displays, the message will likely appear in the Settings app under General > About as necessary, and the warning may also appear as a notification on the Lock screen for a short period.\nApple's similar display-related warning does not affect the ability to use the iPhone or display in any way, so this will presumably be the case with the camera-related warning as well, but it remains to be seen. Apple also displays a similar message for non-genuine iPhone batteries that likewise does not impact use of the device beyond disabling a battery health feature that displays the battery's maximum capacity remaining.\nThe non-genuine camera message will likely direct users towards a future Apple support document emphasizing the importance of iPhone repairs being completed by authorized, trained technicians using genuine Apple parts, including Apple, Apple Authorized Service Providers, and those part of Apple's Independent Repair Provider Program.\nLast year, repair website iFixit reported that authorized technicians are required to run Apple's proprietary, cloud-based System Configuration app to complete camera and display repairs on all iPhone 12 models, citing internal Apple documentation. Failure to complete this step can result in the camera experiencing issues or becoming completely unresponsive in a short amount of time, according to iFixit's testing.\nUpdate: Apple has publicly confirmed this feature in the iOS 14.4 Release Candidate notes (\"Notifications for when the camera on your \u200ciPhone\u200c is unable to be verified as a new, genuine Apple camera in \u200ciPhone 12\u200c, iPhone 12 mini, iPhone 12 Pro and iPhone 12 Pro Max\").",
        "sentiment": -0.11136786225769255
    },
    "article_20": {
        "title": "Brad Cox creator of Objective-C passed away",
        "body": "BRAD COX OBITUARY\nDr. Brad J.\nCox Ph.D\nDr. Brad Cox, Ph.D of Manassas, Virginia, died on January 2, 2021 at his residence. Dr. Cox was a computer scientist known mostly for creating the Objective \u2013 C programming language with his business partner, Tom Love, and for his work in software engineering (specifically software reuse) and software componentry. Brad was born on May 2, 1944 in Fort Benning, Georgia, to the late Nancy Hinson Cox and Dewey McBride Cox of Lake City, South Carolina. Brad grew up on the family's dairy farm in South Carolina but found himself most interested in science. After graduating from Lake City High School, he received his Bachelor of Science Degree in Organic Chemistry and Mathematics from Furman University, and his Ph.D. from the Department of Mathematical Biology at the University of Chicago, and worked on an early form of neural networks. He soon found himself more interested in computers and got a job at International Telephone and Telegraph (ITT) and later joined Schlumbeger \u2013 Doll Research Labs, and ultimately formed his own Connecticut startup, Productivity Products International (PPI) later named Stepstone. Among his first known software projects, he wrote a PDP-8 program for simulating clusters of neurons. He worked at the National Institutes of Health and Woods Hole Oceanographic Institute before moving into the software profession. Dr. Cox was an entrepreneur, having founded the Stepstone Company together with Tom Love for releasing the first Objective-C implementation. Stepstone hoped to sell \"ICPaks\" and Dr. Cox focused on building his ICPak libraries and hired a team to continue work on Objective-C, including Steve Naroff. The late Steve Jobs', NeXT, licensed the Objective-C language for it's new operating system, NEXTSTEP. NeXT eventually acquired Objective- C from Stepstone. Objective-C continued to be the primary programming language for writing software for Apple's OS X and iOS.\nDr. Cox won a Paul Allen Distance Education Award in 1998 for his online course, \"Taming the Electronic Frontier\". In 1991, Dr. Cox published his book, Object Oriented Programming: an Evolutionary Approach and in 1996 published Superdistribution: Objects as Property on the Electronic Frontier which was translated into 10 different languages.\nDr. Cox joined George Mason University's Program on Social and Organizational Learning, developing early online courses over the internet. After leaving the academia, Dr. Cox began a career in government consulting which included assignments with Boeing and at the Pentagon. Ultimately, Dr. Cox returned to his neural net roots and worked in applying machine learning and data science to cybersecurity.\nDr. Cox was sought- after and traveled Europe extensively lecturing, making speeches and demonstrating how to program software. He and his wife, Etta, enjoyed traveling for leisure, as well, and visited the Caribbean often as they both enjoyed scuba diving. Belize especially held fond memories for them. On one scuba diving excursion while in the compound having lunch, Brad engaged a couple from Germany in conversation. Brad asked about the fellow travelers occupation and discovered he was a computer programmer. Lifewise, Brad was asked about his life's work and stated I am also a computer programmer. \"What do you do?\" Brad was asked. I wrote Objective-C. Astonished, the gentlerman said, \"No, Brad Cox wrote that\". \"Hi, I am Brad Cox\", was the response and the introduction. Needless to say, much conversation ensued after the scuba diving concluded. Throughout Brad's life and career, countless instances such as this one occurred repeatedly. One of Brad's mothers favorite stories to tell was about her accompanying them on one of their trips to Belize and how much she enjoyed staying on the yacht. The delectable cuisine was much to anticipate. Her interaction with the chef was most entertaining and his final presentation was most palatable and much admired. Getting to know the captain as he safely navigated them from one beautiful destination to another was most comforting in light of his calm and charismatic personality and calmed whatever fear or anxiety she may have possessed. Memories of the Belize trip she cherished until her death at the age of 98. She was very proud of her son and all of his accomplishments.\nBrad enjoyed music and played the piano and the quitar. In earlier years he was a member of a band which played mostly blue grass music which was his favorite. He enjoyed communing with nature and taking long walks in the woods was to his delight. He had a wonderful sense of humor.\nDr. Cox was predeceased by his parents, Nancy and Dewey Cox of Lake City. He is survived by his wife of 44 years, Etta Glenn of Manassas, Virginia. Also, his brother, Dan (Donna) Cox, nephews Neil (Wendy) Cox and Chad (Danielle) Cox and 12 great nieces and nephews. Brooklyn, Daniel, Dixie, Ryan, Kyle, Manning, Whitt, Lacey, Eli, Tatum, Harper, and Kingston Cox, all of Lake City, South Carolina.\nA memorial service in celebration of his life is planned for Wednesday, January 13, 2021 at 11:00 am at Cornerstone Ministries, 1900 New Zion Rd. Lake City SC, 29560.\nPublished by SCNow on Jan. 8, 2021.",
        "sentiment": 0.6905247601954376
    },
    "article_21": {
        "title": "Apple Has Threatened To Ban Parler From The App Store",
        "body": "On Parler, CEO John Matze struck a defiant tone. \u201cWe will not cave to pressure from anti-competitive actors! We will and always have enforced our rules against violence and illegal activity. But we WONT cave to politically motivated companies and those authoritarians who hate free speech!\u201d he wrote in a message.\n\u201cAnyone who buys an Apple phone is apparently a user. Apperently they know what is best for you by telling you which apps you may and may not use,\u201d he added.\nLaunched in 2018, Parler has become a safe haven for people that have been banned or suspended by popular social networks including Facebook and Twitter for violating those platforms' rules. The Henderson, Nevada\u2013based company has billed itself as a free speech alternative to mainstream social networks and taken a more relaxed approach to content moderation, attracting conspiracy theorists, members of hate groups, and right-wing activists who have openly incited violence. Recent threads on Parler, for example, have called for the execution of Vice President Mike Pence, and encouraged the conspiracy theory that left-wing antifa activists were behind Wednesday\u2019s events.\nRepublican lawmakers including Sen. Ted Cruz and House Minority Leader Kevin McCarthy as well as President Donald Trump\u2019s family members and surrogates have all established accounts on Parler, and have urged their followers on other social media sites to migrate there.\n\u201cContent of this dangerous and harmful nature is not appropriate for the App Store. As you know from prior conversations with App Review, Apple requires apps with user generated content to effectively moderate to ensure objectionable, potentially harmful content is filtered out,\u201d the iPhone maker wrote in its letter. \u201cContent that threatens the well being of others or is intended to incite violence or other lawless acts has never been acceptable on the App Store.\u201d\nThe full text of the letter follows:",
        "sentiment": -0.06652197043100992
    },
    "article_22": {
        "title": "Fitness+ Subscribers Now Have Nearly 300 Workouts to Choose From",
        "body": "Apple on Monday added over two dozen new video workout sessions to Fitness+, continuing a recent trend of building out the catalog of its fledgling subscription service on a weekly basis.\nFitness+ launched December 14, 2020, and is designed to help Apple Watch owners keep fit through a series of guided workouts that are available across multiple workout categories. As you follow along with Fitness+ routines, the Apple Watch tracks your movement, workout length, calories burned, heart rate, and more, just like other workouts.\nIn the latest motivational introductory video, trainer Bakari introduces the new workouts like so:\nHere at Fitness+ more than two dozen workouts just dropped, like Sam's 30-minute treadmill which challenges your endurance, whether you're walking or running. For a workout that needs no equipment at all, I teach 10-minute hit with four total body moves. Each move is inspired by different sports like basketball and soccer, which go perfectly with the song \"No Hands\" by Waka Flocka Flame. That's what's new at Fitness+ to help you close your rings. Let's go!\nIn all, Apple has added 26 new Fitness+ workouts covering all video categories, which include cycling, strength, yoga, HIIT, core, dance, treadmill, mindful cool down, and rowing. That takes the total number of workouts in the Fitness+ catalog to 293 so far.\nFitness+ costs $9.99 per month or $79.99 per year, which breaks down to $6.67 per month. For that price, up to six family members total can use the Fitness+ service. Fitness+ is also included in the Apple One Premier bundle, which is priced at $29.99 per month and also offers \u200c\u200cApple Music\u200c\u200c, Apple TV+, Apple Arcade, Apple News+, and 2TB iCloud storage.",
        "sentiment": 0.722542577638076
    },
    "article_23": {
        "title": "Apple Patent for Watch with Light Field Camera - \"Vein Print Unlock\"",
        "body": "Patent Application Summary\nU.S. patent application number 17/024574 was filed with the patent office on 2021-01-07 for wearable electronic device having a light field camera. The applicant listed for this patent is Apple Inc.. Invention is credited to Giovanni Gozzini, Dale Setlak, Manohar B. Srikanth, Mohammad Yeke Yazdandoost.\nApplication Number 20210004444 17/024574 Document ID / Family ID Filed Date 2021-01-07\nUnited States Patent Application 20210004444 Kind Code A1 Setlak; Dale ; et al. January 7, 2021\nWearable Electronic Device Having a Light Field Camera\nA method of authenticating a user of a wearable electronic device includes emitting light into a dorsal side of a forearm near a wrist of the user; receiving, using a light field camera, remissions of the light from the dorsal side of the forearm near the wrist of the user; generating a light field image from the remissions of the light; performing a synthetic focusing operation on the light field image to construct at least one image of at least one layer of the forearm near the wrist; extracting a set of features from the at least one image; determining whether the set of features matches a reference set of features; and authenticating the user based on the matching. In some embodiments, the method may further include compensating for a tilt of the light field camera prior to or while performing the synthetic focusing operation.\nInventors: Setlak; Dale; (Merritt Island, FL) ; Gozzini; Giovanni; (Berkeley, CA) ; Srikanth; Manohar B.; (Mountain View, CA) ; Yeke Yazdandoost; Mohammad; (Santa Clara, CA) Applicant: Name City State Country Type\nApple Inc.\nCupertino\nCA\nUS Appl. No.: 17/024574 Filed: September 17, 2020\nApplication Number Filing Date Patent Number 16132241 Sep 14, 2018 10817594 17024574 62564916 Sep 28, 2017\nCurrent U.S. Class: 1/1 International Class: G06F 21/32 20060101 G06F021/32; G06F 1/16 20060101 G06F001/16; G06K 9/00 20060101 G06K009/00; G06K 9/20 20060101 G06K009/20\n1. A watch body, comprising: a housing; a cover mounted to the housing, the cover having: a first surface exterior to the watch body; and a second surface interior to the watch body; a light emitter positioned to emit light through the cover into a dorsal side of a forearm near a wrist of a user when the first surface of the cover is positioned adjacent the dorsal side of the forearm near the wrist of the user; a light field camera positioned adjacent the second surface to receive remissions of the light through the cover from the dorsal side of the forearm near the wrist; and a processor configured to operate the light emitter and the light field camera, obtain a light field image from the light field camera, and perform a synthetic focusing operation on the light field image to construct at least one image of at least one layer of the forearm near the wrist.2. The watch body of claim 1, wherein the light field camera comprises: an array of non-overlapping image sensing regions; a pinhole mask positioned between the array of non-overlapping image sensing regions and the second surface of the cover; and a spacer between the array of non-overlapping image sensing regions and the pinhole mask; wherein, the array of non-overlapping image sensing regions is aligned with an array of pinholes in the pinhole mask to form a set of micro-cameras having fields of view that overlap exterior to the watch body; and the processor is further configured to extract a set of features from the at least one image, determine whether the set of features matches a reference set of features, and perform an operation in response to whether the set of features matches the reference set of features.3. The watch body of claim 1, wherein the processor is further configured to extract a set of features from the at least one image, determine whether the set of features matches a reference set of features, and perform an operation in response to whether the set of features matches the reference set of features.4. The watch body of claim 3, wherein the operation comprises a bioauthentication operation.5. The watch body of claim 1, wherein the light field camera comprises: a first array of non-overlapping image sensing regions; and a second array of light-transmissive elements positioned between the first array and the second surface of the cover; wherein, the first array is aligned with the second array to form a set of micro-cameras having fields of view that overlap exterior to the watch body.6. The watch body of claim 5, wherein the non-overlapping image sensing regions comprise discrete image sensors.7. The watch body of claim 5, wherein the non-overlapping image sensing regions comprise regions of a single image sensor.8. The watch body of claim 5, wherein the light-transmissive elements comprise lenses.9. The watch body of claim 5, further comprising: a pinhole mask positioned between the first array and the second surface of the cover; wherein, the light-transmissive elements comprise pinholes defined by the pinhole mask.10. The watch body of claim 5, wherein the light field camera further comprises: a spacer between the first array and the second array.11. The watch body of claim 10, wherein the spacer comprises a layer of glass.12. The watch body of claim 1, further comprising: a set of light emitters including the light emitter, wherein the light emitters are positioned to emit light from around the light field camera.13. The watch body of claim 1, further comprising: a set of light emitters including the light emitter; wherein, the light field camera comprises a set of micro-cameras having fields of view that overlap exterior to the watch body; and the set of light emitters is positioned to emit light from within a boundary defined by the set of micro-cameras.14. The watch body of claim 1, wherein the processor is further configured to: determine, using the light field camera, a tilt of the light field camera with respect to the dorsal side of the forearm near the wrist; and compensate for the determined tilt prior to or while performing the synthetic focusing operation.15. The watch body of claim 1, wherein the light emitter comprises an infrared emitter.16. A watch body, comprising: a housing; a cover mounted to the housing, the cover having: a first surface exterior to the watch body; and a second surface interior to the watch body; a light emitter positioned to emit light through the cover into a dorsal side of a forearm near a wrist of a user when the first surface of the cover is positioned adjacent the dorsal side of the forearm near the wrist; a light field camera positioned adjacent the second surface to receive remissions of the light through the cover from the dorsal side of the forearm near the wrist; and a tilt sensor configured to detect a tilt of the light field camera with respect to the dorsal side of the forearm near the wrist.17. The watch body of claim 16, wherein the tilt sensor comprises a set of proximity sensors.18. The watch body of claim 17, wherein the set of proximity sensors comprises at least one capacitive sensor.19. The watch body of claim 17, wherein the set of proximity sensors comprises at least one optical sensor.20. A method of authenticating a user of a wearable electronic device, comprising: emitting light into a dorsal side of a forearm near a wrist of the user; receiving, using a light field camera, remissions of the light from the dorsal side of the forearm near the wrist of the user; generating a light field image from the remissions of the light; performing a synthetic focusing operation on the light field image to construct at least one image of at least one layer of the forearm near the wrist; extracting a set of features from the at least one image; determining whether the set of features matches a reference set of features; and authenticating the user in response to the set of features matching the reference set of features.CROSS REFERENCE TO RELATED APPLICATIONS[0001] This application is a continuation of U.S. patent application Ser. No. 16/132,241, filed Sep. 14, 2018, which is a nonprovisional of and claims the benefit under 35 U.S.C. .sctn. 119(e) of U.S. Provisional Patent Application No. 62/564,916, filed on Sep. 28, 2017, the contents of which are incorporated by reference as if fully disclosed herein.FIELD[0002] The described embodiments relate generally to a watch or other electronic device (e.g., another type of wearable electronic device). More particularly, the described embodiments relate to a wearable electronic device having a light field camera, and to techniques for performing bioauthentication of a user of the device from a dorsal side of a forearm near a wrist of the user.BACKGROUND[0003] An electronic device may include a fingerprint sensor, a facial recognition sensor, a retina scanner, or other form of bioauthentication sensor. In some devices, such as a phone or tablet computer, a bioauthentication sensor may be provided adjacent (or as part of) a display of the device. However, in a wearable electronic device such as a watch, there may be little or no room for providing a bioauthentication sensor adjacent (or as part of) a display of the device. User authentication may therefore be provided by means of a password or similar input.SUMMARY[0004] Embodiments of the systems, devices, methods, and apparatus described in the present disclosure are directed to a watch or other electronic device (e.g., another type of wearable electronic device) having a light field camera. The light field camera may be used to image a forearm near a wrist of a user. The imaging may be performed from a dorsal side of the forearm. A synthetic focusing operation may be performed on a light field image obtained from the light field camera, to construct at least one image of at least one layer of the forearm near the wrist. A set of features of the forearm near the wrist may be extracted from the at least one image and compared to a reference set of features (e.g., a hair follicle pattern, a vascular pattern, a vein pattern, an artery pattern, a blood perfusion pattern in skin, a blood perfusion pattern in tendons, a blood perfusion pattern in fascia, a tendon pattern, a connective tissue pattern, a skin pigmentation pattern, a pore pattern, and/or a bone shape pattern obtained during a bioauthentication enrollment process performed for the user). An operation (e.g., a bioauthentication operation, a bioauthentication enrollment operation, a secure transaction operation, a health monitoring operation, or a health assessment operation) may be performed in response to whether the set of features matches the reference set of features. In some embodiments, a tilt of the light field camera with respect to the dorsal side of the forearm may be determined and compensated for, prior to or while performing the synthetic focus operation.[0005] In a first aspect, the present disclosure describes a watch body. The watch body includes a housing, a cover mounted to the housing, a light emitter, a light field camera, and a processor. The cover has a first surface exterior to the watch body, and a second surface interior to the watch body. The light emitter is positioned to emit light through the cover into a dorsal side of a forearm near a wrist of a user when the first surface of the cover is positioned adjacent the dorsal side of the forearm near the wrist of the user. The light field camera is positioned adjacent the second surface to receive remissions of the light through the cover from the dorsal side of the forearm near the wrist. The processor is configured to operate the light emitter and the light field camera, obtain a light field image from the light field camera, and perform a synthetic focusing operation on the light field image to construct at least one image of at least one layer of the forearm near the wrist.[0006] In another aspect, the present disclosure describes another watch body. The watch body has a housing, a cover mounted to the housing, a light emitter, a light field camera, and a tilt sensor. The cover has a first surface exterior to the watch body, and a second surface interior to the watch body. The light emitter is positioned to emit light through the cover into a dorsal side of a forearm near a wrist of a user when the first surface of the cover is positioned adjacent the dorsal side of the forearm near the wrist. The light field camera is positioned adjacent the second surface to receive remissions of the light through the cover from the dorsal side of the forearm near the wrist. The tilt sensor is configured to detect a tilt of the light field camera with respect to the dorsal side of the forearm near the wrist.[0007] In still another aspect of the disclosure, a method of authenticating a user of a wearable electronic device is described. The method includes emitting light into a dorsal side of a forearm near a wrist of the user; receiving, using a light field camera, remissions of the light from the dorsal side of the forearm near the wrist of the user; generating a light field image from the remissions of the light; performing a synthetic focusing operation on the light field image to construct at least one image of at least one layer of the forearm near the wrist; extracting a set of features from the at least one image; determining whether the set of features matches a reference set of features; and authenticating the user in response to the set of features matching the reference set of features.[0008] In addition to the exemplary aspects and embodiments described above, further aspects and embodiments will become apparent by reference to the drawings and by study of the following description.BRIEF DESCRIPTION OF THE DRAWINGS[0009] The disclosure will be readily understood by the following detailed description in conjunction with the accompanying drawings, wherein like reference numerals designate like structural elements, and in which:[0010] FIGS. 1 and 2 show an example of a watch including a biosensor system;[0011] FIG. 3 shows the watch of FIGS. 1 and 2 attached to a forearm of a user near the user's wrist;[0012] FIGS. 4 and 5 show various features that may be identified in a set of one or more layer images constructed from a light field image obtained from a dorsal side of a forearm of a user near the user's wrist;[0013] FIGS. 6-8 show a set of components that may be included in a biosensor system such as the biosensor system shown in FIGS. 2 and 3;[0014] FIG. 9 shows the conical fields of view of some of the micro-cameras shown in FIGS. 6-8, as well as the feature of the forearm shown in FIG. 8;[0015] FIG. 10 shows an exploded view of another set of components that may be included in a biosensor system such as the biosensor system shown in FIGS. 2 and 3;[0016] FIG. 11 shows an exploded view of another set of components that may be included in a biosensor system such as the biosensor system shown in FIGS. 2 and 3;[0017] FIG. 12 shows a watch attached to, and tilted on, a forearm of a user near the user's wrist;[0018] FIG. 13 shows an alternative embodiment of the watch described with reference to FIGS. 1-3, in which a tilt sensor is included in the watch;[0019] FIG. 14 shows the conical fields of view of some of the micro-cameras shown in FIGS. 6-8, as well as the feature of the forearm shown in FIG. 8;[0020] FIG. 15 shows a sample electrical block diagram of an electronic device such as a watch or other wearable electronic device; and[0021] FIG. 16 shows an example method of authenticating a user of a watch or other wearable electronic device.[0022] The use of cross-hatching or shading in the accompanying figures is generally provided to clarify the boundaries between adjacent elements and also to facilitate legibility of the figures. Accordingly, neither the presence nor the absence of cross-hatching or shading conveys or indicates any preference or requirement for particular materials, material properties, element proportions, element dimensions, commonalities of similarly illustrated elements, or any other characteristic, attribute, or property for any element illustrated in the accompanying figures.[0023] Additionally, it should be understood that the proportions and dimensions (either relative or absolute) of the various features and elements (and collections and groupings thereof) and the boundaries, separations, and positional relationships presented therebetween, are provided in the accompanying figures merely to facilitate an understanding of the various embodiments described herein and, accordingly, may not necessarily be presented or illustrated to scale, and are not intended to indicate any preference or requirement for an illustrated embodiment to the exclusion of embodiments described with reference thereto.DETAILED DESCRIPTION[0024] Reference will now be made in detail to representative embodiments illustrated in the accompanying drawings. It should be understood that the following description is not intended to limit the embodiments to one preferred embodiment. To the contrary, it is intended to cover alternatives, modifications, and equivalents as can be included within the spirit and scope of the described embodiments as defined by the appended claims.[0025] As previously mentioned, a wearable electronic device such as a watch may have little or no room for providing a biometric authentication (i.e., bioauthentication) sensor adjacent (or as part of) a display of the device. In a watch, a sensor can obtain measurements of biological parameters from the backside of the watch. However, bioauthentication via a backside sensor is complicated by the pattern, type, and density of a user's arm hair, which tends to be longer and more dense on the dorsal side of the forearm where a backside of a watch normally sits. Hair also tends to have a highly variable position, which can make it difficult to use for purposes of bioauthentication.[0026] One type of sensor that could be used for bioauthentication via the backside of a watch is a vein pattern sensor (e.g., a camera). However, veins and arteries and other significant structures of the forearm near the wrist tend to be deep beneath the surface (skin) of the forearm. In some cases, light may emitted into a user's skin via the backside of a watch, and remitted (e.g., reflected or refracted) from features within a user's forearm near their wrist. However, light remitted from features deep within the forearm can be blurred by scattering as it propagates toward the surface of the skin Remitted light may also be scattered by arm hair. Depending on the user, a typical image sensor (or camera) positioned on the backside of a watch may receive little remitted light, and/or a processor may be unable to identify features of a user's forearm based on the light remitted from features of the forearm.[0027] Another issue with performing bioauthentication using a sensor on the backside of a watch is that the footprint of the sensing area may be limited by the dimensions of the watch body, which can restrict the amount of data that an image sensor is able to acquire.[0028] Wearable electronic devices and techniques described herein utilize a biosensor system including a light field camera. In such a system, light, such as infrared light, is emitted deep within a user's forearm. Remission of the light is captured by a light field camera. A light field camera is a camera that not only captures the intensity of received light, but the intensities of light received in particular light rays (or small sets of light rays) received from different directions. In some embodiments, a light field camera may include an array of micro-cameras, with each micro-camera having an image sensing region, which image sensing region has a limited field of view defined by a pinhole or microlens. The pinhole or microlens limits the image sensing region's field of view to a set of light rays that passes through the same pinhole or microlens, but from different directions. Thus, each pixel value of the image sensing region can be associated with directional information based on the relationship of the pinhole or microlens to particular pixels of the image sensing region. A light field image may include a set of images acquired by the different micro-cameras in the light field camera (or a set of images acquired by a subset of the micro-cameras). The fields of view (or targets) of each micro-camera overlap, and the resolution of a light field camera can be increased by configuring the fields of view to have significant overlap (e.g., in some embodiments, there may be a 50%, 75%, 90%, or greater overlap between the fields of view of adjacent micro-cameras).[0029] A synthetic focusing operation (e.g., a tomographic-style focusing operation) may be performed on a light field image. The synthetic focusing operation may constructively and deconstructively combine the pixel values of different images (e.g., elemental images) within a light field image, to construct images of layers at different distances from the sensing plane (i.e., images of different layers of a user's forearm). A variety of exemplary synthetic focusing operations are well-known in the art and may be based on shifting, scaling, adding, subtracting, and averaging pixel values. A processor may therefore extract, from the images of one or more layers constructed during a synthetic focusing operation, a set of features of a user's forearm. The set of features may include, for example, a hair follicle opening pattern, a hair follicle pattern, a vascular pattern, a vein pattern, an artery pattern, a blood perfusion pattern in skin, a blood perfusion pattern in tendons, a blood perfusion pattern in fascia, a tendon pattern, a connective tissue pattern, a skin pigmentation pattern, a small scale folding pattern of skin, a pore opening pattern, a pore pattern, and/or a bone shape pattern.[0030] Synthetic focusing performed on a light field image may not only be used to extract features of a user's forearm from the light field image, may be used to classify the features based on size, depth, color, movement, variance between layers, existence in one or more layers, relationships with other features (e.g., relationships based on size, depth, color, or movement with respect to other features), and so on. Although synthetic focusing may not fully remove the blurring caused by turbid biological materials and light scattering as remitted light propagates toward a user's skin from deeply located structures, synthetic focusing may de-emphasize features/patterns located in front of other features/patterns, which close features/patterns might otherwise obscure deeper features/patterns.[0031] In some embodiments, a processor that performs a synthetic focusing operation may utilize a hardware accelerator to perform part or all of the synthetic focusing operation.[0032] These and other embodiments are discussed with reference to FIGS. 1-16. However, those skilled in the art will readily appreciate that the detailed description given herein with respect to these figures is for explanatory purposes only and should not be construed as limiting.[0033] The present disclosure recognizes that personal information data, including the biometric data acquired using the presently described technology, can be used to the benefit of users. For example, the use of biometric authentication data can be used for convenient access to device features without the use of passwords. In other examples, user biometric data is collected for providing users with feedback about their health or fitness levels. Further, other uses for personal information data, including biometric data, that benefit the user are also contemplated by the present disclosure.[0034] The present disclosure further contemplates that the entities responsible for the collection, analysis, disclosure, transfer, storage, or other use of such personal information data will comply with well-established privacy policies and/or privacy practices. In particular, such entities should implement and consistently use privacy policies and practices that are generally recognized as meeting or exceeding industry or governmental requirements for maintaining personal information data private and secure, including the use of data encryption and security methods that meets or exceeds industry or government standards. For example, personal information from users should be collected for legitimate and reasonable uses of the entity and not shared or sold outside of those legitimate uses. Further, such collection should occur only after receiving the informed consent of the users. Additionally, such entities would take any needed steps for safeguarding and securing access to such personal information data and ensuring that others with access to the personal information data adhere to their privacy policies and procedures. Further, such entities can subject themselves to evaluation by third parties to certify their adherence to widely accepted privacy policies and practices.[0035] Despite the foregoing, the present disclosure also contemplates embodiments in which users selectively block the use of, or access to, personal information data, including biometric data. That is, the present disclosure contemplates that hardware and/or software elements can be provided to prevent or block access to such personal information data. For example, in the case of biometric authentication methods, the present technology can be configured to allow users to optionally bypass biometric authentication steps by providing secure information such as passwords, personal identification numbers (PINS), touch gestures, or other authentication methods, alone or in combination, known to those of skill in the art. In another example, users can select to remove, disable, or restrict access to certain health-related applications collecting users' personal health or fitness data.[0036] FIGS. 1 and 2 show an example of a watch 100 including a biosensor system 116. The watch 100 may include a watch body 102 and a watch band 104. The watch 100 and watch body 102 are examples of wearable electronic devices. Other wearable electronic devices that may include a biosensor system, such as one of the biosensor systems described in the present disclosure, include health monitoring or fitness devices, portable computing devices, mobile phones (including smart phones), or digital media players. In some embodiments, the functions of some or all of these other wearable electronic devices may be incorporated into a watch, such as the watch 100.[0037] The watch body 102 may include a housing 106. The housing 106 may include one or more housing members. A singular housing member is shown in FIG. 1. The housing 106 may be metallic, plastic, ceramic, or crystalline, or may include a combination of such materials, or may include other materials.[0038] A cover 108 may be mounted to the housing 106 on a front side of the watch body 102 (i.e., facing away from a user's skin), as shown in FIG. 1. The cover 108 may protect a display within the housing 106 (and in some cases, the display may be mounted partly or wholly to the cover 108). The display may be viewable by a user through the cover 108. In some cases, the cover 108 may be part of a display stack, which display stack may include a touch sensing or force sensing capability. The display may be configured to depict a graphical output of the watch 100, and a user may interact with the graphical output (e.g., using a finger, stylus, crown 110, or button 112). As one example, the user may select (or otherwise interact with) a graphic, icon, or the like presented on the display. For example, the user may interact with a graphic on the display by touching or pressing on the display at the location of the graphic. The cover 108 may be considered separate from the housing 106, or alternatively, the cover 108 may be considered a component (e.g., a housing member) of the housing 106. In some examples, the cover 108 may be a crystal, such as a sapphire crystal. The cover 108 may alternatively be formed of glass, plastic, or another material (or materials) that is transmissive to at least one wavelength of light (e.g., visible light).[0039] Another cover 114 may be mounted to the housing 106 on a back side of the watch body 102 (i.e., facing a user's skin), as shown in FIG. 2. The cover 114 may protect part or all of a biosensor system 116 mounted within the housing 106 (and in some cases, the biosensor system 116 may be mounted partly or wholly to the cover 114). The biosensor system 116 may include a light field camera positioned adjacent the cover 114 to receive light through the cover 114. The biosensor system 116 may be configured to image a forearm of a user near a wrist of the user, and perform a bioauthentication of the user based on the imaging. The cover 114 may be considered separate from the housing 106, or alternatively, the cover 114 may be considered a component (e.g., a housing member) of the housing 106. In some examples, the cover 114 may be a crystal, such as a sapphire crystal. The cover 114 may alternatively be formed of glass, plastic, or another material that is transmissive to at least one wavelength of light (e.g., visible light, such as infrared light, or invisible light (i.e., other forms of electromagnetic radiation)).[0040] The watch body 102 may include at least one input device or selection device, such as a crown assembly, scroll wheel, knob, dial, button, or the like, which input device may be operated by a user of the watch 100. For example, the housing 106 may include an aperture through which a shaft extends. A crown 110 may be attached to the shaft, and may be accessible to a user exterior to the housing 106. The crown 110 may be manipulated by a user to rotate or translate the shaft. The shaft may be mechanically, electrically, magnetically, and/or optically coupled to components within the housing 106 as one example. A user's manipulation of the crown 110 and shaft may be used, in turn, to manipulate or select various elements displayed on the display, to adjust a volume of a speaker, to turn the watch 100 on or off, and so on. The housing 106 may also include an aperture through which a button 112 protrudes.[0041] The housing 106 may include structures for attaching the watch band 104 to the watch body 102. In some cases, the structures may include elongate recesses or apertures through which ends of the watch band 104 may be inserted and attached to the watch body 102. In other cases (not shown), the structures may include indents (e.g., dimples or depressions) in the housing 106, which indents may receive ends of spring pins that are attached to or threaded through ends of a watch band to attach the watch band to the watch body.[0042] The watch band 104 may be used to secure the watch 100 to a user, another device, a retaining mechanism, and so on.[0043] In some examples, the watch 100 may lack the cover 108, the display, the crown 110, or the button 112. For example, the watch 100 may include an audio input or output interface, a touch input interface, a haptic (force) input or output interface, or other input or output interface that does not require the display, crown 110, or button 112. The watch 100 may also include the afore-mentioned input or output interfaces in addition to the display, crown 110, or button 112. When the watch 100 lacks the display, the front side of the watch 100 may be covered by the cover 108, or by a metallic or other type of housing member.[0044] In some embodiments, each of the covers 108, 114 may include any transparent, semi-transparent, or translucent surface made out of glass, a crystalline material (such as sapphire or zirconia), plastic, or the like, has and may have a crystal or non-crystalline atomic structure.[0045] Turning now to FIG. 3, the watch 100 is shown attached to a forearm 300 of a user near the user's wrist. Attachment of the watch 100 to the forearm 300 near the wrist may position a first or exterior surface of the cover 114 (i.e., a surface exterior to the watch body 102) adjacent a dorsal side of the forearm 300 near the user's wrist.[0046] The biosensor system 116 may include a light emitter (or set of light emitters) positioned to emit light into the dorsal side of the forearm 300 near the wrist. The biosensor system 116 may also include a light field camera positioned within the watch body 102, adjacent a second or interior surface of the cover 114 (i.e., a surface interior to the watch body 102). The light field camera may receive, from the dorsal side of the forearm 300 near the wrist, remissions of the light emitted by the light emitter(s). The remissions of light may include reflections and refractions of the light by features on or within the forearm 300 of the user near the user's wrist (e.g., features of skin, hair, hair follicles, pores, vascular structures, connective tissues, bones, and so on).[0047] A processor (e.g., a processor within the watch body 102) may be configured to operate the light emitter(s) and light field camera to obtain a light field image from the light field camera. In some embodiments, the light field camera or another sensor of the watch 100 (e.g., a proximity sensor) may detect when the watch 100 is positioned on and/or attached to the user's forearm 300 and trigger the processor to operate the light emitter(s) and light field camera, obtain a light field image from the light field camera, and perform an operation such as a bioauthentication operation, a bioauthentication enrollment operation, a secure transaction operation, a health monitoring operation, or a health assessment operation using the light field image. In some embodiments, a utility or application running on the watch 100 (or a utility or application running remote from the watch 100) may trigger the processor to operate the light emitter(s) and light field camera, obtain a light field image from the light field camera, and perform an operation such as a bioauthentication operation, a bioauthentication enrollment operation, a secure transaction operation, a health monitoring operation, or a health assessment operation using the light field image.[0048] In some examples, the light field camera of the biosensor system 116 may include a set of micro-cameras having overlapping fields of view (i.e., an array of micro-cameras). Each camera may be operable to acquire an image (e.g., an elemental image), which image may include a plurality of pixel values. Each pixel value may indicate the intensity of a light ray remitted from a particular direction (or in practice, the intensity of a small number of light rays remitted from a small set of directions). Associations between pixel values and light ray directions may be established by an array of light-transmissive elements such as pinholes or microlenses, which light-transmissive elements restrict the apertures of the micro-cameras and/or direct rays of light remitted from particular directions to particular pixels of the micro-cameras.[0049] The light field image obtained by the processor may include a set of images (e.g., a set of elemental images) acquired by some or all of the micro-cameras of a light field camera. The processor may be configured to perform a synthetic focusing operation on the light field image to construct images of one or more layers of the user's forearm 300 near the user's wrist (e.g., an image of at least one layer in the three-dimensional space 302 defined by the overlapping fields of view of the micro-cameras of the light field camera in the biosensor system 116; see FIGS. 2 and 3). Performance of the synthetic focusing operation may entail combining pixel values in the set of images in different ways to construct images of different layers of the user's forearm 300.[0050] The processor may be configured to extract a set of features from the image(s) constructed during the synthetic focusing operation. The set of features may include, for example, features of skin, hair, hair follicles, pores, vascular structures, connective tissues, bones, and so on, as described in greater detail with reference to FIGS. 4 and 5. In some embodiments, the processor may compare the set of features to a reference set of features, to determine whether the set of features matches the reference set of features. When the reference set of features are features of an authorized user (e.g., an authorized user of the watch 100, of a service provided by the watch (e.g., a payment, banking, or other secure transaction service), etc.), and when the set of features matches the reference set of features, the processor may, for example, authenticate the user of the watch 100 and enable the user to access the watch 100, or access a function of the watch 100, or complete a secure transaction. When the reference set of features are features associated with a health status or condition, and when the set of features matches the reference set of features, the processor may, for example, indicate that the user of the watch 100 may have the associated health status or condition, or provide an alert to medical personnel. When the biosensor system 116 is operated as a part of a bioauthentication enrollment operation, the processor may store the set of features identified from the image(s) constructed during the synthetic focusing operation, so that the set of features may be accessed at a later time as a reference set of features (e.g., as a set of features associated with an authorized user, or as a set of features associated with a baseline health or condition of the user).[0051] FIG. 4 shows various features that may be identified in a set of one or more layer images constructed from a light field image obtained from a dorsal side of a forearm near a wrist. The features may be associated with structures at or just beneath the surface of a user's skin (e.g., on or into the epidermis 400), or structures that start at the surface of the user's skin and extend deeper beneath the user's skin (e.g., into the dermis 402).[0052] Examples of features associated with structures at or just beneath the surface of a user's skin include a vascular pattern 404 (e.g., a blood perfusion pattern in skin), a pigmentation pattern 406 (e.g., a skin (or melanin) pigmentation pattern), a small scale folding pattern of skin, a hair follicle opening pattern 408, and a pore opening pattern 410. A blood perfusion pattern in skin (e.g., a capillary blood perfusion pattern) may vary significantly with temperature, exercise, and other biological or environmental variables, and may have limited use for biometric identification. A skin pigmentation pattern may be more stable than a blood perfusion pattern in skin, but may vary with exposure to ultraviolet light (e.g., sunlight).[0053] Examples of features associated with structures that start at the surface of a user's skin and extend beneath the user's skin include a hair follicle pattern 412, and a pore pattern 414 (e.g., sweat ducts). In some cases, hair follicles may be identified by first identifying points at which hair intersects (or enters) the epidermis (e.g., a hair follicle opening pattern 408 in one or more shallow layers of a forearm), and then correlating these points with structures (e.g., cross-sections of hair follicles) identified in deeper layers of a forearm. In some cases, sweat ducts may be identified based on the different light absorption rates of sweat ducts and surrounding turbid tissues, which may contain different amounts of water and absorb light (e.g., infrared light) at different rates. Similarly to the identification of hair follicles, pores may identified in surface or shallow layers of a forearm (e.g., in a pore opening pattern 410) and correlated with structures (e.g., cross-sections of sweat glands) identified in deeper layers of a forearm.[0054] FIG. 5 shows additional features that may also or alternatively be identified in a set of one or more layer images constructed from a light field image obtained from a dorsal side of a forearm 500 near a wrist 502. The features may be associated with structures deep beneath the surface of a user's skin (e.g., in subcutaneous tissue or bone).[0055] Examples of features associated with structures deep beneath the surface of a user's skin include a vascular pattern 504 (e.g., a vein pattern, an artery pattern, a blood perfusion pattern in tendons, or a blood perfusion pattern in fascia), a connective tissue pattern 506 (e.g., a tendon pattern), and a bone shape pattern. A vascular pattern in subcutaneous tissue, and particularly a vein pattern or an artery pattern, may vary less than a vascular pattern in epidermal or dermal tissue. A vein pattern or an artery pattern, for example, may also include vein or artery cross-sectional size information that provides an extra level of detail and more biometric entropy than can be obtained for a capillary blood perfusion pattern.[0056] FIGS. 6-8 show a set of components 600 that may be included in a biosensor system such as the biosensor system 116 shown in FIGS. 2 and 3. FIGS. 6 and 8 show the components 600 in assembled form, and FIG. 7 shows the components 600 in exploded form. The biosensor system includes a set of light emitters 602 and a light field camera 604, both of which may be positioned adjacent an interior surface of a cover 606. The cover 606 may be the backside cover 114 that is mounted to the housing 106 of the watch body 102 shown in FIGS. 1-3.[0057] The light emitters 602 may be positioned to emit light into a dorsal side of a forearm near a wrist of a user when an exterior surface of the cover 606 is positioned adjacent (e.g., on) the dorsal side of the forearm near the wrist. The light field camera 604 may be positioned to receive emissions of the light (i.e., the light emitted by the light emitters 602) from the dorsal side of the forearm near the wrist.[0058] As shown in FIG. 7, the light field camera 604 may include a first array of non-overlapping image sensing regions 608, and a second array of light-transmissive elements (e.g., an array of pinholes 610 defined by a pinhole mask 612). The pinhole mask, including the light-transmissive elements (e.g., pinholes), may be positioned between the array of non-overlapping image sensing regions 608 and the interior surface of the cover 606. In some embodiments, the pinhole mask 612 may include a substrate made of opaque plastic or another opaque material. In some embodiments, the pinhole mask 612 may include patterned ink deposited on the interior surface of the cover 606.[0059] The array of non-overlapping image sensing regions 608 may be aligned with the array of light-transmissive elements to form a set of micro-cameras. In some embodiments, and as shown, the array of non-overlapping image sensing regions 608 may include regions of a single image sensor. Alternatively (and not shown), the array of non-overlapping image sensing regions 608 may include discrete image sensors (e.g., one image sensor per image sensing region, or one image sensor per subset of image sensing regions). The light field camera 604 may optionally include a spacer 614 that separates (e.g., is between) the array of non-overlapping image sensing regions 608 and the array of light-transmissive elements. In some embodiments, the spacer 614 may include a layer of glass.[0060] The light emitters 602 may be positioned around the light field camera 604, and may emit light from around the light field camera 604. In some embodiments, the light emitters 602 may include light emitting diodes (LEDs). In some embodiments, the light emitters 602 may include infrared emitters.[0061] The micro-cameras may have fields of view 616 that overlap on a side of the cover 606 opposite the light field camera 604, as shown in FIG. 8. In some cases, the fields of view 616 may begin to overlap within the cover 606. By way of example, the fields of view 616 may be conical. When the external surface of the cover 606 is positioned adjacent a forearm of a user near the user's wrist, the conical fields of view 616 may extend into the forearm and define the extent of the three-dimensional space 302 described with reference to FIG. 3.[0062] As shown in FIG. 8, a feature 618 of a forearm may fall within the fields of view 616 of multiple micro-cameras. FIG. 9 shows the conical fields of view 616 of some of the micro-cameras shown in FIGS. 6-8, as well as the feature 618 of the forearm shown in FIG. 8. The light emitters 602 and light field camera 604 described with reference to FIGS. 6-8 may be operated to obtain a light field image at an array of image sensing regions 900 positioned at a first plane. A synthetic focusing operation may then be performed on the light field image (e.g., by constructively and/or destructively combining the pixel values of a set of images included in the light field image) to construct images of one or more layers 902, 904, 906 of the forearm. An image of one layer (e.g., the layer 906) may include, or bring into focus, the feature 618 of the forearm. Images of other layers (e.g., layers 902 and 904) may not include, or obfuscate, the feature 618 of the forearm.[0063] FIG. 10 shows an exploded view of another set of components 1000 that may be included in a biosensor system such as the biosensor system 116 shown in FIGS. 2 and 3. The components 1000 may include the cover 606, set of light emitters 602, array of non-overlapping image sensing regions 608, array of light-transmissive elements, and spacer 614 described with reference to FIGS. 6-8. However, in the biosensor system of FIG. 10, the light-transmissive elements may include a set of lenses 1002, instead of or in addition to pinholes. In some embodiments, the set of lenses 1002 may be included in a microlens array (MLA).[0064] FIG. 11 shows an exploded view of another set of components 1100 that may be included in a biosensor system such as the biosensor system 116 shown in FIGS. 2 and 3. The components 1100 may include the cover 606, set of light emitters 602, array of non-overlapping image sensing regions 608, array of light-transmissive elements, and spacer 614 described with reference to FIGS. 6-8. However, in the biosensor system of FIG. 11, the set of light emitters 602 may be intermingled with the set of micro-cameras, and may be positioned to emit light from within a boundary defined by the set of micro-cameras.[0065] In some embodiments of the biosensor systems described with reference to FIGS. 2, 3, 6-8, 10, and 11, the light emitters 602 of a biosensor system may include light emitters of different type, such as light emitters that emit different wavelengths of light. Operation of a biosensor system while simultaneously or sequentially activating light emitters of different type (e.g., of different wavelengths) can help differentiate features having different light absorption and reflection spectra. For example, melanin pigmentation can have a very different absorption spectrum than blood. Hence, a vascular pattern (e.g., a blood perfusion pattern in skin) may in some cases be distinguished from a pigmentation pattern (e.g., a skin (or melanin) pigmentation pattern) based at least in part on whether the features of the patterns absorb or reflect light of a particular wavelength.[0066] In some embodiments, light emitters may be tuned to emit different wavelengths of light, or different color filters may be positioned over (or deposited on) different light emitters.[0067] Referring now to FIG. 12, there is shown a watch attached to a forearm of a user near the user's wrist. By way of example, the watch may be the watch 100 described with reference to FIGS. 1-3. As previously described, attachment of the watch 100 to the forearm 300 near the wrist may position a first or exterior surface of a cover 114 (i.e., a surface exterior to a watch body 102) adjacent a dorsal side of the forearm 300 near the user's wrist. However, in contrast to the watch position shown in FIG. 3, the watch body 102 is tilted with respect to the dorsal side of the forearm 300 in FIG. 12. As a result, the biosensor system 116 and included light field camera are tilted with respect to the dorsal side of the forearm 300.[0068] In some embodiments, the biosensor system 116 may be configured to compensate for tilt of a light field camera with respect to the dorsal side of the forearm 300. FIG. 13 shows an alternative embodiment of the watch 100 described with reference to FIGS. 1-3, in which a tilt sensor 1300 is included in the watch 100. The tilt sensor 1300 may be configured to detect the tilt described with reference to FIG. 12, so that a processor associated with the biosensor system 116 may compensate for the tilt. By way of example, the tilt sensor 1300 is shown to include a set of proximity sensors (e.g., four proximity sensors 1302). The proximity sensors 1302 may be capacitive sensors, optical sensors, other types of sensors, or a combination thereof.[0069] The proximity sensors 1302 may determine respective distances between the watch body 102 (or a sensing plane of the biosensor system 116) and the skin of a user's forearm, and may determine a complex angle (e.g., a set of angles in x, y, and z planes) between the sensing plane and the skin[0070] FIG. 14 shows the conical fields of view 616 of some of the micro-cameras shown in FIGS. 6-8, as well as the feature 618 of the forearm shown in FIG. 8. As described with reference to FIG. 9, the light emitters and light field camera described with reference to FIGS. 6-8 may be operated to obtain a light field image at an array of image sensing regions 900 positioned at a first plane. However, in contrast to the synthetic focusing operation described with reference to FIG. 9, the synthetic focusing operation illustrated in FIG. 14 may be performed on the light field image (e.g., by constructively and/or destructively combining the pixel values of a set of images included in the light field image) while compensating for tilt of a sensing plane (i.e., the plane including image sensing regions 900) with respect to a user's skin Alternatively, the tilt may be compensated for prior to performing the synthetic focusing operation (e.g., by distorting the elemental images of the light field image based on the determined tilt). Thus, images may be constructed for layers 1102, 1104, 1106 of the forearm that intersect the conical fields of view 616 of the micro-cameras at an angle (i.e., the images may be normalized so that the images are parallel to the surface of the skin instead of parallel to the sensing plane defined by image sensing regions 900). In some embodiments, images may be normalized using a procedure such as shear-warp factorization.[0071] In alternative embodiments, tilt of a biosensor system, light field camera, or sensing plane with respect to the skin of a user's forearm may be determined using the light field camera. For example, a synthetic focusing operation may be performed to determine when the surface of the skin (e.g., features on the surface of the skin) comes into focus at some or all of the micro-cameras in the light field camera. Depths associated with the layers at which the surface of the skin comes into focus may be determined, and the determined depths may be used similarly to distances determined by proximity sensors to determine the tilt of the biosensor system, light field camera, or sensing plane with respect to the skin of the user's forearm.[0072] In further alternate embodiments, a processor associated with a biosensor system including a light field camera may determine a tilt of the biosensor system, light field camera, or sensing plane with respect to the skin of a user's forearm, and then: 1) warn the user that the tilt exists, 2) prompt the user to reposition the biosensor system or device that includes the biosensor system, and/or 3) instruct the user on how to reposition the biosensor system or device.[0073] A biosensor system, such as one of the biosensor systems described with reference to FIGS. 2, 3, and 6-14, may be randomly positioned in various locations on the dorsal side of a forearm near a user's wrist. For example, the position of the biosensor system may vary with each attachment of a device that includes the biosensor system to the user's forearm, or with the user's adjustment of a watch band or other member used to secure the device to the user's forearm, or with movement of the user, or as a result of the user intentionally moving the device to make the device feel comfortable based on environmental conditions. To account for random positioning of the biosensor system on the user's forearm, the biosensor system may capture multiple light field images from different positions during a bioauthentication enrollment operation or health monitoring baselining operation. The multiple light field images may captured at a predetermined time, such as when the device or the user initiates an enrollment or baselining operation, or in the background when the device is worn by the user and not being used for other purposes. The multiple light field images may be used to construct a composite template of the user's forearm. The composite template may be associated with a larger three-dimensional space than the three-dimensional space associated with a single light field image.[0074] FIG. 15 shows a sample electrical block diagram of an electronic device 1500, which electronic device 1500 may in some cases take the form of any of the watches or other wearable electronic devices described with reference to FIGS. 1-14, or other portable or wearable electronic devices. The electronic device 1500 may include a display 1502 (e.g., a light-emitting display), a processor 1504, a power source 1506, a memory 1508 or storage device, a sensor 1510, and an input/output (I/O) mechanism 1512 (e.g., an input/output device, or input/output port). The processor 1504 may control some or all of the operations of the electronic device 1500. The processor 1504 may communicate, either directly or indirectly, with some or all of the components of the electronic device 1500. For example, a system bus or other communication mechanism 1514 may provide communication between the processor 1504, the power source 1506, the memory 1508, the sensor 1510, and the input/output mechanism 1512.[0075] The processor 1504 may be implemented as any electronic device capable of processing, receiving, or transmitting data or instructions. For example, the processor 1504 may be a microprocessor, a central processing unit (CPU), an application-specific integrated circuit (ASIC), a digital signal processor (DSP), or combinations of such devices. As described herein, the term \"processor\" is meant to encompass a single processor or processing unit, multiple processors, multiple processing units, or other suitably configured computing element or elements.[0076] It should be noted that the components of the electronic device 1500 may be controlled by multiple processors. For example, select components of the electronic device 1500 (e.g., the sensor 1510) may be controlled by a first processor and other components of the electronic device 1500 (e.g., the display 1502) may be controlled by a second processor, where the first and second processors may or may not be in communication with each other. In some cases, the processor 1504 may perform one or more of a bioauthentication operation, a bioauthentication enrollment operation, a secure transaction operation, a health monitoring operation, a health assessment operation, and so on.[0077] The power source 1506 may be implemented using any device capable of providing energy to the electronic device 1500. For example, the power source 1506 may be one or more batteries or rechargeable batteries. Additionally or alternatively, the power source 1506 may be a power connector or power cord that connects the electronic device 1500 to another power source, such as a wall outlet.[0078] The memory 1508 may store electronic data that can be used by the electronic device 1500. For example, the memory 1508 may store electrical data or content such as, for example, audio and video files, documents and applications, device settings and user preferences, timing signals, control signals, data structures or databases, or reference sets of features used in a bioauthentication, health monitoring, or health assessment operation. The memory 1508 can be configured as any type of memory. By way of example only, the memory 1508 may be implemented as random access memory, read-only memory, Flash memory, removable memory, other types of storage elements, or combinations of such devices.[0079] The electronic device 1500 may also include one or more sensors 1510 positioned almost anywhere on the electronic device 1500. The sensor(s) 1510 can be configured to sense one or more type of parameters, such as but not limited to, pressure, light (e.g., a light field), touch, heat, movement, relative motion, biometric data (e.g., biological images or parameters), and so on. For example, the sensor(s) 1510 may include a heat sensor, a position sensor, a light or optical sensor, an accelerometer, a pressure transducer, a gyroscope, a magnetometer, a health monitoring sensor, a light field camera, and so on. Additionally, the one or more sensors 1510 can utilize any suitable sensing technology, including, but not limited to, capacitive, ultrasonic, resistive, optical, light field, ultrasound, piezoelectric, and thermal sensing technology. In some examples, the sensor(s) 1510 may include one of the biosensor systems described herein.[0080] The I/O mechanism 1512 may transmit and/or receive data from a user or another electronic device. An I/O device may include a display, a touch sensing input surface, one or more buttons (e.g., a graphical user interface \"home\" button), a crown, one or more cameras, one or more microphones or speakers, one or more ports such as a microphone port, and/or a keyboard. Additionally or alternatively, an I/O device or port may transmit electronic signals via a communications network, such as a wireless and/or wired network connection. Examples of wireless and wired network connections include, but are not limited to, cellular, Wi-Fi, Bluetooth, IR, and Ethernet connections.[0081] FIG. 16 shows an example method 1600 of authenticating a user of a watch or other wearable electronic device, such as one of the watches or wearable electronic devices described herein.[0082] At block 1602, the method may include emitting light into a dorsal side of a forearm near a wrist of the user. The operation(s) at 1602 may be performed, for example, using the biosensor system described with reference to FIGS. 2, 3, and 6-15, the light emitter(s) described with reference to FIGS. 2, 3, 6-8, 10, and 11, or the processor described with reference to FIGS. 2, 3, 6-11, and 13-15.[0083] At block 1604, the method may include receiving remissions of the light from the dorsal side of the forearm near the wrist of the user. The remissions of the light may be received using a light field camera. The operation(s) at 1604 may be performed, for example, using the biosensor system described with reference to FIGS. 2, 3, and 6-15, the light field camera described with reference to FIGS. 2, 3, and 6-14, or the processor described with reference to FIGS. 2, 3, 6-11, and 13-15.[0084] At block 1606, the method may include generating a light field image from the remissions of the light. The operation(s) at 1606 may be performed, for example, using the biosensor system described with reference to FIGS. 2, 3, and 6-15, or the processor described with reference to FIGS. 2, 3, 6-11, and 13-15.[0085] At block 1608, the method may include performing a synthetic focusing operation on the light field image to construct at least one image of at least one layer of the forearm near the wrist. The operation(s) at 1608 may be performed, for example, using the biosensor system described with reference to FIGS. 2, 3, and 6-15, or the processor described with reference to FIGS. 2, 3, 6-11, and 13-15.[0086] At block 1610, the method may include extracting a set of features from the at least one image. The operation(s) at 1610 may be performed, for example, using the biosensor system described with reference to FIGS. 2, 3, and 6-15, or the processor described with reference to FIGS. 2, 3, 6-11, and 13-15.[0087] In some embodiments, the features extracted at block 1610 may include at least one or two of a hair follicle opening pattern, a hair follicle pattern, a vascular pattern, a vein pattern, an artery pattern, a blood perfusion pattern in skin, a blood perfusion pattern in tendons, a blood perfusion pattern in fascia, a tendon pattern, a connective tissue pattern, a skin pigmentation pattern, a small scale folding pattern of skin, a pore opening pattern, a pore pattern, and a bone shape pattern. In some embodiments, the features may include a relationship between at least two of a hair follicle opening pattern, a hair follicle pattern, a vascular pattern, a vein pattern, an artery pattern, a blood perfusion pattern in skin, a blood perfusion pattern in tendons, a blood perfusion pattern in fascia, a tendon pattern, a connective tissue pattern, a skin pigmentation pattern, a small scale folding pattern of skin, a pore opening pattern, a pore pattern, and a bone shape pattern.[0088] At block 1612, the method may include determining whether the set of features matches a reference set of features. The operation(s) at 1612 may be performed, for example, using the biosensor system described with reference to FIGS. 2, 3, and 6-15, or the processor described with reference to FIGS. 2, 3, 6-11, and 13-15.[0089] At block 1614, the method may include authenticating the user in response to the set of features matching the reference set of features. The operation(s) at 1614 may be performed, for example, using the biosensor system described with reference to FIGS. 2, 3, and 6-15, or the processor described with reference to FIGS. 2, 3, 6-11, and 13-15.[0090] In alternative embodiments of the method 1600, the operation(s) at block 1614 may include the performance of a health monitoring, health assessment, payment, banking, or other secure transaction operation. In some embodiments, the operation(s) at block 1612 may not be performed, and the operation(s) at block 1614 may include a biauthentication enrollment operation or health monitoring baselining operation.[0091] In some embodiments of the method 1600, the method may further include determining a tilt of the light field camera with respect to the dorsal side of the forearm near the wrist, and compensating for the determined tilt prior to or while performing the synthetic focusing operation, as described for example with reference to FIGS. 12-14.[0092] The foregoing description, for purposes of explanation, uses specific nomenclature to provide a thorough understanding of the described embodiments. However, it will be apparent to one skilled in the art that the specific details are not required in order to practice the described embodiments. Thus, the foregoing descriptions of the specific embodiments described herein are presented for purposes of illustration and description. They are not targeted to be exhaustive or to limit the embodiments to the precise forms disclosed. It will be apparent to one of ordinary skill in the art that many modifications and variations are possible in view of the above teachings.",
        "sentiment": -0.25109967201734784
    },
    "article_24": {
        "title": "Adobe recommends users to immediately uninstall Flash Player to help protect their systems",
        "body": "After a three-year warning, the end of Adobe Flash has officially happened. And with that, Adobe is strongly recommending users uninstall it from their computers immediately. Read along for how to fully remove Adobe Flash from your Mac.\nUpdate 1/12: After ending support on January 1, Adobe is now blocking all Flash content from running as of today, January 12, 2021. If you see any request to update Flash player, you can assume it\u2019s malware/spam. And if you haven\u2019t already fully removed Flash from you Mac, read on\u2026\nAdobe officially dropped support for Flash on January 1, 2021. And in just under two weeks, Adobe will block all Flash content from working in Flash Player.\nAdobe strongly recommends all users immediately uninstall Flash Player to help protect their systems. Some users may continue to see reminders from Adobe to uninstall Flash Player from their system.\nHere\u2019s why Adobe says it\u2019s important to remove it: \u201cUninstalling Flash Player will help secure your system since Adobe does not intend to issue Flash Player updates or security patches after the EOL Date.\u201d Essentially this is meant to help people realize if any Flash updates appear from here on out, they\u2019re malicious or spam and should be avoided.\nKeep in mind you don\u2019t want to just remove Adobe Flash from the preference pane in System Preferences. Here\u2019s what to do to fully uninstall it from your Mac\u2026\nHow to fully remove Adobe Flash from your Mac\nDeauthorizing Flash on your Mac\nIf you\u2019re privacy conscious, first head to System Preferences > Flash Player (found at the bottom)\nIn the top right, click the Advanced tab\nChoose Deauthorize This Computer\u2026\nFully remove Adobe Flash\nOpen a Finder window then select Applications, now open Utilities (or search for the app Adobe Flash Player Install Manager)\nOpen Adobe Flash Player Install Manager > click Uninstall\nYou\u2019ll get a confirmation when it\u2019s been successfully removed\nYou can use a third-party app like Clean My Mac or Sensei to search for and remove any extraneous Flash files but for me the official Adobe Flash uninstaller removed everything\nAt the end of the process, Safari will also relaunch with Adobe showing a thank you message:\nYou can read more about Adobe Flash end of life on this support document.\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.381225960701704
    }
}