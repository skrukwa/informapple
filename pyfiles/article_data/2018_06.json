{
    "article_0": {
        "title": "iPhones on iOS 12 Will Automatically Share Precise Location Data During 911 Calls in United States",
        "body": "Apple today announced that, starting later this year with iOS 12, iPhones will automatically share precise location data with first responders during 911 calls in the United States, helping to reduce emergency response times.\nThe new functionality will be powered by RapidSOS's IP-based data pipeline, which will quickly and securely provide 911 call centers with Hybridized Emergency Location data, which is determined based on proximity to cell towers and Wi-Fi access points, and on-device data sources like assisted GPS.\nApple said the location data will only be used for emergency purposes, and only accessible to responding dispatch centers during 911 calls.\n\"Communities rely on 911 centers in an emergency, and we believe they should have the best available technology at their disposal,\" said Apple CEO Tim Cook, in a prepared statement. \"When every moment counts, these tools will help first responders reach our customers when they most need assistance.\"\nThe current 911 system, developed by AT&T in the 1960s, was intended for landline phones, which have fixed addresses. Apple notes that approximately 80 percent of 911 calls today come from mobile devices, however, and most dispatching systems can only obtain their estimated location based on cell towers.\nRapidSOS's system, which will be integrated with existing software at 911 centers, should help solve this problem. The modern technology is part of a broader initiative known as Next Generation 911.\n\"911 telecommunicators do extraordinary work managing millions of emergencies with little more than a voice connection,\" said RapidSOS CEO Michael Martin. \"We are excited to work with Apple to provide first responders a new path for accurate, device-based caller location using transformative Next Generation 911 technology.\"\nAs many as 10,000 lives could be saved each year if 911 emergency dispatchers were able to get to callers just one minute faster, according to the FCC. Whether someone is experiencing cardiac arrest, or a house is on a fire, it is obviously critical for first responders to arrive as quickly as possible.\n\"This new functionality is an example of how companies and first responders can use technology to dramatically improve public safety,\" said former FCC Chairman Tom Wheeler. \"Lives will be saved thanks to this effort by Apple and RapidSOS.\"\nThe feature is somewhat similar to Advanced Mobile Location, implemented in iOS 11.3 in a handful of European countries.\nAdvanced Mobile Location recognizes when an emergency call is made and, if not already activated, activates an iPhone's GPS or Wi-Fi to collect the caller's precise location information. The device then sends an automatic SMS to the emergency services with the caller's location, before turning the GPS off again.\nAdvanced Mobile Location must be supported by carriers. As of earlier this year, the service was fully operational in many European countries, including the United Kingdom, Estonia, Lithuania, Austria, and Iceland, as well as New Zealand.\nApple says the RapidSOS functionality will be limited to the iPhone for now, ruling out the Apple Watch and other devices at launch.\nApple's announcement coincides with the annual NENA Conference & Expo in Nashville this week, focused on the future of the 911 system.\n\"It can be hard in an emergency to know exactly where you are, and if you\u2019re not on a landline, our first responders can't always help. This upgrade will save lives by giving our 911 dispatchers, police, firefighters and paramedics the thing they need the most: time,\" said Nashville Mayor David Briley.",
        "sentiment": 0.3094871321615453
    },
    "article_1": {
        "title": "iOS 12 to run on everything that runs iOS 11",
        "body": "SAN JOSE\u2014iOS 12 is official, and, as predicted, it puts a heavy emphasis on improving performance and stability. At the same time, the latest iteration of Apple\u2019s mobile OS is poised to bring a handful of noteworthy new features to iPhones and iPads as well.\nThe software update was announced during a keynote presentation at Apple's annual Worldwide Developers Conference (WWDC). A developer preview is available today, with a public beta scheduled for later in the month. A full release will then arrive in the fall.\nApple says the update will be available on all the same devices as the current iOS 11 release, which includes the iPhone 5s and later, every iPad released since the original iPad Air and iPad mini 2, and the 6th-generation iPod Touch.\nThe company said it is putting a particular focus on ensuring the update works smoothly on older devices. To give a point of reference, Craig Federighi, Apple's senior vice president of software engineering, said iOS 12 will launch apps up to 40-percent faster and bring up the keyboard up to 50-percent faster on an iPhone 6 Plus. Federighi said the update will make a compatible phone \"instantly ramp up performance to its highest state\" when it recognizes that it needs a performance boost\u2014such as when you're loading an app\u2014and then more quickly bring it down to help preserve battery life.\nARKit 2 and more augmented reality\nAlso as expected, Apple is updating its augmented reality functionality with iOS 12. The company announced the launch of ARKit 2, the next iteration of the AR development framework it introduced last year. Apple says the update will bring improved face-tracking performance, more realistic rendering, and the ability to create multiplayer augmented reality apps. (Google announced similar multi-user functionality for its ARCore platform last month.) LEGO demonstrated an app in which multiple users could play with a shared virtual LEGO kit, for instance.\nBeyond that, Apple said it worked with movie studio Pixar to create a new open-file format specifically designed for AR called usdz. The company says this will help make it simpler for developers to share AR files across iOS and create 3D objects that can be pulled into the real world. Federighi gave the example of customizing a guitar on the Fender website in Safari, then tapping on it to pull out a 3D model that can be inspected with AR. Adobe was listed as one company that will support the new file format, so the idea is that developers could make an AR object in an app like Photoshop and bring it to iOS in a more straightforward fashion. Along these lines, Apple introduced a new app called Measure that can use an iPhone's camera to measure real-world objects.\nApple\nApple\nApple\nPhotos\nThe company said iOS 12 will improve its Photos app with a beefed-up search library and better suggestions. Apple says the app will use object and scene recognition, letting you combine multiple search terms to more accurately find a specific photo. So you could, say, refine your search to pictures of (1) your dog at (2) the beach in (3) New Jersey, much like you can with Google Photos today. There's a new \"For You\" tab within the app as well, which will compile photos, Memories, and iCloud Shared Albums the app thinks are significant and suggest photos to share with specific people. Apple says Photos uses encrypted \"on-device machine learning\" to make these suggestions.\nAdvertisement\nSiri shortcuts for third-party apps\nThe update will bring new features to Apple's Siri digital assistant as well. The biggest of the bunch is a feature called Shortcuts, which will let users create a voice prompt to ask Siri to perform commonly made actions with third-party apps and let developers integrate further with the assistant for certain quick actions. Apple gave the example of assigning the phrase \"help me find my keys\" in conjunction with the Tile app: if you say that, Siri could be made to automatically activate the Tile app and use it to help you find your keys right from within the Siri interface.\nYou could also assign a multistep routine to the assistant: a custom phrase like \"heading home,\" for instance, could prompt Siri to start up a favorite radio station, adjust your home thermostat, send a message to your spouse, and tell you how long you'll take to get home. This would seem to explain what the Workflow team has been up to after Apple acquired the automation app last year.\nAlong with that, Apple says Siri will be more proactive about suggesting things you do on a regular basis. If it's a loved one's birthday, for instance, Siri could present a home screen shortcut reminding you to call them. If you regularly order coffee from a specific app, it could give you a quick link to do that in the morning. There's a dedicated editor for setting up custom prompts, and Apple says this kind of behavior will work across iPhone, iPad, the Apple Watch, and the HomePod.\nGrouped notifications\nApple says iOS 12 will bring long-anticipated updates to how the software handles notifications, too. Most notably, it will now group notifications by app, text thread, and the like. Instead of getting individual notifications for each text from a given person, for instance, everything from said person will be bundled in one bubble, which you can swipe aside if you wanted to dismiss everything altogether. This is something Google's Android OS has allowed for many years, but it should be a welcome change nonetheless. (That said, when you go to expand and ungroup a bundle of notifications, iOS 12 will then lay them out the old way, one by one.) Apple says users will be able to turn off notifications for specific apps right from the lock screen as well.\nGroup FaceTime\nFaceTime will be updated as well with the ability to chat with multiple people in a group call. Group FaceTime, as Apple calls it, will support up to 32 people on a call. Apple says these calls won't discriminate by device, so people could join at any time from an iPhone, iPad, or Mac or through an audio-only feed on Apple Watch.\nDigital health features\nSimilar to Google's \"digital well-being\" initiative with Android P, Apple touted a few new features designed to curb iPhone overuse. The Do Not Disturb setting, for instance, will now let you designate a specific time period as \"bedtime;\" the feature will then mute any incoming notifications over that time and let you tap an icon in the morning to have them start rolling in again.\nAdvertisement\nA new \"Screen Time\" feature will give a weekly activity summary of how you've engaged with your iPhone or iPad, telling you things like which apps you've used, which apps are sending you the most notifications, how often you pick up your phone, and what time of day you've used certain apps. Another new feature called \"App Limits\" will let you set a limit on how long you want to use a specific app, then send you a notification when that time is nearing its end. Apple says this can be set up for kids, too, and can make it so they only see certain movies, apps, or websites.\n\u201cMemoji\u201d and more Animoji\nThe new iOS will update Apple's \"Animoji\" icons, which for now are limited to the iPhone X but will likely arrive on the next crop of iPhones when they are introduced later this year. The headline feature is \"Memoji,\" which looks like Apple's spin on the vaguely horrifying AR Emoji feature of Samsung's Galaxy S9 phones. This lets you make an animoji of yourself using a compatible phone's front-facing 3D camera, complete with customizable options for hairstyle, facial features, skin tone, eyewear, and so on. Apple is adding ghost, koala, tiger, and T-rex Animoji to the current set, too. The company says all of these will now be able to capture winks and tongue movements. The company is also adding Animoji, along with Snapchat-like filters and stickers, to the camera function in the Messages app.\nApple\nApple\nApple\nApple\nApple\nCarPlay, Apple Books, Voice Memos, and more\nSeveral other Apple apps will receive small refreshes as part of the iOS 12 update as well. CarPlay will support third-party navigation apps like Waze and Google Maps. iBooks will be rechristened as Apple Books; it will include an overhauled store page and a \"Reading Now\" tab that lets you jump back into the spot you left your last-read book. Voice memos will gain a new look and be able to sync recordings across iOS 12 devices through iCloud. Apple News will get a new sidebar on iPads to make it easier to navigate topics and outlets. The Stocks app will arrive on the iPad and use Apple News to present financial news stories.\nAll told, iOS 12 doesn't promise a ton of show-stopping changes, but its many smaller-scale improvements should help polish up iOS from a quality-of-life standpoint. It's worth noting that all of these new features may not roll out with iOS 12 right away, though. It took Apple several releases over many months to add all the features that were promised in iOS 11. For example, Messages in iCloud was promised in iOS and macOS at last year's WWDC, but it just hit both platforms last week.",
        "sentiment": 0.15088275249396998
    },
    "article_2": {
        "title": "Apple CEO Tim Cook says family separations at US border are \u2018inhumane\u2019 and \u2018need to stop\u2019",
        "body": "Apple CEO Tim Cook called the Trump administration\u2019s policy of separating children from their parents at the United States border \u201cinhumane\u201d during remarks he made in Dublin on Tuesday, according to The Irish Times. \u201cIt\u2019s heartbreaking to see the images and hear the sounds of the kids,\u201d Cook said. \u201cKids are the most vulnerable people in any society. I think that what\u2019s happening is inhumane. It needs to stop.\u201d Among technology companies, Microsoft and Airbnb both voiced strong disagreement with the policy on Monday.\nWith his public remarks, Cook joins the ongoing national outcry from US citizens, politicians, and business leaders who have sharply criticized the Trump administration for a rise in family separations since April. Nearly 2,000 children were separated from their parents from April through May, according to the Department of Homeland Security, after the US Justice Department implemented a \u201czero tolerance policy\u201d for border-crossing offenses. Yesterday, audio obtained by ProPublica of children crying at one of the holding facilities quickly spread across social media and stirred yet more outrage over the separations.\n\u201cWe\u2019ve always felt everyone should be treated with dignity and respect. In this case, that\u2019s not happening,\u201d Cook said. \u201cThis one in particular is just heartbreaking and tragic.\u201d Apple\u2019s chief executive said the company would engage with the White House to advocate its point of view. \u201cI have spoken with him several times on several issues, and I have found him to listen,\u201d Cook said of President Trump. \u201cI haven\u2019t found that he will agree on all things.\u201d",
        "sentiment": -0.11024382896721363
    },
    "article_3": {
        "title": "iOS 12 includes support for reporting unwanted texts &amp; calls as spam",
        "body": "iOS 12 includes a notable update for handling spam messages and calls. As outlined on its developer webpage, app makers can now create an app extension that allows users to report unwanted messages and calls as spam\u2026\nApple\u2019s documentation explains that this feature will allow developers to build app extensions that add an \u201cUnwanted Communication\u201d feature that would need to be enabled in the Settings application.\nWhen enabled, users would be able to swipe left on an item in the \u201cRecents\u201d tab of the Phone app to reveal a new \u201cReport\u201d option. In the Messages app, SMS messages can be reported by a similar swipe left on the thread or a long-press on the message itself.\nAfter the user reports the text or call, the application must then launch its extension to gather additional details from the user:\nTo report SMS messages and calls, the user must enable an Unwanted Communication extension in the Settings app. They can only enable one Unwanted Communication extension at a time. In order to report calls, the user swipes left on an item in the Recents list and selects Report. For SMS messages, they press the Report Messages button when it appears in the Messages transcript. Users can also select messages by long-pressing a message and selecting additional messages, then selecting Report Messages. When the user reports an SMS message or call, the system launches your Unwanted Communication extension. Your extension gathers additional information from the user, before deciding whether to report or block the number, as shown in Figure 1.\nWhat\u2019s important to note here is that this isn\u2019t a first-party feature from Apple, but rather something that Apple has implemented and expects the necessary third-parties to build on. This platform also only applies to SMS messages and calls \u2013 iMessage spam reporting has been available for a while.\nMediaNama speculates that this new feature for developers comes in response to Apple\u2019s feud with the Trai telecommunications regulator in India. Last year, the regulatory body called Apple \u201canti-consumer\u201d as it refused to approve a government-designed application meant for blocking and reporting unwanted sales calls.\nThis new iOS 12 feature seemingly paves the way for such an application, and it isn\u2019t limited to just India. Read more on Apple\u2019s developer website.\nSubscribe to 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.43049932956254405
    },
    "article_4": {
        "title": "Apple strips Facebook &amp; Twitter integration from macOS Mojave",
        "body": "Article Hero Image\nAppleInsider may earn an affiliate commission on purchases made through links on our site.\nContinuing a policy that began with iOS 11, Apple has apparently stripped out integration of third-party accounts in macOS Mojave beyond email and calendar services.\nFacebook, Flickr, Twitter, and Vimeo are no longer options in the first developer beta of Mojave, AppleInsider can verify. Saving login info made it simpler for users and apps to access and share to those services.\nSupport initially came to the Mac with 2012's OS X Mountain Lion, the first edition of OS X with Share Sheets. Once people were signed into a service, they could simply click a button to publish to it, instead of having to use the Web or sign into an app's specific integration.\nThe switch in Mojave means that sharing options will disappear in many parts of macOS, assuming Apple decides to make the change permanent.\nThe move is presumably a part of Apple's attempts to bolster the overall security of macOS. The company is tightening permissions, making it harder for advertisers to \"fingerprint\" users, and creating the concept of \"notarized\" apps for titles sold outside the Mac App Store.",
        "sentiment": 0.21536026739825806
    },
    "article_5": {
        "title": "Apple is rebuilding Maps from the ground up",
        "body": "Apple is rebuilding Maps from the ground up The company is rolling out more detailed maps built from its own data for the first time\nI\u2019m not sure if you\u2019re aware, but the launch of Apple Maps went poorly. After a rough first impression, an apology from the CEO, several years of patching holes with data partnerships and some glimmers of light with long-awaited transit directions and improvements in business, parking and place data, Apple Maps is still not where it needs to be to be considered a world-class service.\nMaps needs fixing.\nApple, it turns out, is aware of this, so it\u2019s re-building the maps part of Maps.\nIt\u2019s doing this by using first-party data gathered by iPhones with a privacy-first methodology and its own fleet of cars packed with sensors and cameras. The new product will launch in San Francisco and the Bay Area with the next iOS 12 beta and will cover Northern California by fall.\nEvery version of iOS will get the updated maps eventually, and they will be more responsive to changes in roadways and construction, more visually rich depending on the specific context they\u2019re viewed in and feature more detailed ground cover, foliage, pools, pedestrian pathways and more.\nThis is nothing less than a full re-set of Maps and it\u2019s been four years in the making, which is when Apple began to develop its new data-gathering systems. Eventually, Apple will no longer rely on third-party data to provide the basis for its maps, which has been one of its major pitfalls from the beginning.\n\u201cSince we introduced this six years ago \u2014 we won\u2019t rehash all the issues we\u2019ve had when we introduced it \u2014 we\u2019ve done a huge investment in getting the map up to par,\u201d says Apple SVP Eddy Cue, who now owns Maps, in an interview last week. \u201cWhen we launched, a lot of it was all about directions and getting to a certain place. Finding the place and getting directions to that place. We\u2019ve done a huge investment of making millions of changes, adding millions of locations, updating the map and changing the map more frequently. All of those things over the past six years.\u201d\nBut, Cue says, Apple has room to improve on the quality of Maps, something that most users would agree on, even with recent advancements.\n\u201cWe wanted to take this to the next level,\u201d says Cue. \u201cWe have been working on trying to create what we hope is going to be the best map app in the world, taking it to the next step. That is building all of our own map data from the ground up.\u201d\nIn addition to Cue, I spoke to Apple VP Patrice Gautier and more than a dozen Apple Maps team members at its mapping headquarters in California this week about its efforts to re-build Maps, and to do it in a way that aligned with Apple\u2019s very public stance on user privacy.\nIf, like me, you\u2019re wondering whether Apple thought of building its own maps from scratch before it launched Maps, the answer is yes. At the time, there was a choice to be made about whether or not it wanted to be in the business of maps at all. Given that the future of mobile devices was becoming very clear, it knew that mapping would be at the core of nearly every aspect of its devices, from photos to directions to location services provided to apps. Decision made, Apple plowed ahead, building a product that relied on a patchwork of data from partners like TomTom, OpenStreetMap and other geo data brokers. The result was underwhelming.\nAlmost immediately after Apple launched Maps, it realized that it was going to need help and it signed on a bunch of additional data providers to fill the gaps in location, base map, point-of-interest and business data.\nIt wasn\u2019t enough.\n\u201cWe decided to do this just over four years ago. We said, \u2018Where do we want to take Maps? What are the things that we want to do in Maps?\u2019 We realized that, given what we wanted to do and where we wanted to take it, we needed to do this ourselves,\u201d says Cue.\nBecause Maps are so core to so many functions, success wasn\u2019t tied to just one function. Maps needed to be great at transit, driving and walking \u2014 but also as a utility used by apps for location services and other functions.\nCue says that Apple needed to own all of the data that goes into making a map, and to control it from a quality as well as a privacy perspective.\nThere\u2019s also the matter of corrections, updates and changes entering a long loop of submission to validation to update when you\u2019re dealing with external partners. The Maps team would have to be able to correct roads, pathways and other updating features in days or less, not months. Not to mention the potential competitive advantages it could gain from building and updating traffic data from hundreds of millions of iPhones, rather than relying on partner data.\nCue points to the proliferation of devices running iOS, now over a billion, as a deciding factor to shift its process.\n\u201cWe felt like because the shift to devices had happened \u2014 building a map today in the way that we were traditionally doing it, the way that it was being done \u2014 we could improve things significantly, and improve them in different ways,\u201d he says. \u201cOne is more accuracy. Two is being able to update the map faster based on the data and the things that we\u2019re seeing, as opposed to driving again or getting the information where the customer\u2019s proactively telling us. What if we could actually see it before all of those things?\u201d\nI query him on the rapidity of Maps updates, and whether this new map philosophy means faster changes for users.\n\u201cThe truth is that Maps needs to be [updated more], and even are today,\u201d says Cue. \u201cWe\u2019ll be doing this even more with our new maps, [with] the ability to change the map in real time and often. We do that every day today. This is expanding us to allow us to do it across everything in the map. Today, there\u2019s certain things that take longer to change.\n\u201cFor example, a road network is something that takes a much longer time to change currently. In the new map infrastructure, we can change that relatively quickly. If a new road opens up, immediately we can see that and make that change very, very quickly around it. It\u2019s much, much more rapid to do changes in the new map environment.\u201d\nSo a new effort was created to begin generating its own base maps, the very lowest building block of any really good mapping system. After that, Apple would begin layering on living location data, high-resolution satellite imagery and brand new intensely high-resolution image data gathered from its ground cars until it had what it felt was a \u201cbest in class\u201d mapping product.\nThere is only really one big company on earth that owns an entire map stack from the ground up: Google.\nApple knew it needed to be the other one. Enter the vans.\nApple vans spotted\nThough the overall project started earlier, the first glimpse most folks had of Apple\u2019s renewed efforts to build the best Maps product was the vans that started appearing on the roads in 2015 with \u201cApple Maps\u201d signs on the side. Capped with sensors and cameras, these vans popped up in various cities and sparked rampant discussion and speculation.\nThe new Apple Maps will be the first time the data collected by these vans is actually used to construct and inform its maps. This is their coming out party.\nSome people have commented that Apple\u2019s rigs look more robust than the simple GPS + Camera arrangements on other mapping vehicles \u2014 going so far as to say they look more along the lines of something that could be used in autonomous vehicle training.\nApple isn\u2019t commenting on autonomous vehicles, but there\u2019s a reason the arrays look more advanced: they are.\nEarlier this week I took a ride in one of the vans as it ran a sample route to gather the kind of data that would go into building the new maps. Here\u2019s what\u2019s inside.\nIn addition to a beefed-up GPS rig on the roof, four LiDAR arrays mounted at the corners and eight cameras shooting overlapping high-resolution images, there\u2019s also the standard physical measuring tool attached to a rear wheel that allows for precise tracking of distance and image capture. In the rear there is a surprising lack of bulky equipment. Instead, it\u2019s a straightforward Mac Pro bolted to the floor, attached to an array of solid state drives for storage. A single USB cable routes up to the dashboard where the actual mapping-capture software runs on an iPad.\nWhile mapping, a driver\u2026drives, while an operator takes care of the route, ensuring that a coverage area that has been assigned is fully driven, as well as monitoring image capture. Each drive captures thousands of images as well as a full point cloud (a 3D map of space defined by dots that represent surfaces) and GPS data. I later got to view the raw data presented in 3D and it absolutely looks like the quality of data you would need to begin training autonomous vehicles.\nMore on why Apple needs this level of data detail later.\nWhen the images and data are captured, they are then encrypted on the fly and recorded on to the SSDs. Once full, the SSDs are pulled out, replaced and packed into a case, which is delivered to Apple\u2019s data center, where a suite of software eliminates from the images private information like faces, license plates and other info. From the moment of capture to the moment they\u2019re sanitized, they are encrypted with one key in the van and the other key in the data center. Technicians and software that are part of its mapping efforts down the pipeline from there never see unsanitized data.\nThis is just one element of Apple\u2019s focus on the privacy of the data it is utilizing in New Maps.\nProbe data and privacy\nThroughout every conversation I have with any member of the team throughout the day, privacy is brought up, emphasized. This is obviously by design, as Apple wants to impress upon me as a journalist that it\u2019s taking this very seriously indeed, but it doesn\u2019t change the fact that it\u2019s evidently built in from the ground up and I could not find a false note in any of the technical claims or the conversations I had.\nIndeed, from the data security folks to the people whose job it is to actually make the maps work well, the constant refrain is that Apple does not feel that it is being held back in any way by not hoovering every piece of customer-rich data it can, storing and parsing it.\nThe consistent message is that the team feels it can deliver a high-quality navigation, location and mapping product without the directly personal data used by other platforms.\n\u201cWe specifically don\u2019t collect data, even from point A to point B,\u201d notes Cue. \u201cWe collect data \u2014 when we do it \u2014 in an anonymous fashion, in subsections of the whole, so we couldn\u2019t even say that there is a person that went from point A to point B. We\u2019re collecting the segments of it. As you can imagine, that\u2019s always been a key part of doing this. Honestly, we don\u2019t think it buys us anything [to collect more]. We\u2019re not losing any features or capabilities by doing this.\u201d\nThe segments that he is referring to are sliced out of any given person\u2019s navigation session. Neither the beginning or the end of any trip is ever transmitted to Apple. Rotating identifiers, not personal information, are assigned to any data or requests sent to Apple and it augments the \u201cground truth\u201d data provided by its own mapping vehicles with this \u201cprobe data\u201d sent back from iPhones.\nBecause only random segments of any person\u2019s drive is ever sent and that data is completely anonymized, there is never a way to tell if any trip was ever a single individual. The local system signs the IDs and only it knows to whom that ID refers. Apple is working very hard here to not know anything about its users. This kind of privacy can\u2019t be added on at the end, it has to be woven in at the ground level.\nBecause Apple\u2019s business model does not rely on it serving to you, say, an ad for a Chevron on your route, it doesn\u2019t need to even tie advertising identifiers to users.\nAny personalization or Siri requests are all handled on-board by the iOS device\u2019s processor. So if you get a drive notification that tells you it\u2019s time to leave for your commute, that\u2019s learned, remembered and delivered locally, not from Apple\u2019s servers.\nThat\u2019s not new, but it\u2019s important to note given the new thing to take away here: Apple is flipping on the power of having millions of iPhones passively and actively improving their mapping data in real time.\nIn short: Traffic, real-time road conditions, road systems, new construction and changes in pedestrian walkways are about to get a lot better in Apple Maps.\nThe secret sauce here is what Apple calls probe data. Essentially little slices of vector data that represent direction and speed transmitted back to Apple completely anonymized with no way to tie it to a specific user or even any given trip. It\u2019s reaching in and sipping a tiny amount of data from millions of users instead, giving it a holistic, real-time picture without compromising user privacy.\nIf you\u2019re driving, walking or cycling, your iPhone can already tell this. Now if it knows you\u2019re driving, it also can send relevant traffic and routing data in these anonymous slivers to improve the entire service. This only happens if your Maps app has been active, say you check the map, look for directions, etc. If you\u2019re actively using your GPS for walking or driving, then the updates are more precise and can help with walking improvements like charting new pedestrian paths through parks \u2014 building out the map\u2019s overall quality.\nAll of this, of course, is governed by whether you opted into location services, and can be toggled off using the maps location toggle in the Privacy section of settings.\nApple says that this will have a near zero effect on battery life or data usage, because you\u2019re already using the \u2018maps\u2019 features when any probe data is shared and it\u2019s a fraction of what power is being drawn by those activities.\nFrom the point cloud on up\nBut maps cannot live on ground truth and mobile data alone. Apple is also gathering new high-resolution satellite data to combine with its ground truth data for a solid base map. It\u2019s then layering satellite imagery on top of that to better determine foliage, pathways, sports facilities, building shapes and pathways.\nAfter the downstream data has been cleaned up of license plates and faces, it gets run through a bunch of computer vision programming to pull out addresses, street signs and other points of interest. These are cross referenced to publicly available data like addresses held by the city and new construction of neighborhoods or roadways that comes from city planning departments.\nBut one of the special sauce bits that Apple is adding to the mix of mapping tools is a full-on point cloud that maps in 3D the world around the mapping van. This allows them all kinds of opportunities to better understand what items are street signs (retro-reflective rectangular object about 15 feet off the ground? Probably a street sign) or stop signs or speed limit signs.\nIt seems like it also could enable positioning of navigation arrows in 3D space for AR navigation, but Apple declined to comment on \u201cany future plans\u201d for such things.\nApple also uses semantic segmentation and Deep Lambertian Networks to analyze the point cloud coupled with the image data captured by the car and from high-resolution satellites in sync. This allows 3D identification of objects, signs, lanes of traffic and buildings and separation into categories that can be highlighted for easy discovery.\nThe coupling of high-resolution image data from car and satellite, plus a 3D point cloud, results in Apple now being able to produce full orthogonal reconstructions of city streets with textures in place. This is massively higher-resolution and easier to see, visually. And it\u2019s synchronized with the \u201cpanoramic\u201d images from the car, the satellite view and the raw data. These techniques are used in self-driving applications because they provide a really holistic view of what\u2019s going on around the car. But the ortho view can do even more for human viewers of the data by allowing them to \u201csee\u201d through brush or tree cover that would normally obscure roads, buildings and addresses.\nThis is hugely important when it comes to the next step in Apple\u2019s battle for supremely accurate and useful Maps: human editors.\nApple has had a team of tool builders working specifically on a toolkit that can be used by human editors to vet and parse data, street by street. The editor\u2019s suite includes tools that allow human editors to assign specific geometries to flyover buildings (think Salesforce tower\u2019s unique ridged dome) that allow them to be instantly recognizable. It lets editors look at real images of street signs shot by the car right next to 3D reconstructions of the scene and computer vision detection of the same signs, instantly recognizing them as accurate or not.\nAnother tool corrects addresses, letting an editor quickly move an address to the center of a building, determine whether they\u2019re misplaced and shift them around. It also allows for access points to be set, making Apple Maps smarter about the \u201clast 50 feet\u201d of your journey. You\u2019ve made it to the building, but what street is the entrance actually on? And how do you get into the driveway? With a couple of clicks, an editor can make that permanently visible.\nWhen you look at places like San Francisco or big cities from that standpoint,\u201d says Cue, \u201cyou have addresses where the address name is a certain street, but really, the entrance in the building is on another street. They\u2019ve done that because they want the better street name. Those are the kinds of things that our new Maps really is going to shine on. We\u2019re going to make sure that we\u2019re taking you to exactly the right place, not a place that might be really close by.\u201d\nWater, swimming pools (new to Maps entirely), sporting areas and vegetation are now more prominent and fleshed out thanks to new computer vision and satellite imagery applications. So Apple had to build editing tools for those, as well.\nMany hundreds of editors will be using these tools, in addition to the thousands of employees Apple already has working on maps, but the tools had to be built first, now that Apple is no longer relying on third parties to vet and correct issues.\nAnd the team also had to build computer vision and machine learning tools that allow it to determine whether there are issues to be found at all.\nAnonymous probe data from iPhones, visualized, looks like thousands of dots, ebbing and flowing across a web of streets and walkways, like a luminescent web of color. At first, chaos. Then, patterns emerge. A street opens for business, and nearby vessels pump orange blood into the new artery. A flag is triggered and an editor looks to see if a new road needs a name assigned.\nA new intersection is added to the web and an editor is flagged to make sure that the left turn lanes connect correctly across the overlapping layers of directional traffic. This has the added benefit of massively improved lane guidance in the new Apple Maps.\nApple is counting on this combination of human and AI flagging to allow editors to first craft base maps and then also maintain them as the ever-changing biomass wreaks havoc on roadways, addresses and the occasional park.\nHere there be Helvetica\nApple\u2019s new Maps, like many other digital maps, display vastly differently depending on scale. If you\u2019re zoomed out, you get less detail. If you zoom in, you get more. But Apple has a team of cartographers on staff that work on more cultural, regional and artistic levels to ensure that its Maps are readable, recognizable and useful.\nThese teams have goals that are at once concrete and a bit out there \u2014 in the best traditions of Apple pursuits that intersect the technical with the artistic.\nThe maps need to be usable, but they also need to fulfill cognitive goals on cultural levels that go beyond what any given user might know they need. For instance, in the U.S., it is very common to have maps that have a relatively low level of detail even at a medium zoom. In Japan, however, the maps are absolutely packed with details at the same zoom, because that increased information density is what is expected by users.\nThis is the department of details. They\u2019ve reconstructed replicas of hundreds of actual road signs to make sure that the shield on your navigation screen matches the one you\u2019re seeing on the highway road sign. When it comes to public transport, Apple licensed all of the type faces that you see on your favorite subway systems, like Helvetica for NYC. And the line numbers are in the exact same order that you\u2019re going to see them on the platform signs.\nIt\u2019s all about reducing the cognitive load that it takes to translate the physical world you have to navigate into the digital world represented by Maps.\nBottom line\nThe new version of Apple Maps will be in preview next week with just the Bay Area of California going live. It will be stitched seamlessly into the \u201ccurrent\u201d version of Maps, but the difference in quality level should be immediately visible based on what I\u2019ve seen so far.\nBetter road networks, more pedestrian information, sports areas like baseball diamonds and basketball courts, more land cover, including grass and trees, represented on the map, as well as buildings, building shapes and sizes that are more accurate. A map that feels more like the real world you\u2019re actually traveling through.\nSearch is also being revamped to make sure that you get more relevant results (on the correct continents) than ever before. Navigation, especially pedestrian guidance, also gets a big boost. Parking areas and building details to get you the last few feet to your destination are included, as well.\nWhat you won\u2019t see, for now, is a full visual redesign.\n\u201cYou\u2019re not going to see huge design changes on the maps,\u201d says Cue. \u201cWe don\u2019t want to combine those two things at the same time because it would cause a lot of confusion.\u201d\nApple Maps is getting the long-awaited attention it really deserves. By taking ownership of the project fully, Apple is committing itself to actually creating the map that users expected of it from the beginning. It\u2019s been a lingering shadow on iPhones, especially, where alternatives like Google Maps have offered more robust feature sets that are so easy to compare against the native app but impossible to access at the deep system level.\nThe argument has been made ad nauseam, but it\u2019s worth saying again that if Apple thinks that mapping is important enough to own, it should own it. And that\u2019s what it\u2019s trying to do now.\n\u201cWe don\u2019t think there\u2019s anybody doing this level of work that we\u2019re doing,\u201d adds Cue. \u201cWe haven\u2019t announced this. We haven\u2019t told anybody about this. It\u2019s one of those things that we\u2019ve been able to keep pretty much a secret. Nobody really knows about it. We\u2019re excited to get it out there. Over the next year, we\u2019ll be rolling it out, section by section in the U.S.\u201d",
        "sentiment": 0.22198256748922907
    },
    "article_6": {
        "title": "All Apple Park employees getting standing desks because \u2018sitting is the new cancer\u2019",
        "body": "One of the things we learned during David Rubenstein\u2019s interview of Tim Cook, posted in full yesterday, was that all employees at Apple Park are getting standing desks.\nWe have given all of our employees, 100%, standing desks. If you can stand for a while, then sit, and so on and so forth, it\u2019s much better for your lifestyle.\nThe desks allow employees to switch between sitting and standing.\nAs Business Insider notes, Cook has previously described sitting as \u2018the new cancer.\u2019 He made the comment back in 2015 when talking about the stand reminders on the Apple Watch.\nIf I sit for too long, it will actually tap me on the wrist to remind me to get up and move, because a lot of doctors think sitting is the new cancer. Ten minutes before the hour, it will remind you to move. We have a lot of people using the Apple Watch at Apple, and ten minutes before the hour, suddenly they all get up and move. It took a little to get used to, but it\u2019s great.\nNo details have yet been shared on which desks have been chosen, but we have seen them in interior photos of the campus. They have minimalist up/down controls, and my guess is that they were custom-made for Apple.\n@businessinsider \u201cApple reveals top secret detail: Apple Park has custom standing desks, images of which have yet to be seen.\u201d@WIRED and @dezeen: \u201cWe photographed two variants of these desks a year ago, including close-ups of the buttons underneath that control them.\u201d\n\ud83d\ude43 pic.twitter.com/jR3BsAaX72 \u2014 \u3164 (@domneill) June 14, 2018\nTo accompany the desks, Apple opted for Vitra chairs rather than the Aeron ones chosen by most large organizations, in part because the company didn\u2019t want staff getting too comfortable sitting for long periods of time.\nToday, you may have noticed, the prevailing ideal is that you spend most of your day collaborating with other people, that meetings and conference calls are the center of the workday. What quiet time you do have is spent wherever suits your mood\nWe reviewed the chair and were impressed. We\u2019ve also tried out a few standing desks, from FluidStance, UpDesk and UpLift.\nCheck out 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": 0.09344928818089622
    },
    "article_7": {
        "title": "A Third Class Action has been filed against Apple in California over MacBook Pro Keyboard Problems",
        "body": "Apple was hit with two class actions (one and two) in May over the MacBook Pro's faulty keyboard that uses a butterfly key mechanism. Now that June has kicked in, a third class action lawsuit over the same issue was filed yesterday June 2, 2018 on behalf of Diego Binatena individually and on behalf of all others similarly situated\nJust prior to the class actions being filed in May, a petition came to light from Charge.org that demanded Apple take action to fix their MacBook Pro's faulty keyboard. The petition was backed by a number of longstanding Apple bloggers such as John Gruber of Daring Fireball.\nOverview of the Action\nThe Plaintiff's complaint before the court begins with an overview of the action that is presented as follows:\n\"Plaintiff brings this class action on behalf of himself and others who purchased MacBook Pro laptop computers on or after late 2016 with defective \"butterfly\" keyboards. These keyboards are prone to failure, and, when replaced, often fail again.\nApple is a market leader in the manufacture, marketing and sale of computers and computing devices. In late 2016, Apple introduced a new version of its high-end MacBook Pro laptop, featuring new a \"butterfly\" keyboard, which is more compact than previous keyboards. This keyboard, however, is particularly prone to failure, especially when dust or other debris becomes lodged underneath the keys.\nThe cost of replacing a keyboard is approximately $700.\nUnfortunately, replacing failed keyboards with the same butterfly keyboard does not fix the issue \u2013 the replacement keyboards are also prone to the same failure.\nKeyboards are an essential part of a laptop computer. Without a functioning keyboard, the utility and value of a laptop is vastly diminished.\nPlaintiff brings this action for monetary, declaratory and equitable relief for Apple's: 1) breach of its express and implied warranties; 2) breach of the Magnuson-Moss Warranty Act and the Song-Beverly Consumer Warranty Act; 3) breach of the duty of good faith and fair dealing; 4) breach of California's Unfair Competition Law; 5) breach of California's Consumers Legal Remedies Act; and 6) fraudulent concealment. \"\nIn his testimony Mr. Binatena further described how the \"b, n and m keys\" were unusable. He brought his MacBook Pro to an Apple Store for repair and they only fixed the \"b\" key by replacing it.\nHe went back to the store 2-3 weeks later, being dissatisfied with the repair, and was told that his MacBook Pro was now out of warranty and it would cost $500 just for a full diagnosis of the problem and additional money for any repairs.\nWhy wasn't a full diagnosis done the first time around? I guess that will come to light during the trial if the class action gets the green light to proceed.\nCauses for Action\nCount 1: Violation of the California Consumers Legal Remedies Act\nCount 2: Violation of Bus. & Prof. Code \u00a7 17200\nCount 3: Fraudulent Concealment\nCount 4: Breach of Written Warranty\nCount 5: Breach of Implied Warranty\nCount 6: Violation of Magnuson-Moss Warranty Act, 15 U.S.C. \u00a7 2301\nCount 7: Violation of Song-Beverly Consumer Warranty Act, Cal. Civ. Code \u00a7 1792\nCount 8: Breach of Contract/Duty of Good Faith and Fair Dealing\nDiego Binatena's attorney Scott C. Borison of Legg Law Firm, LLP filed this class action against Apple Inc. on June 2, 2018 in the California Northern District Court, San Jose Office in Santa Clara.\nAbout Comments: Patently Apple reserves the right to post, dismiss or edit comments. Those using abusive language or negative behavior will result in being blacklisted on Disqus.",
        "sentiment": -0.31292831655591724
    },
    "article_8": {
        "title": "Apple quietly banned developers from selling and sharing users\u2019 contacts",
        "body": "Apple has quietly tightened its App Store rules to better protect users from developers who want to harvest their data or sell it to third parties. Previously, developers would ask for users\u2019 phone contacts and sometimes then sold that data without the explicit consent of the users or their contacts, according to Bloomberg, which spotted the change.\nThe changes to the rules, which were made last week, explicitly state that developers are banned from turning address books into a database of contacts and from selling that database. They also can\u2019t turn data into user profiles. Developers can still ask users for contact lists for use within their app, but they\u2019ll have to tell users exactly what they\u2019re going to do with the data. If they have more than one purpose in mind, they\u2019ll have to ask for further consent.",
        "sentiment": -0.7087413966655731
    },
    "article_9": {
        "title": "Supply Chain Rumor Claims Apple will switch to USB Type-C in its Series of 2019 iPhones",
        "body": "According to a new supply chain report this morning, Apple is redesigning chargers and related interface for its next-generation iPhone and iPad devices, and will likely have its 2019 series of iPhones come with USB Type-C support. The rumor was generated from sources at analog IC vendors.\nThe adoption of USB Type-C in Apple's MacBook series has already encouraged other notebook vendors to follow suit. However, sales of their new models that come with a Type-C port have been affected negatively by a general slowdown in the global PC market.\nApple's adoption of Type-C in its iPhones will accelerate other smartphone companies' adoption of the interface in their products, the sources indicated. The popularity of Type-C interface among handsets will still depend on the adoption in Apple's iPhones, nevertheless, the sources said.\nSpeculation circulated previously in the smartphone market that Apple would adopt Type-C interface in its next-generation iPhone series slated for launch later in 2018. Apple is still in its redesign phase and will not be able to equip the technology in its upcoming iPhones, the sources claimed.\nIn February a Wall Street Journal report hinted that Apple would be dropping their proprietary lightning connector for USB-C.\nSamsung's Galaxy S9 smartphone uses USB Type-C as well as most Android smartphones.\nAbout Making Comments on our Site: Patently Apple reserves the right to post, dismiss or edit any comments. Those using abusive language or negative behavior will result in being blacklisted on Disqus.",
        "sentiment": -0.23112707616140446
    },
    "article_10": {
        "title": "Keyboard Service Program for MacBook and MacBook Pro",
        "body": "Please choose one of the options below for service. Your Mac notebook will be examined prior to any service to verify that it is eligible for this program.\nThe type of service will be determined after the keyboard is examined and may involve the replacement of one or more keys or the whole keyboard. The service turn-around time may vary depending upon the type of service and availability of replacement parts.",
        "sentiment": 0.08883096743375063
    },
    "article_11": {
        "title": "Apple Confirms Mojave is the Last macOS Release to Support 32-Bit Apps",
        "body": "As expected, Apple confirmed yesterday during its WWDC keynote that macOS 10.14 Mojave will be the last version of macOS to support legacy 32-bit apps.\nApple commenced its plan to begin phasing out 32-bit apps on Macs in macOS High Sierra 10.13.4. When a 32-bit app is opened in High Sierra, users get a warning about its future incompatibility with the macOS operating system.\nLikewise, when opening 32-bit apps in macOS 10.14 Mojave (beta 1), users are shown a dialog box with a similar message telling them that \"This app will not work with future versions of macOS\". Clicking \"OK\" on the prompt then allows the app to open.\nCurrently, the warning is only shown one time for each app. That could well change in subsequent betas of macOS Mojave, however, since Apple previously said it would include \"aggressive\" warnings about 32-bit apps in the next version of macOS after High Sierra before they are phased out entirely.\n32-bit app warning in macOS 10.14 Mojave (beta 1)\nApple's effort to phase them out on Macs mirrors the path it took when ending 32-bit app support on iOS devices. In iOS 10, Apple provided increasingly more insistent warnings to let users know that their apps wouldn't work with future versions of iOS before phasing out 32-bit support entirely in iOS 11.\nOnce 32-bit apps are phased out on Macs, they won't be able to be used at all, so users will need to find replacements for older 32-bit apps that aren't likely to be updated to 64-bit. You can find out which apps on your Mac are still running in 32-bit by following our how-to guide.",
        "sentiment": -0.287400042553517
    },
    "article_12": {
        "title": "Samsung rejects $539M Apple patent infringement verdict, demands a retrial",
        "body": "The seven-year long battle between Apple and Samsung over the Korean company infringing iPhone patents seemingly isn\u2019t over yet.\nSamsung has rejected the \u2018final\u2019 $533M damages awarded against it, and has now filed a motion to have the verdict thrown out \u2026\nLaw360 reports that Samsung has asked for the decision to be reversed, or a new trial to be held.\nIn Samsung Electronics Co. Ltd.\u2019s 34-page post-trial motion, the Korean tech giant said the jury\u2019s verdict, which awarded Apple Inc. about half of what it requested, wasn\u2019t supported by the evidence and \u201cno reasonable jury\u201d could reach the jury\u2019s conclusion. \u201cThe jury\u2019s verdict is excessive and against the weight of the evidence on each and every issue identified above, and \u2026 the evidence supports a verdict of no more than $28.085 million,\u201d the motion said.\nSamsung\u2019s motion appears to mostly reiterate the points it made in court, arguing once more that the \u2018article of manufacture\u2019 found to have infringed Apple\u2019s patents should have been specific components, not the whole phone.\nJust for good measure, Samsung is also demanding a refund on a separate $145M award in an unrelated patent infringement case relating to multi-touch. Samsung argues that since the patent was later invalidated, Apple must reimburse the sum paid.\nApple has until June 21 to respond to the motion, and a hearing on it has been scheduled for July 26.\nImage: CNET\nCheck out 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.42392511796206234
    },
    "article_13": {
        "title": "Got 11k? iMac Pro with 128GB RAM hits the Refurb Store.",
        "body": "Features\nPower to the pro.\nPros love iMac. So we created one just for you. It\u2019s packed with the most powerful graphics and processors ever in a Mac, along with the most advanced storage, memory, and I/O \u2014 all behind a breathtaking Retina 5K display in a sleek, all-in-one design. For everyone from photographers to video editors to 3D animators to musicians to software developers to scientists, iMac Pro is ready to turn your biggest ideas into your greatest work.\nUp to 18 cores in an iMac. No, that\u2019s not a typo.\nAn iMac with 4 cores is remarkable enough. But an iMac with 8, 10, 14, or 18 cores is an entirely different creature. Add Turbo Boost speeds up to 4.5GHz, and iMac Pro has the power and flexibility to balance multicore processing with single-thread performance. With new AVX-512 vector instructions and a new cache architecture, the processor handles even more data \u2014 even more quickly. Which means you can render images, edit up to 8K video, manipulate photos, create real-time audio effects, or compile your next five-star app \u2014 all at lightning speed.\nVega graphics. The beast behind the beauty.\nFeaturing Radeon Pro Vega graphics, iMac Pro delivers the best workstation-class graphics of any Mac. The first Mac with Vega architecture features up to 16GB of High Bandwidth Memory, which helps deliver a major jump in performance \u2014 up to two times faster than any other iMac GPU and up to three times faster than the GPU in Mac Pro. This translates to higher frame rates for VR, real-time 3D rendering, more lifelike special effects, and gameplay at max settings. It also supports both single- and half-precision computing, so operations that don\u2019t require a full 32 bits of precision can be performed twice as fast. How fast? Up to 22 teraflops fast.\nRetina 5K display. Dream in a billion colors.\nIt almost goes without saying: If it\u2019s an iMac, it will have a gorgeous display. And iMac Pro doesn\u2019t disappoint. In fact, its 27-inch screen is our best ever, with 500 nits of brightness across its 14.7 million pixels. It\u2019s as if the sleek, all-in-one design disappears behind the stunning screen \u2014 so you can focus on your content. Along with P3 color and support for a billion colors, iMac Pro delivers spectacular, true-to-life images.\nHigh-performance storage. Load faster. Launch faster.\nUp to 4TB of all-flash storage lets you work on large 4K and HD projects regardless of codec. And with up to 3GB/s throughput, loading huge files and launching apps is faster than ever.\nT2. Enhanced integration and security.\nIntroducing the Apple T2 chip, our second-generation custom Mac silicon. By redesigning and integrating several controllers found in other Mac systems \u2014 like the system management controller, image signal processor, audio controller, and SSD controller \u2014 T2 delivers new capabilities to the Mac. For instance, the T2 image signal processor works with the FaceTime HD camera to enable enhanced tone mapping, improved exposure control, and face detection\u2013based auto exposure and auto white balance. T2 also makes iMac Pro even more secure, thanks to a Secure Enclave coprocessor that provides the foundation for new encrypted storage and secure boot capabilities. The data on your SSD is encrypted using dedicated AES hardware with no effect on the SSD\u2019s performance, while keeping the Intel Xeon processor free for your compute tasks. And secure boot ensures that the lowest levels of software aren\u2019t tampered with and that only operating system software trusted by Apple loads at startup.\nSerious I/O. For serious work.\nAs you\u2019d expect from a pro machine, there are plenty of high-speed ports to create a high-performance workstation. Four Thunderbolt 3 ports let you connect two 5K monitors \u2014 for 44 million pixels total \u2014 and transfer data at a blazing 40Gb/s. And there\u2019s something you probably didn\u2019t expect in an iMac: 10Gb Ethernet. So sharing files between desktops, or working with high-performance network storage, is up to 10 times faster.\nmacOS",
        "sentiment": 0.618222505805771
    },
    "article_14": {
        "title": "iOS 12 will upgrade ARKit with ability for two iPhones to see same virtual object",
        "body": "As we inch closer to Apple\u2019s WWDC keynote on Monday, new details are starting to slip out about what the company plans to present. Reuters is reporting that Apple plans to upgrade ARKit, Apple\u2019s augmented reality framework for iOS, with the ability for two users to see the same virtual object in space. The feature will reportedly be implemented in a way that considers data privacy for users as well.\nReuters says Apple is avoiding privacy concerns with uploading imagery to Apple\u2019s servers by keeping the process local between iPhones:\nApple next week will debut tools to let two iPhone users share augmented reality. The goal is for two users to see the same virtual object in the same space via their individual devices. The approach, which has not been previously reported, differs from Google\u2019s, which requires scans of a player\u2019s environment to be sent to, and stored in, the cloud.\nThe report also mentions Bloomberg\u2019s earlier reporting that originated last November and says Apple plans to introduce AR-powered gaming between players, although it sounds like the technology described in today\u2019s report will be used in the earlier report.\nAt any rate, we\u2019ll see what exactly Apple has planned on Monday. Stay tuned for live coverage on 9to5Mac from San Jose.\nSubscribe to 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.18365558837963777
    },
    "article_15": {
        "title": "Apple revised App Store rule after rejecting Valve\u2019s game streaming app Steam Link. Probably means it\u2019s coming very soon to iOS",
        "body": "Apple quietly updated its App Store guidelines on Monday, while the keynote address of WWDC was happening in San Jose, to allow for mobile apps like Valve\u2019s Steam Link game streaming service, according to Reuters. Steam owner Valve said last week Apple had initially rejected Steam Link, which lets consumers access and stream their PC game library onto a mobile device, due to apparent \u201cbusiness\u201d conflicts.\nThe change does not void Apple\u2019s long-standing policy that no iOS app may host an app store of its own and allow users to purchase and download software not approved through its own marketplace. Apple does that because it likes to keep tight control over the type of content allowed on iOS devices, and also because it has since the advent of the App Store typically taken 30 percent of all revenue. (In 2016, Apple adjusted that policy to let developers keep an additional 15 percent of in-app revenue for subscription services, so long as a user was subscribed for at least 12 months.) Apple\u2019s growing services business, which includes the App Store, is a pillar of its evolving business model, so it makes sense that Apple would prevent any app that tries to undermine that.",
        "sentiment": 0.20502131261552373
    },
    "article_16": {
        "title": "Apple invests $25 billion in 5-nanometer node technology for future A-series chips.",
        "body": "Apple supplier TSMC will reportedly invest a massive $25 billion in 5 nanometer node technology in its quest to fabricate next-generation chips which could help power iOS devices.\nNo timeframe was announced for the investment. Since the company is supposedly currently fabricating 7nm chips for the 2018 iPhone, this will hopefully help TSMC hold onto its status as Apple\u2019s A-series chipmaker for at least a while longer.\nTSMC has been the exclusive producer of Apple\u2019s A-series chips since the A9-series days. However, it is constantly having to stay on top of its game as the result of competition from Samsung.\nThe squabble to win orders between the two companies has resulted in plenty of drama \u2014 even including a lawsuit when a former TSMC employee was charged with leaking trade secrets to Samsung. In the past, Foxconn CEO Terry Gou has gotten involved to lobby on behalf of TSMC, prompted by fears that Samsung will take over more and more of the work involved in producing iPhone components.\nAs ever, working for Apple as a supplier is no easy ride. While there is always competition from other companies, Apple is constantly on the lookout for ways to cut costs \u2014 which often means paying suppliers less. Recently, for instance, Apple stopped suppliers negotiating their own deals for third party components like screws \u2014 thereby stopping them being able to carve out a few extra bucks of profit.\nNonetheless, when you\u2019re dealing with the kind of massive orders Apple can hand out, it\u2019s no surprise that the likes of TSMC are so keen to hold on to Apple\u2019s business!\nSource: Reuters\nUpdate: A previous version of this story suggested Apple had directly invested the money, rather than TSMC. I have updated the story to clarify this.",
        "sentiment": -0.08243853885393876
    },
    "article_17": {
        "title": "PSA: Apple Maps is Down for Many Users as 'Directions Not Available' Right Now",
        "body": "If you are attempting to navigate with Apple Maps right now, you may be unable to do so, as the service appears to be experiencing downtime.\n\"Directions Not Available,\" the app says, when attempting to search for or navigate to destinations. \"Route information is not available at this moment.\"\nThe issue appears to be widespread, with many users reporting that Apple Maps is down across the United States, Canada, and other countries. The outage affects all platforms with Apple Maps, including the iPhone, iPad, Mac, Apple Watch, and CarPlay.\niPhone, iPad, and Mac users can simply use an alternative such as Google Maps or Waze in the meantime, but CarPlay doesn't currently support third-party mapping apps, something that is changing with iOS 12.\nApple has yet to update its system status page to reflect any issues. We'll update this article if that changes or when the issues are resolved.\nUpdate 8:18 AM: Apple has now acknowledged and is \"investigating\" ongoing issues with Maps Search and Routing & Navigation, according to its system status page. Apple says \"all users are affected.\"\nUpdate 10:13 AM: Directions appear to now be working in Maps, although Apple has yet to update its status page to confirm the issues have been resolved.\nUpdate 10:28 AM: Apple has updated its status page to note the issues have been resolved as of 10:05 AM Pacific Time.",
        "sentiment": -0.3036769757357736
    },
    "article_18": {
        "title": "Not Only Is the Mac Mini Outdated, It\u2019s No Longer Mini",
        "body": "Jason Snell, writing at Six Colors back in March:\nThe Mac mini was last updated 1245 days ago, in October of 2014. (And that was a lackluster upgrade.) Taking a cue from my dreams about what a modern Mac mini might be like, I bought a tiny Intel NUC PC and installed macOS on it. My Mac mini was becoming unreliable and I was hoping to experiment with Intel\u2019s hardware in advance of a real Mac mini being released. This was intended to be a temporary experiment. And, in fact, I hope to replace the NUC with a real Mac mini just as soon as Apple finally releases that all-new Mac mini that\u2019s hopefully percolating inside Cupertino. But in the meantime, I have been running macOS on non-Apple hardware, and it\u2019s been an instructive experience.\nCheaper and faster, but a pain in the ass to keep updated software-wise. All of that is to be expected. But the striking thing to me is just how much smaller the Intel NUC is. It\u2019s only a little bit bigger than an Apple TV. Calling the Mac Mini \u201cmini\u201d is absurd in 2018.\nI wrote about this last September, when the Apple TV 4K came out:\nApple TV 4K is tiny compared to a Mac Mini, but judging by Geekbench scores (Mac Mini; iPad Pro, which uses the A10X in the Apple TV) it\u2019s a slightly faster computer than even the maxed-out Mac Mini configuration. Apple TV 4K probably has better GPU performance too. In addition to all the performance problems stemming from the fact that the Mac Mini hasn\u2019t been updated in three years, it\u2019s also inarguable that it\u2019s no longer even \u201cmini\u201d. You could arrange four Apple TV units in a 2 \u00d7 2 square and they\u2019d take up the same volume as one Mac Mini.\nApple TV proves that Apple can make an amazing compact puck-sized computer. They just seem to have lost any interest in making one that runs MacOS.\n\u2605 Thursday, 14 June 2018",
        "sentiment": -0.17544554939700496
    },
    "article_19": {
        "title": "Apple\u2019s Spike Jonze-directed HomePod ad wins \u2018advertising excellence\u2019 award at AICP show",
        "body": "Coinciding with the release of the HomePod earlier this year, Apple debuted a trippy 4-minute advertisement for the smart speaker directed by Spike Jonze and starring FKA Twigs. Now, that ad has taken home an award at the Association of Independent Commercial Producers Show\u2026\nAs reported by AdAge, the HomePod advertisement won in the \u201cAdvertising Excellence/Single Commercial\u201d category:\nApple\u2019s musical short film was directed by Spike Jonze from MJZ and features FKA Twigs as a forlorn office worker who finds her world literally expanded by a song by Anderson .Paak played on her Apple HomePod virtual assistant. The spot won in the Advertising Excellence/Single Commercial category.\nThe AICP awards are held every year and highlight the best of that year\u2019s advertisements. Founded in 1972, The Association of Independent Commercial Producers is a nonprofit organization that \u201crepresents the interests of United States companies that specialize in commercial production.\u201d\nApple\u2019s Spike Jonze-directed HomePod ad was widely praised when it debuted. A behind the scenes look at its creation showed how the ad favored practical efforts over CGI. The colorful set was primarily built by hand, as opposed to generated with computer graphics.\nWatch the ad in its entirety below:\nRelated stories:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": 0.48677999864925037
    },
    "article_20": {
        "title": "Apple CEO Tim Cook: 'Privacy is a Fundamental Human Right'",
        "body": "Apple CEO Tim Cook this evening sat down for an exclusive interview with CNN's Laurie Segall, where he discussed everything from his iPhone usage habits to the importance of privacy.\nOn the subject of device usage, Cook says that when he started using the new Screen Time feature built into iOS 12, he found he was spending too much time on the iPhone. \"I found I was spending a lot more time than I should,\" he said. \"I thought I was fairly disciplined about this. And I was wrong.\"\nCook said that Apple is aiming to provide the tools to consumers to let them make their own decisions about device usage, and what's considered too much will vary from person to person. He said that while Apple wants customers to be \"incredibly satisfied and empowered,\" it's not the goal to get customers to spend all of their time on iOS devices.\nI think the power is now shifted to the user and that has been What Apple has always been about - giving the power from the institution to the user. I am hopeful great things are going to happen from this.\nCook reiterated his stance on privacy, and called it a \"fundamental human right,\" as he has done in the past. He said it's \"not healthy\" to point a finger at companies like Facebook, and instead, we should be focusing on making the web an \"unbelievable place.\"\nTo me, and we feel this very deeply, we think privacy is a fundamental human right. So that is the angle that we look at it. Privacy from an American point of view is one of these key civil liberties that define what it is to be American.\nCook said customers can trust Apple to be \"on their side.\" \"We're the trusted adviser and company here,\" he said, explaining that people are not fully aware of how their data is being used and who has access to it. \"I think this needs to be addressed.\"\nThe interviewer asked Cook whether or not he was concerned about machines taking over the world, and he said it's not something that he worries about. Instead, he is concerned about people becoming more machine-like.\nI don't subscribe to machines the taking over the world. And I don't worry about that. I worry much more about people thinking like machines then machines thinking like people. I mean forgetting the humanity in things. Forgetting that all of our products should be infused with humanity. Forgetting that we have a broader obligation to society.\nCook said he doesn't consider himself to be political, and suggested that he would not run for political office.\nI'm not political. I'm not sure I would really do well in that environment. I think that I can make the greatest contribution doing what I'm doing. [...] I love getting things done and I don't love the political machine in the background. I love Apple, it is the privilege of a lifetime to be leading this company at this time.\nHe also said that he believes Steve Jobs would support Apple's current trajectory when asked how Jobs would feel \"about this moment in time.\"\nIn terms of the broader issue of humanity, that was his philosophy. That is the DNA of this company. Apple should always be trying to change the world, and change means make it better. That is the thing that we get up in the morning and focus on doing. And I don't see that changing. That is the north star that keeps us going.\nSeveral snippets of Cook's full interview, which cover topics like DACA, immigration, tariffs, and more, are available over on CNN and are well worth checking out for those who want to see everything Cook had to say this evening.\nCook also did a separate interview with NPR, which covered topics like privacy, rumors that Apple had access to Facebook users' personal information, the new Screen Time feature, and the Trump administration.",
        "sentiment": 0.08803769757505506
    },
    "article_21": {
        "title": "Third-party password managers will be able to integrate with AutoFill in iOS 12!",
        "body": "Machine learning\nCore ML\nUse Xcode 14 to analyze and optimize your Core-ML-powered features. Generate performance reports for Core ML models on your Mac or any connected iOS device without having to write any code. Review a summary of load and prediction times along with a breakdown of compute unit usage. Profile your app to view Core ML API calls and associated models using the Core ML template in Instruments. Combine information from the Core ML, Neural Engine, and GPU Instruments to track when and where models are executed on accelerated hardware. Aggregate timing data is summarized for each event, model, and submodel.\nThe Core ML framework now supports Float16 input and output feature types. This, combined with APIs for supplying your own output buffer backings for predictions, enables more control of how efficiently data flows in and out of your Core ML models. Support for sparse weight compression, restricting compute to the CPU and Neural Engine, and in-memory model instantiation are also now available.\nCreate ML app\nInteractively learn about your model\u2019s accuracy in the new evaluation UI in the Create ML app. Explore key metrics and their connections to specific examples to help identify challenging use cases and further investments in data collection to help improve model quality. Preview your model\u2019s predictions on live video from your iPhone camera.\nCreate ML framework\nCreate ML is now available as a Swift framework on tvOS, along with iOS, iPadOS, and macOS. In addition to task-specific training APIs being available for many common model types, you can now define your own custom model and training pipelines by combining a rich set of ML building blocks with the new Create ML Components framework.\nWatch the latest videos",
        "sentiment": 0.5691047364047596
    },
    "article_22": {
        "title": "Game Devs Express New Fears Over Future of Mac Gaming as Apple Deprecates OpenGL and OpenCL in macOS Mojave",
        "body": "Yesterday at WWDC 2018, Apple revealed macOS Mojave, which is set to bring users a Dark Mode, redesigned Mac App Store, organizable Stacks, streamlined screenshots, and more when it launches wide in the fall. Alongside the new features, Apple has confirmed that it is deprecating OpenGL (Open Graphics Library) and OpenCL (Open Computing Language) in favor of Metal.\nThis means that apps built using OpenGL and OpenCL will still run in Mojave, but they will no longer be updated after macOS 10.14 launches. Apple encourages games and \"graphics-intensive apps\" built with OpenGL to adopt Metal ahead of Mojave's launch, and apps that use OpenCL for computational tasks \"should now adopt Metal and Metal Performance Shaders.\"\nRise of the Tomb Raider: 20 Year Celebration is one of the latest Mac games to run on Metal\nLaunched four years ago, Metal is Apple's own 3D graphic and programming interface that combines the functions of OpenGL and OpenCL under a singular API. In explaining the move of deprecating the \"legacy technologies\" of OpenGL and OpenCL, Apple said that \"Metal avoids the overhead inherent in legacy technologies and exposes the latest graphics processing functionality\" of GPUs found in devices across iOS, macOS, and tvOS.\nAlthough Apple's decision to deprecate the older technology in favor of its own graphics API may not be surprising, some game developers have begun criticizing Apple for the move, particularly how it affects the future of gaming on Mac. Notably, OpenGL is an open-source, cross-platform solution that made it simple for developers to build games on both Mac and PC at the same time, providing some parity to a platform that many have agreed is lacking as a gaming hub.\nSince \"many games and apps continue to use OpenGL,\" particularly those that released prior to Metal in 2014, the shift to Metal-focused development is leaving Mac developers worried about any potential to grow as a gaming platform (via PC Gamer). Game developer Sam Loeschen tweeted that he feels \"conflicted\" about the decision, calling Metal a \"really, really good\" graphics API but admitting that \"this decision alienates macOS further as a gaming platform.\"\nSpeaking with PC Gamer, game designer Rami Ismail said that while \"it's not doomsday,\" it appears that Apple is preparing for such an occasion in regards to fully terminating OpenGL/OpenCL on Mac. He explained that for now, \"the worst that's going to happen\" is that parts of old apps will \"break,\" and pointed out that lacking a single cross-platform graphics API is a \"pain\" and \"not very good for developer confidence\" in Apple.\n\"With deprecation, abandonment can vary from 'soon' to 'never', so until we have information on that, we can't really say,\" he said. \"All we know is Apple seems to have shown intent to rid itself of OpenGL in favor of its own graphics API, Metal. The problem with Metal is very similar to the problem with DirectX: it's not cross-platform. \"It's not doomsday, it's more like Apple building a giant EMP machine and saying 'we might or might not use this.' The worst that's going to happen is old stuff will break, and our engines and libraries will grow a bit to support both Direct3D and Metal. Not having a clear guideline for future actions Apple might take in this regard isn't very good for developer confidence, I'd guess, and not having a single cross-platform graphics API is just a pain.\"\nMore developers and programmers chimed in on the news to PC Gamer, including Alex Austin, who ultimately said that while he likes to develop on Mac to \"support fans if I can,\" he's most likely \"not going to spend any time on Metal because Macs are a pretty small percentage of the market and really probably not worth it even now.\"",
        "sentiment": -0.18370335980465538
    },
    "article_23": {
        "title": "Shortcuts: A New Vision for Siri and iOS Automation",
        "body": "In my Future of Workflow article from last year (published soon after the news of Apple\u2019s acquisition), I outlined some of the probable outcomes for the app. The more optimistic one \u2013 the \u201cbest timeline\u201d, so to speak \u2013 envisioned an updated Workflow app as a native iOS automation layer, deeply integrated with the system and its built-in frameworks. After studying Apple\u2019s announcements at WWDC and talking to developers at the conference, and based on other details I\u2019ve been personally hearing about Shortcuts while at WWDC, it appears that the brightest scenario is indeed coming true in a matter of months.\nOn the surface, Shortcuts the app looks like the full-blown Workflow replacement heavy users of the app have been wishfully imagining for the past year. But there is more going on with Shortcuts than the app alone. Shortcuts the feature, in fact, reveals a fascinating twofold strategy: on one hand, Apple hopes to accelerate third-party Siri integrations by leveraging existing APIs as well as enabling the creation of custom SiriKit Intents; on the other, the company is advancing a new vision of automation through the lens of Siri and proactive assistance from which everyone \u2013 not just power users \u2013 can reap the benefits.\nWhile it\u2019s still too early to comment on the long-term impact of Shortcuts, I can at least attempt to understand the potential of this new technology. In this article, I\u2019ll try to explain the differences between Siri shortcuts and the Shortcuts app, as well as answering some common questions about how much Shortcuts borrows from the original Workflow app. Let\u2019s dig in.\nApp Shortcuts and Suggestions\nThere\u2019s an important difference between shortcuts and the Shortcuts app. As a system functionality, a shortcut is a convenient way to reopen or interact with a key feature of an app that the user has previously seen or completed. For instance, in iOS 12 you may see shortcuts for ordering a coffee on your way to work or playing a podcast playlist in a third-party app as you\u2019re driving back home. App shortcuts are not workflows; they are the equivalent of a \u201cpoint of interest\u201d in an app that you can easily find again.\nIn Apple\u2019s parlance, shortcuts are \u201cdonated\u201d by apps to the system after a user performs an action in an app. Then, iOS 12 suggests shortcuts in Spotlight search results (where they appear as tappable items featuring the app\u2019s icon and labeled action) and on the Lock screen as notification-like bubbles. In watchOS 5, you\u2019ll see suggested shortcuts on the Siri watch face.\nShortcut suggestions are generated on-device by taking into account contextual triggers such as the time of day and day of week, the user\u2019s location, and detected motion (such as walking or driving). Apple has been developing and refining their so-called Proactive technologies for years now, but the Shortcuts project marks the company\u2019s first foray into deep-linked app actions that react to user patterns and environmental triggers.\nShortcut suggestions in iOS 12\u2019s Spotlight screen.\nThere are two types of app shortcuts. The first kind is a basic shortcut that opens a specific piece of content or section in an app. These simpler shortcuts are based on NSUserActivity, an API that Apple first introduced in iOS 8 to enable third-party apps to hand off activity to other devices and later expanded in iOS 9 to offer search results in Spotlight. The same API, with minimal tweaks on the developers\u2019 side, is used in iOS 12 to provide users with shortcuts that launch apps into specific screens or activities. I expect developer adoption of shortcuts based on NSUserActivity to be massive when iOS 12 launches later this year; especially for apps that do not require execution of tasks inline within Siri/Spotlight/Lock screen, NSUserActivity should be enough.\nAccording to Apple, while NSUserActivity enables basic integrations with apps, SiriKit Intents provide the best experience for users who want to accomplish more with shortcuts. This is not a new API either, but the way it\u2019s surfaced throughout the system is.\nSiriKit Intents have been around since iOS 10 as a way for developers to integrate their apps natively with the Siri voice experience and perform actions inline within the assistant. Apple launched SiriKit as a domain-based API designed for specific kinds of apps, and slightly improved it last year with the addition of visual code, list, and note-taking apps. Just as it was becoming clear that Apple\u2019s narrow domain-specific approach couldn\u2019t scale to thousands of apps that can\u2019t be easily categorized, the company is turning SiriKit on its head.\nIn iOS 12, developers can now create their own custom intents based on built-in semantic templates; furthermore, existing SiriKit Intents can break out of the Siri UI and also work as shortcuts in other places such as Spotlight, the Lock screen, and even the Siri watch face. Apple\u2019s approach isn\u2019t surprising: if iOS apps can have the ability to perform tasks with custom interfaces and responses outside of the main app environment (as is currently possible with SiriKit Intents), why not expand the same functionality to other types of proactive assistance? With shortcuts, any essential, repeatable feature of an app can become an action that can be executed from anywhere on the system without launching the full app.\nThe idea of frequent usage and user routine is what separates intent-based shortcuts from traditional SiriKit voice interactions. For example, iOS 12 may learn that, on the way back home from work on Thursdays, you like to order pizza and have it delivered at 8 PM. Or that on an average workday around 1 PM, you open Things into your \u2018Office\u2019 project. These are repeatable actions that developers can turn into shortcuts with custom interfaces using the same underlying Intents technology first launched in iOS 10.\nDevelopers who are planning to integrate with SiriKit in iOS 12 will have to consider whether users may want to execute actions from their apps elsewhere on the system; those who have shied away from integrating with SiriKit so far should probably look into custom intents now.\nThis new feature allows any app to offer a custom interface and custom responses that are used when the intent is invoked via Siri or shortcuts. To create a custom intent, developers can choose from building blocks that include verbs such as \u201cdo\u201d or \u201corder\u201d and other templates; these actions essentially define how Siri talks about the task it\u2019s executing. I\u2019m excited about the prospect of any app becoming eligible for Siri integration; going forward, I expect Apple to continue expanding its custom intent technology as it may open up Siri to hundreds of thousands of new app integrations.\nEven though the opposite may seem true, the shortcut features I\u2019ve described so far do constitute a form of automation. Arguably, suggested shortcuts are system automations \u2013 actions to trigger a specific function that are conveniently presented at the best available time or location to anticipate users\u2019 needs. Some old-school power users may disagree with me on this, but, more broadly, I consider Apple\u2019s Proactive technologies \u2013 whether in surfacing a calendar event in an email message or ordering coffee while driving to work \u2013 a new kind of automation. Only time and developer adoption will tell if Apple\u2019s bet is successful; conceptually speaking, I see suggested shortcuts as an effortless, almost invisible way to get users accustomed with the idea of actions that are automatically surfaced by the OS.\nThe line between system and user automation gets blurry once we start considering the second layer of Apple\u2019s Shortcuts initiative: the ability for users to create custom phrases to launch shortcuts.\nAdd to Siri\nAvailable in Settings \u21fe Siri & Search, iOS 12 features an option for users to define their own phrases for launching specific shortcuts via voice. This is done by speaking a custom phrase into a Siri recording UI that transcribes the command and creates a shortcut that can be invoked at any time. The Settings app automatically suggests recently used app shortcuts as well as other shortcuts that were previously \u201cdonated\u201d by apps. Both recording a custom shortcut phrase and launching the phrase via Siri require an active Internet connection. Once given a custom phrase, user-configured shortcuts appear under the My Shortcuts section in Settings.\nAdding shortcut phrases in iOS 12.\nThere are a few details worth noting about adding custom shortcut phrases to Siri. In their apps, third-party developers can embed messages and buttons (which they can design) to bring up the Siri UI to record a shortcut phrase. This means we\u2019ll start seeing apps populate important screens or actions with suggestions and buttons to record a shortcut phrase. Moreover, in the Siri recording UI, developers can include a phrase suggestion, but it\u2019s up to the user to decide what they want to record.\nMore importantly, users always have to create personalized shortcut phrases through direct interaction: apps cannot automatically fill the \u2018My Shortcuts\u2019 page in Settings with shortcuts and custom phrases. The user has to associate a custom phrase to a shortcut first.\nThe more I think about it, the more I see custom shortcut phrases as the next big step in making Siri a more personal assistant that is unique to each user. As would happen with an actual assistant, shortcut phrases allow users to form their own language over time, creating a personalized set of instructions that only their assistant can interpret and act upon. It\u2019s the equivalent of jargon in a group of friends, but applied to Siri and app actions. The potential accessibility perks are tremendous too: Apple now enables everyone to create custom Siri phrases that can be however long or short they want; this removes the need to find actions nested in apps, multiple levels deep into their navigation stack.\nAn example of a Siri shortcut button in a third-party app.\nHere\u2019s why I believe Apple and the Workflow (now Shortcuts) team have been incredibly smart in reframing the concept of user automation around Siri and voice: when you think about it, custom phrases aren\u2019t too dissimilar from keyboard shortcuts. However, spoken phrases are easier to remember \u2013 they don\u2019t feel like dark magic to regular users who have never bothered with \u201cautomation\u201d before, and, most of all, they are natively supported across the entire spectrum of Apple products, from iPhones and AirPods to HomePods and Watches.\nI strongly believe that personalized phrases are the first step towards changing the fundamental Siri experience, which is going to evolve into a personal command log \u2013 from one Siri to a million Siris, each uniquely tailored to the user who customized it. Furthermore, custom phrases reveal the third (and, for now, final) layer of Apple\u2019s automation and Siri personalization strategy: the brand new Shortcuts app.\nCustom Shortcuts and the Shortcuts App\nThe Shortcuts app, announced at WWDC last week, is the new version of Workflow. The app will not be part of iOS 12 itself; instead, it\u2019ll be available on the App Store. In conversations I had last week, it appears that Apple\u2019s goal is to offer full compatibility with existing workflows previously created in the Workflow app. My understanding is that Apple is very much aware of the fact that a sizable portion of the pro/prosumer community relies on Workflow to enhance their iOS experience in key ways; they don\u2019t want to change that relationship for the worse. Very little if nothing should break in the transition to the Shortcuts app; in fact, I\u2019m optimistic about the prospect of retaining all the actions from the original Workflow plus new ones created specifically for Shortcuts.\nAt first glance, Shortcuts looks like a cleaner, more intuitive version of Workflow designed for the modern iOS 12 design language. The app lets you create workflows \u2013 now referred to as \u201ccustom shortcuts\u201d \u2013 with an editor that, just like the Workflow app, supports drag and drop to move actions from a library (organized in content types) into the shortcut editor. In the Shortcuts app, Apple removed the swipe gesture to navigate between the action library and editor in favor of a bottom panel that is reminiscent of Apple Maps and Stocks in iOS 12. A search field is always available at the bottom of the editor; tap on it, and you\u2019ll be able to view all the actions Shortcuts offers. Abundant design refinements and new action panel aside, the editor\u2019s core structure looks just like Workflow\u2019s.\nDespite the close resemblance, Shortcuts isn\u2019t just a redesigned version of Workflow. And it\u2019s also more than a glorified utility for people who want to geek out on an iOS device. As the \u201cthird layer\u201d after suggested shortcuts and custom phrases, custom shortcuts are the most versatile tool for every iOS user who wants to deeply personalize Siri, automate sequences of actions, and, yes, even augment their productivity. The Shortcuts app aims to be a powerful blend of Workflow, app shortcuts, and Siri all rolled into one as a new take on personal assistants and iOS automation. It\u2019s a bold idea that keeps what made Workflow unique while also opening it up to a broader user base and deeper system integrations.\nThere are a few key elements to consider. First, app shortcuts \u2013 the aforementioned actions donated by developers with NSUserActivity or SiriKit Intents \u2013 can be part of a custom shortcut created in the Shortcuts app. These shortcuts should either appear under Siri Suggestions or Apps in the action library. The inclusion of these actions in the Shortcuts app is a big deal: for the first time, users can create chains of actions that execute native third-party app commands without launching apps through URL schemes. Whether you want to launch an activity in an app or perform an action, these native actions won\u2019t require you to write any code or talk to any web API \u2013 and in the case of intent-based actions, they will run inline within the Shortcuts app itself.\nIn a way, this is the first step toward the WorkflowKit framework I imagined last year \u2013 a solution for apps to be supported in an automation environment without the limitations and security concerns of URL schemes. What I couldn\u2019t foresee was that Apple would reuse SiriKit for this. I don\u2019t think these new integrations will obviate the need for more customizable URL scheme actions just yet (more on this below), but it\u2019s a move in the right direction.\nPerhaps more impressively, it seems that, upon assembling a custom shortcut, users will be able to choose to display native third-party app actions or not with a new \u2018Show When Run\u2019 toggle. If I were to guess, I\u2019d say that this setting applies both to shortcuts being run in the Shortcuts app as well as the execution of multiple steps in Siri.\nThe \u2018Show When Run\u2019 toggle in a native app action in Shortcuts.\nWhich brings me to the second notable trait of the Shortcuts app: custom phrases. Just like shortcuts provided by apps to the system can be assigned a custom Siri invocation phrase, custom shortcuts from the Shortcuts app can be assigned a phrase to quickly trigger them from Siri. This ties into another key functionality of custom shortcuts: whenever possible, Siri on iOS 12 will try to run the multiple steps that comprise a custom shortcut inline, without launching the Shortcuts app; it\u2019ll do so simply by moving down the sequence of actions and confirming results to the user.\nAn intent-based extension summoned via a custom shortcut in Siri.\nThis was demonstrated by Apple\u2019s Kimberly Beverett at last week\u2019s keynote: with a custom shortcut, Siri was able to send an iMessage to a contact, set a HomeKit scene, play audio in a third-party radio app, and open directions in Maps \u2013 all in a single request that ran contextually inside Siri. The demo showcased two powerful aspects of Shortcuts\u2019 integration with Siri: background execution and the ability to skip confirmation steps in a series of automated actions. Just like in a traditional \u201cworkflow\u201d, Siri completed multiple actions in a row, displayed a summary message, and only launched an app at the very end. I cannot even imagine all the advanced custom shortcuts I could build by mixing background execution with web APIs, native app shortcuts, and Siri in the same sequence of actions.\nExecution of multiple background tasks inside Siri feels to me like the final piece of the Workflow acquisition puzzle. If you consider all the layers I covered above \u2013 app shortcuts, custom phrases, and custom shortcuts \u2013 it\u2019s clear that Apple wants to transform Siri into an assistant capable of not only handling a variety of app-related requests but, more importantly, bundles of sequential requests that are routinely requested by users. Of all the Shortcuts features I\u2019ve seen so far, running whole sequences of steps in Siri is the one I\u2019m most curious about.\nMore Shortcuts Details\nBelow, I\u2019ve assembled a collection of details about shortcuts and the Shortcuts app that I was able to put together by rewatching the WWDC videos, as well as having lots of interesting conversations last week.\nMagic Variables and Scripting actions are in. Two of Workflow\u2019s most advanced functionalities will continue to be supported in Shortcuts and will likely play an essential role in the creation of complex chains of actions.\nMagic Variables, introduced in Workflow 1.7, enable actions to continuously create variables behind the scenes, letting users dynamically convert them between types using the Content Graph engine. Magic Variables are unique to Workflow and they have dramatically changed how advanced users can chain actions together. Judging from Apple\u2019s screenshots of the Shortcuts app, Magic Variables will be supported in custom shortcuts and users will also be able to include them as part of a response read aloud by Siri.\nI\u2019m also glad to see the inclusion of the Scripting category of actions in Shortcuts. Currently, the Scripting section of Workflow features options such as conditional blocks, repeat loops, getting data types and setting file names, and even fetching device details like battery and network information. All signs are pointing to Shortcuts retaining the same functionalities.\nShortcuts has an action extension for the share sheet. One of Workflow\u2019s most powerful (and flexible) system integrations is the action extension that lets users run a workflow contextual to the app they\u2019re in and the item they\u2019re sharing. From what I hear, Apple plans to keep the same action extension for the Shortcuts app. As someone who regularly uses dozens of workflows activated from the extension in apps like Safari, Ulysses, and Photos, I look forward to adapting them to the Shortcuts extension.\nThe Gallery stays, but questions remain about public sharing. As confirmed by Apple, the Shortcuts app will have a built-in Gallery for users to browse and download hundreds of pre-made custom shortcuts for different tasks. Shortcuts will be organized in categories and curated by Apple on a regular basis. The Gallery isn\u2019t new to the Workflow app, and it\u2019s unclear if public sharing of user-created custom shortcuts will be part of it. Shortly after Apple acquired Workflow, the company removed the ability for users to share workflows directly to the gallery with public user profiles; it seems like Shortcuts will follow in the same footsteps with a Gallery limited to custom shortcuts created and promoted by Apple.\nI am confident that the Shortcuts app will continue to offer ways for users to share custom shortcuts with each other, but I believe sharing won\u2019t be tied to the Gallery, at least initially. That said, I would love to see user sharing return to the Gallery in a future update as a curated marketplace of custom shortcuts created by the community and vetted by Apple. I see tremendous potential in letting the iOS community natively extend Siri and iOS apps.\nHomeKit integration. The Shortcuts app will support HomeKit; during the keynote, Apple demonstrated how a custom shortcut could set a thermostat to a specific temperature and toggle a fan on and off. We haven\u2019t actually seen how HomeKit actions can be set up in Shortcuts yet though, so it\u2019s not clear if HomeKit actions will let you configure individual characteristics of accessories with granular options. I wouldn\u2019t be surprised if, for simplicity\u2019s sake, the first version of Shortcuts only supports triggering existing HomeKit scenes.\nThere is a new Show Result action for Siri output. Judging from the slides shown at WWDC, this is the action that will let users craft custom Siri responses in the Shortcuts app. The Show Result action can be filled with Magic Variables and arbitrary plain text; when a custom shortcut is run via Siri, the assistant will speak the text contents of the action.\nThe new \u2018Show Result\u2019 action in Shortcuts.\nI can already imagine the possibilities this action opens up \u2013 such as the ability to end up with different Siri responses depending on the result of a conditional block in Shortcuts. I\u2019m curious to know what happens to Show Result when a custom shortcut is run outside of the Siri UI though.\nNo support for passing input to a custom shortcut from Siri. My understanding is that triggering a custom shortcut from Siri won\u2019t allow you to pass along a message as input text. For instance, if you have a custom shortcut that sends the input text to Ulysses and have associated the \u201cUlysses append\u201d phrase to it, you won\u2019t be able to say \u201cUlysses append Buy Nintendo Switch\u201d to Siri and expect the \u201cBuy Nintendo Switch\u201d part to be sent to the Ulysses app.\nFrom what I\u2019ve been able to gather so far, Siri in iOS 12 doesn\u2019t currently support the ability to pass an input message to a custom shortcut activated with a user phrase, and it\u2019s unlikely to gain such functionality in the first version of iOS 12. This means that Siri will only be a launcher for custom shortcuts, not an actual interface that can pass user commands for specific parameters at runtime. There\u2019s a lot of complexity involved in this, and I assume it is one of the next big features in the pipeline for Shortcuts.\nNative app shortcuts don\u2019t support custom input, output, and customizable fields. On a similar note, native app shortcuts based on SiriKit Intents that execute within the Shortcuts app can\u2019t receive a custom input from previous actions. They also can\u2019t set Magic Variables as custom output and don\u2019t have customizable parameters. As shown in Apple\u2019s screenshots of the Shortcuts app, native app actions are disconnected from every other step in a custom shortcut, which is likely going to limit their flexibility for advanced users.\nURL scheme actions for third-party apps should remain available. In addition to manually launching URL schemes with the \u2018Open URLs\u2019 and \u2018Open x-callback-url\u2019 actions, Workflow has long offered a selection of built-in third-party app actions that are based on URL schemes but abstract that complexity with a visual module. For example, Bear, Things, and Ulysses come with native Workflow actions that can pass along custom parameters when launching the respective apps.\nExamples of visual app actions based on URL schemes in Workflow.\nAfter talking to several developers at WWDC, it sounds like there\u2019s a good chance third-party app actions powered by URL schemes should remain in the Shortcuts app as well. Personally, I think these will continue to be solid workarounds until SiriKit Intents are powerful and customizable enough to offer the same functionality of URL scheme actions. Ideally, in a future version of Shortcuts, these actions should be replaced by visual SiriKit Intents that can be customized with multiple variables and parameters by users. For now, it seems like traditional URL scheme actions will still allow for deeper customization and control than native app shortcuts.\nShortcuts has a widget with limited user interaction. As shown on Apple\u2019s website, the Shortcuts app will keep the same widget that lets Workflow users run workflows from outside the app.\nThe widget is one of Workflow\u2019s most peculiar features: it supports remote execution of workflows with basic interactivity, but it kicks you back to the main app if it comes across an action that can\u2019t be completed from the widget, such as entering text with the keyboard or editing an image. The idea of running the same sequence of actions in different, more constrained environments brings me to\u2026\nRunning custom shortcuts in Siri and audio-only contexts. Because Shortcuts has a widget, and because Apple said custom shortcuts will be offered on iOS, watchOS, HomePod, and even CarPlay, I can then infer that the ability for the same shortcut to run in different contexts is expanding to audio and the watch face\u2019s limited UI. Just like the widget, I assume this means a custom shortcut will completely execute within Siri (whether on HomePod or the Watch) unless an action requires manual user input. In that case, Siri would probably ask you to continue running the shortcut on your iPhone.\nIf this is the case (and I think my explanation is mostly accurate), I can imagine that custom shortcuts that embed actions such as \u2018Choose from List\u2019 or \u2018Ask for Input\u2019 will require jumping from Siri to the Shortcuts app. I would be completely okay with this as a first version. Generally speaking though, I\u2019d love for Siri to give me multiple options to choose from a list, allow me to enter input via voice, and interact with a shortcut while it is executing in an audio-only context.\nThe new Play Media intent. Finally, speaking of audio: SiriKit in iOS 12 supports a new INPlayMedia intent which, as the name suggests, allows the assistant to play audio and video content from third-party apps. The intent can launch apps in the background (such as the radio station demo from the keynote) and supports playing audio on HomePod as well as suggestions on the Lock screen.\nIn terms of app adoption and shortcut integrations, this intent should make it possible for the likes of Spotify and Overcast to offer users a way to play their favorite content via Siri just like they can with Apple Music and Podcasts. Overcast and other third-party podcast apps could, for instance, offer Siri shortcut buttons in various places of their UIs to let users record phrases such as \u201cPlay my Overcast queue\u201d or \u201cLet\u2019s listen to Connected\u201d; playback would then kick off immediately in the background and play through the device\u2019s speakers, HomePod, or other devices. If my interpretation of this is correct, the combination of shortcuts and the new Media intent may alleviate a lot of the annoyances typically involved with using Siri and third-party media services.\nThe Future of Workflow and Siri\nIn iOS 12, Apple is providing users with a path from simple shortcuts to custom automation; both revolve around the guiding principle of letting users choose how they want Siri to get things done on their behalf. There is a progressive disclosure of automation happening from system shortcuts to the Shortcuts app: whether you\u2019ve never dabbled with app shortcuts before or are a Workflow expert, iOS 12 wants to help you spend less time using your phone \u2013 a recurring theme this year \u2013 and let shortcuts do the work for you.\nShortcuts are going to be the feature for developers to adopt this summer. It was the talk of WWDC 2018 and, based on my conversations, developers are excited and optimistic about embedding SiriKit and shortcuts within their apps. In particular, custom phrases and custom SiriKit intents seem to be the most attractive proposition for developers who want to let users conveniently open and interact with their apps.\nShortcuts and custom phrases feel like the future of Siri and apps: they\u2019re supported in every permutation of Siri and, most importantly, they let users develop their own language to access commonly used actions instead of forcing them to remember a syntax made of app names and verbs. Shortcuts, phrases, and custom intents feel like an app- and user-centric response to Alexa skills that Apple is uniquely positioned to build.\nWhile shortcuts are a way to \u201csell\u201d the idea of lightweight automation to regular users, the Shortcuts app is shaping up to be the automation powerhouse we were hoping to see following the Workflow acquisition. From what I\u2019ve seen so far, the Shortcuts team has been able to build a cohesive narrative around basic shortcuts and custom ones, going far beyond what Workflow could have achieved as an independent app. I\u2019m optimistic that heavy Workflow users won\u2019t be disappointed by Shortcuts.\nEven more than feature parity with Workflow though, I see integration of custom shortcuts with Siri as the next frontier for making automation accessible to more people. I believe this is what\u2019s going to push automation forward as something more than \u201cscripting\u201d and other old preconceptions. Giving users the tools to create automations with drag and drop and easily trigger them with their voice is a remarkably powerful idea; it can turn Siri into a truly personal, programmable assistant capable of performing entire series of actions with just one request. I don\u2019t think I\u2019ve ever seen anything even remotely similar to Apple\u2019s demo of the Shortcuts app and Siri integration on other platforms.\nSome people, however, may argue that this isn\u2019t real support for multiple commands in Siri; after all, you still have to create a custom shortcut with your favorite actions and manually set it up for Siri. And maybe the Shortcuts app is a way for Apple to circumvent the fact that Siri, unlike Google Assistant, isn\u2019t capable of handling multiple questions in the same sentence yet. Ultimately however, it all goes back to whether you see the beauty and potential of user automation or not. With custom shortcuts, you won\u2019t even have to speak entire sentences containing multiple requests every time you want to execute them; you can just tell Siri a short custom phrase and it\u2019ll fire off multiple steps on its own.\nFrom my perspective, this is exactly what automation is about: making a computer more useful, accessible, and faster so that we can save time to focus on something else. Custom shortcuts and the Shortcuts app show that not only is this vision still very much alive inside Apple, but it\u2019s evolving with the times too.",
        "sentiment": 0.13970856387366323
    },
    "article_24": {
        "title": "macOS Mojave Wallpapers for iPhone, iPad, and Apple Watch",
        "body": "Wallpaper Weekends is a series that works to bring you stunning, high-quality wallpapers for your iPad, iPhone, iPod touch, Mac, and Apple Watch. This week Wallpaper Weekends brings you macOS Mojave wallpaper for the iPad, iPhone, and Apple Watch.\nThis week\u2019s wallpapers comes from the macOS Mojave beta. We also have them for your Mac, available here.\niPad\niPhone\nApple Watch\nDownload Directions for iPhone and iPad\nFrom your device, link to the full resolution image by tapping the individual images. When the new window opens, tap the Safari Extensions icon in the bottom area of your browser, (it looks like a box with an up arrow sticking out of it), and tap \u201cSave image\u201d to save the Blue Lagoon wallpaper image to your Photo Library.\nFrom your desktop computer, link to the full resolution image by clicking the image. Right-click and save the image to your iTunes photo sync folder.\nDownload Directions for Apple Watch\nFrom your iPhone, link to the full resolution image by tapping the individual images. When the new window opens, tap the Safari Extensions icon in the bottom area of your browser, (it looks like a box with an up arrow sticking out of it), and tap \u201cSave image\u201d to save the image to your Photo Library.\nAfter you\u2019ve saved all of the wallpapers you\u2019d like to use, open the Photos app on your iPhone and move the new images to the album you use to share images with you Apple Watch. Now you can go to your Apple Watch, select the new blue lagoon wallpaper as a watch face, and customize it to your heart\u2019s content. If you do download several of the wallpapers and move them to your Watch, you can even set them to rotate between them each time you view your watch face!",
        "sentiment": -0.11691189313737246
    }
}