{
    "article_0": {
        "title": "Samsung sues its own brand ambassador for $1.6M for using an iPhone X",
        "body": "Update: The BBC reports that Samsung has denied that there is a lawsuit, but refused to confirm or deny that it is asking for some money back from Sobchak.\nIt\u2019s not unusual for celebrities paid to promote Android brands to be caught using an iPhone. In many cases, they have been caught out on Twitter \u2013 like Wonder Woman star Gal Gadot.\nBut Samsung appears to take the matter more seriously than most: they are suing their own Russian brand ambassador for using an iPhone X for a massive $1.6M \u2026\nReality TV show host and politician Ksenia Sobchak is the face of Samsung in Russia, and is required by contract to use a Samsung smartphone. But the Mirror reports that she was seen on a television interview using an iPhone X.\nShe did her best to disguise the fact, using a piece of paper to try to hide the phone from view, but plenty of people noticed, soon promoting widespread discussion on social media.\nThe Mirror says it\u2019s not the only time Sobchak has been spotted using her iPhone.\nMs Sobchak has reportedly been seen on television, and at some of the hottest social events in the capital city of Moscow, using her iPhone. Her representatives have yet to comment on the story [\u2026] Ms Sobchak \u2013 who ran against her Putin in the Russian presidential elections earlier this year \u2013 used to have a party girl reputation and was dubbed the Russian Paris Hilton. She is widely believed to be Putin\u2019s goddaughter though neither has confirmed it publicly.\nAppleInsider speculates that the $1.6M Samsung is seeking from Sobchak may be substantially more than she was paid to endorse the brand, and cites several other examples of similar embarrassments.\nIn 2014, Maroon 5 singer Adam Levine promoted the launch of Samsung\u2019s Milk Music service, including the suggestion of holding an \u201ciPhone burning,\u201d but one week later he was caught promoting a TV appearance from an iPhone. Levine was also due to perform at the iTunes Festival at that time. Tennis player David Ferrer sent a tweet to his followers promoting the Galaxy S4 in 2013, but the message was labeled as \u201cvia Twitter for iPhone.\u201d In 2015, BlackBerry\u2019s PR team sent a tweet promoting the BlackBerry Classic from an iPhone, mirroring a similar mistake made by one-time creative director Alicia Keys in 2013.\nWe\u2019re sure this won\u2019t be the last time a celebrity paid to endorse an Android phone is spotting using an iPhone.\nCheck out 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.3582186878604047
    },
    "article_1": {
        "title": "Apple seems to have completely blocked police password cracking tool in iOS 12",
        "body": "Earlier this year, Apple updated iOS to block passcode cracking tools like GrayKey (used by police and government law enforcement officers). But the original iOS 11.4.1 patch wasn\u2019t perfect, with researchers still finding ways around it. That seems to have changed with the release of iOS 12 last month, which a recent Forbes report notes appears to have completely blocked the GrayKey tool, preventing it from cracking the password of any devices running the latest software.\nIt\u2019s not clear what Apple\u2019s done to lock out GrayKey\nGrayKey first made waves earlier this year as a tool specifically developed and sold to police departments to break passwords on iPhones for use in investigations. But now, instead of brute-forcing passwords, GrayKey is apparently limited to just a \u201cpartial extraction,\u201d only offering access to unencrypted metadata like file sizes. Forbes doesn\u2019t make it clear if the patch is something Apple has done recently, or if the improved block has been in place since iOS 12 was released in mid-September, but for now at least, it appears to have shut down GrayKey.\nIt\u2019s also not clear what Apple\u2019s done to lock out GrayKey this time. One forensic expert speculated to Forbes that \u201cit could be everything from better kernel protection to stronger configuration-profile installation restrictions,\u201d but no one seems to know for sure. But according to police officer Captain John Sherwin (from the Rochester Police Department in Minnesota), it\u2019s a \u201cfairly accurate assessment\u201d to say that Apple has stopped GrayKey from unlocking updated devices.",
        "sentiment": -0.32714259568601844
    },
    "article_2": {
        "title": "The new iPhone does not have an intentional skin smoothing filter, just more aggressive noise reduction",
        "body": "Last week we detailed the camera hardware changes of the iPhone XS vs. the iPhone X, and I wondered why Apple\u2019s keynote focused on changes in camera software rather than the new hardware. After testing the iPhone XS cameras for the last week, I get it.\nThe iPhone XS doesn\u2019t just have a bigger sensor: It has a whole new camera \u2014 and the biggest change is its reliance on computational photography.\nIt\u2019s A Smart Thing\nApple is smart. They see diminishing returns cramming more and more electronics in a fingernail-sized sensor. Photographic technology is the science of capturing light, which is limited by optics and physics.\nThe only way to circumvent the laws of physics is with something known as \u2018computational photography\u2019. With the powerful chips in modern iPhones, Apple can take a whole bunch of photos\u2014some of them before you even pressed the shutter\u2014and merge them into one perfect shot.\nAn iPhone XS will over- and underexpose the shot, get fast shots to freeze motion and retain sharpness across the frame and grab every best part of all these frames to create one image. That\u2019s what you get out of the iPhone XS camera, and that\u2019s what makes it so powerful at taking photos in situations where you usually lose details because of mixed light or strong contrast.\nThis isn\u2019t the slight adjustment of Auto HDR on the iPhone X. This is a whole new look, a drastic departure from the \u201clook\u201d of every iPhone before it. In a sense, a whole new camera.\nWhat\u2019s this about a \u2018soft filter\u2019 on my selfies?\nIt doesn\u2019t exist. I don\u2019t want to say that some people make up controversies to get Youtube impressions, but you do have to take things on the internet with a grain of salt.\nPeople feel the iPhone XS \u2018smoothens\u2019 things for two reasons:\nBetter and more aggressive noise reduction due to merged exposures, and\nMerged exposures reducing sharpness by eliminating sharp light/dark contrasts where light hits parts of the skin\nFor the latter, it\u2019s important to understand how our brains perceive sharpness, and how artists make things look sharper.\nIt doesn\u2019t work like those comical CSI shows where detectives yell \u2018enhance\u2019 at a screen. You can\u2019t add detail that\u2019s already been lost. But you can fool your brain by adding small contrasty areas.\nEnhance! Okay, maybe that\u2019s a bit too enhanced.\nPut simply, a dark or light outline adjacent to a contrasting light or a dark shape. That local contrast is what makes things look sharp.\nImage via Wikipedia\nTo enhance sharpness, simply make the light area a bit lighter near the edge, and the dark area a bit darker near the edge. That\u2019s sharpness.\nImage from the Verge\nThe iPhone XS merges exposures and reduces the brightness of the bright areas and reduces the darkness of the shadows. The detail remains, but we can perceive it as less sharp because it lost local contrast. In the photo above, the skin looks smoother simply because the light isn\u2019t as harsh.\nObservant people noticed it isn\u2019t just skin that\u2019s affected. Coarse textures and particularly anything in the dark\u2014 from cats to wood grain\u2014 get a smoother look. This is noise reduction at work. iPhone XS has more aggressive noise reduction than previous iPhones.\nI talked about noise reduction on iPhone X in The Power of RAW on iPhone, Part 1.\nWhy The Noise Reduction?\nAfter testing the iPhone XS side by side with the X, we found the XS prefers a faster shutter speed and higher ISO level. In other words, it takes photos a lot faster, but comes at the cost of noise.\niPhone X RAW on the left, iPhone XS RAW on the right. Note the increase in visible noise!\nTwo shots taken with the iPhone X (left) and iPhone XS (right). Taken in RAW so the extra noise can be seen\u2014RAW on iPhone omits any noise-reduction steps. Why does the iPhone XS\u2019 frame have to be noisier?\nRemember that line-up of frames showing how the iPhone camera works?\nUnless you have bionic arms, it\u2019s impossible to hold your phone perfectly still for this long. To get a sharp, perfectly aligned burst of images, the iPhone needs to take photos really fast. That requires a shorter shutter speed \u2014 and that, in turn, means that there will be more noise in the image.\nThat noise has to be removed, somehow, and that comes at a cost: noise reduction removes a bit of detail and local contrast.\nThe iPhone XS RAW exposure on the left shows less \u2018smoothed\u2019 detail in the reflections, compared to its regular Smart HDR counterpart on the right.\nBut mostly selfies are smoother \u2014 especially faces!\nYep. The front facing selfie camera hardware is worse in low-light than the back facing camera. The selfie cam has a tiny, pinkie-fingernail sized sensor, which means it takes in less light, which in turn means more noise, and thus more noise reduction.\nThe result is a smoother image, which with the new Smart HDR and computational-photography-heavy pipeline smoothens out the image a bit more than in the past.\nIn the images below, notice the smoothing in low light compared to daylight:\nImages by Apple\nThe tradeoff is that selfies, which traditionally are worse in mixed or harsh lighting (the majority of lighting!) are now no longer blown out, and in most cases it just looks better, if just a little on the smooth side.\nThe good news is that Apple can also tweak this a bit if people find it too heavy-handed, but given it\u2019s a simple choice between unflattering lighting and noise versus too much smoothness, it\u2019s logical for version 1.0 to err on the side of smoothness.\nWith regards to false claims that faces are specifically targeted: I tried images of a lemon, coarse textured paper and regular old facial selfies and the level of smoothing was identical.\nSo, the iPhone XS camera is worse?\nNo, the camera is not worse than the iPhone X.\nThe iPhone XS camera is better than iPhone X. It has superior dynamic range, but comes with a few tradeoffs in Apple\u2019s software. If you don\u2019t like the newfangled way of doing things, don\u2019t worry.\nA shot like this is impossible to achieve on pre-XS iPhones. By Austin Mann.\nWhat Apple is doing is better for virtually all use cases: casual users get better photos with more detail in highlights and shadows, without any editing. Pro users can regain contrast with a little bit of editing; the opposite is impossible: with a contrasty image, the detail was already gone.\nYou can now take selfies or photos with harsh backlight, side light or other unflattering light sources and end up with a usable result. This is kind of magic!\nThat being said, there\u2019s a two slight problems:\nThe Faithfulness Problem\nAs cameras become less of an simple instrument and more of a \u2018smart device\u2019 that uses a variety of complex operations to merge several images into one, you wonder if you\u2019re looking at \u2018undoctored\u2019 images.\nTake this shot of Yosemite at night:\nYosemite, by Tanner Wendell Stewart.\nThis is doctored. To properly expose the landscape the photographer used a very long exposure. Then captured the stars with a much shorter exposure, otherwise they would\u2019ve turned into star trails. Then they merged the two images into one. Technically this is fake.\nNow, back to the iPhone: the Smart HDR takes various exposures and merges them to get better shadow and highlight detail. There\u2019s a degree of fakery involved. Photography purists might very well be bothered by that:\nWithout Smart HDR, manual HDR is always hard to pull off without looking fake. There\u2019s an entire subreddit devoted to \u2018shitty HDR\u2019.\nThe two leftmost images above were both taken with iPhone XS; left, with Smart HDR, and in the middle without. On the right, a shot taken with iPhone X.\nWith Smart HDR disabled (the middle), it\u2019s still recovering more dynamic range, but feels a little less \u201cauto tuned.\u201d There\u2019s a lot more to say about that middle image, but a deep dive into dynamic range (and true HDR) deserves a future post.\nThis is just how the camera works on iPhones now. And I\u2019d wager that it\u2019ll stay that way in the future.\nAnd yes, this applies to the viewfinder of any camera app, as well. Apple applies its dynamic range improvements live, to the video stream, so will always see an \u2018altered\u2019 image.\nProblems with RAW\nHere\u2019s where it gets problematic in a practical sense: iPhone XS behaves entirely different than iPhone X when it comes to exposing an image. That matters when you shoot RAW. A lot.\nTake this casual shot:\nLeft: iPhone X RAW, no edits. Right: iPhone XS RAW, no edits. What happened?\nImmediately you\u2019ll notice it\u2019s overexposed. If you go to edit the iPhone XS RAW file, you\u2019ll notice you find highlights were lost due to clipping.\nWhen you dive into the technical details, you\u2019ll see the second problem: iPhone X exposed for 1/60th of a second at ISO 40 whereas the iPhone XS exposed for 1/120th of a second at ISO 80. We suspect the XS camera now just prefers shorter exposure times at higher ISO, to get the best possible Smart HDR photo.\nWe make a camera app that takes RAW photos, so this is very bad. Not only does RAW not benefit from merging multiple photos, but iPhone photos generally get very noisy above ISO 200. This is a major step in the wrong direction.\nTo add insult to injury, iPhone XS sensor\u2019s noise is just a bit stronger and more colorful than that of the iPhone X.\nThis isn\u2019t the kind of noise we can easily remove in post-processing. This isn\u2019t the gentle, film-like grain we previously saw in iPhone X and iPhone 8 RAW files.\nAs it stands today, if you shoot RAW with an iPhone XS, you need to go manual and under-expose. Otherwise you\u2019ll end up with RAWs worse than Smart HDR JPEGs. All third-party camera apps are affected. Bizarrely, RAW files from the iPhone X are better than those from the iPhone XS.\nOur solution: Smart RAW\nFortunately, it doesn\u2019t have to stay that way. Since the iPhone XS came out, we\u2019ve spent days, nights and weekends working to figure out a solution so we can get the most out of the bigger sensor and its deeper pixels.\nWe\u2019re happy to announce a new feature in Halide 1.10 called Smart RAW, which uses the new sensor technology in the iPhone XS to get better images than an iPhone X could ever take. Smart RAW does not use any aspect of Smart HDR \u2014 in fact, it avoids it altogether, so you end up with almost no noise reduction.\nWe use a combination of entirely new logic for exposing the image and a touch of magic to get superior RAW shots.\nNow that we\u2019ve bypassed the iPhone XS exposure issues, there\u2019s a pretty crazy amount of detail in these Smart RAW files. Let\u2019s look at that last image of the First Aid Kit concert with and without Smart RAW:\nNo Smart RAW on the left vs. Smart RAW on the right.\nIt\u2019s significantly better than the iPhone X. Here\u2019s some GIF comparisons of the iPhone XS vs. an iPhone X RAW of the same scene:\nLook at all that detail! (Perspective change due to the iPhone XS\u2019 new, wider angle lens.)\nAnother scene, comparing the iPhone XS with Smart RAW with an iPhone X:\nDetails! Glorious details! Both of these images got the same quick Lightroom filter applied.\nThe results are remarkable: less noise and more detail in every shot.\nHere\u2019s the best part about Smart RAW: thanks to specific fine-tuning for the iPhone XS sensor, we can now get more quality out of the camera than ever before. There\u2019s a remarkable increase in resolution and quality going from the iPhone X to the iPhone XS.\nSmart RAW vs. Smart HDR\nYou might be wondering how it stacks up against Smart HDR, since shooting in RAW means you do not get any HDR whatsoever. I took a tricky shot, exposure-wise, with Smart HDR and our new RAW:\nLeft: Smart HDR. Right: Unedited RAW\nWithout any edits, they stack up pretty decent. Smart HDR has a slight edge.\nBut RAW files are only a starting point. Here are the same photos after a few edits:\nLeft: Smart HDR. Right: Edited Smart RAW.\nWe don\u2019t quite get the dynamic range of Smart HDR here \u2014 note the sky gets a hint of blue in it in the Smart HDR version \u2014 but we\u2019re pretty close. I personally prefer the look of the edited RAW: just a bit more natural.\nBut here\u2019s the real edge: detail.\nI always prefer shooting in RAW for the sheer sharpness of details, and on the iPhone XS with its larger pixels there\u2019s more detail than ever. The leaves in the top-right tree are a great example:\nThis goes for most shots. With the RAW capture, you\u2019ll have to do some work to edit it, but the results are often worth it. Of course, this is an image with a lot of available light \u2014 we should also see how it stacks up when the sun goes down.\nIn low-light situations, there\u2019s a tradeoff. As light goes down, noise goes up. The iPhone XS paired with Smart HDR in the stock camera app does some pretty great low-light photography, but there\u2019s also times where it won\u2019t get the sharpest shots.\nVery low light photography will always have a lot of noise. Smart HDR aggressively hides it, losing detail. If you want more detail and you\u2019re ok with a little noise, try Smart RAW in Halide:\nLeft: Halide\u2019s Smart RAW. Right: iPhone\u2019s Smart HDR.\nThere\u2019s noise in the shadows here, and the skin look harsher as there is less flattening of the highlights and shadows, but the detail is fantastic compared to the straight-out-of-camera image from the stock iPhone camera app here.\nSmart RAW is still in testing and will require a very large amount of photographic proof to ensure it works perfectly in all conditions. We expect to launch it at the end of this week.\nConclusion\niPhone XS has a completely new camera. It\u2019s not just a different sensor, but an entirely new approach to photography that is new to iOS. Since it leans so heavily on merging exposures and computational photography, images may look quite different from those you\u2019ve taken in similar conditions on older iPhones.\nBut unlike previous cameras, exactly because many of its leaps in quality are based on software, we can expect it to change, and even improve. This is just the first version of iOS 12 and Smart HDR.\nLikewise, us developers need to update apps to take full advantage of the new iPhone XS and XS Max\u2019s very capable sensor. Since it is such a different animal, simply treating it as any other iPhone will yield subpar results. We\u2019re almost done doing our first take at it and we\u2019ll no doubt have to work more on it in the future.\nIf you\u2019re a user that\u2019s bothered by some aspect of this brave new era of computational photography, or some of Apple\u2019s image processing, know that there\u2019s options for you out there: you can disable some of the heavy handed HDR in the Camera settings\u00b9, or you can shoot in RAW.\nAnd on that last option, we\u2019d be happy to help you get started.\nMany of our Smart RAW testing shots taken in and around San Francisco.\n\u00b9 Go to Settings -> Camera, then disable Smart HDR. Now open the camera app and a new \u2018HDR\u2019 setting will appear in the top controls. Tap it to disable HDR.",
        "sentiment": 0.007916187363788017
    },
    "article_3": {
        "title": "After 1475 days of waiting, Apple unveils new Mac mini: quad-core, &gt;32 GB RAM, all SSDs",
        "body": "Apple has just unveiled a new Mac mini at its October media event. In a space grey finish, every Mac mini now starts with quad-core CPU configurations. Apple will also offer six-core models for the first time. The new Mac mini can include up to 64 GB RAM, and all models feature solid state drives.\nThe Mac mini starts at $799, with various build-to-order upgrade options. Orders start today, with first deliveries from November 7.\nOn the back side, the Mac mini includes an array of ports: gigabit Ethernet, four Thunderbolt 3 ports, HDMI port, and USB-A. You can also configure it with 10 Gigabit Ethernet like the iMac Pro.\nThe new Mini also features the Apple T2 chip for security. Apple has redesigned the internals for better cooling. Apple is using recycled aluminium for the enclosure and 60% recycled plastics.\nThe most specced-out Mac mini can feature 64 GB RAM, 4.6GHz 6-core Core i7 CPUs, a 2 TB SDD and 10 Gigabit ethernet.\nFans of the Mac Mini have had to wait for a long time for this update. Before today, the last update to the product was in 2014. With this revision, Apple has dramatically updated the internal components, although the exterior has mostly stayed the same aside from the new space gray finish.\nhttps://www.youtube.com/watch?v=hVEaL9izgjs\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": 0.16545870939367696
    },
    "article_4": {
        "title": "Apollo for Reddit removed from App Store over push notification implementation, despite guidance from Apple",
        "body": "Update: Selig says Apple has corrected the error and Apollo for Reddit should soon return to the App Store.\nAfter a feature-rich update over the weekend, Apollo for Reddit has today been removed from the App Store. Developer Christian Selig took to Reddit to explain that he received an email from the App Store Review team explaining that his app violates App Store Guidelines by offering push notifications as an in-app subscription upgrade.\nIn the email, the App Store Review team explains that offering an \u201cin-app purchase auto-renewable subscription product\u201d that provides access to \u201cbuilt-in Push Notification capabilities\u201d is \u201cnot appropriate for the App Store\u201d:\nUpon re-evaluation, we found that your app is not in compliance with the App Store Review Guidelines. Specifically, we found: Business \u2013 3.2.2 We noticed that your in-app purchase auto-renewable subscription product provides access to built-in Push Notification capabilities, which is not appropriate for the App Store. For this reason, your app will be removed from sale on the App Store at this time.\nFor those unfamiliar, Apollo for Reddit version 1.3 was released over the weekend and includes a new \u2018Apollo Ultra\u2019 subscription package, which offers features such as realtime notifications, app theming, and more.\nWhat makes Apple\u2019s approval and subsequent rejection puzzling, however, is that Selig worked directly with Apple to ensure the update did not violate the App Store Guidelines. He was originally told by Apple that so long as he bundled realtime push notification support with other features, such as theming and custom app icons, the update would be approved and not in violation of the guidelines.\nThis is ultimately what he did with Apollo for Reddit version 1.3. Two weeks ago, Selig explained the situation on Reddit:\nA really nice representative from Apple called a few days ago to talk about 1.3, which was super cool of them. The gist of it is what y\u2019all suggested and we discussed in the previous thread is what Apple suggested as well. The issue is that while they completely understand there\u2019s intrinsic server costs associated with push notifications (and that I\u2019m associating a fee more with the server end of things that is required to get to the notifying step, rather than the notification itself), since at the core push notifications are a system capability, they\u2019re not comfortable allowing notifications to be locked behind a purchase due to the potential of abuse in other situations. They suggested bundling it with things that aren\u2019t system capabilities as part of a feature/bundle pack like we talked about (Apollo Premium or Apollo Plus, dunno yet), so long as the other features in this pack aren\u2019t system features (for example he said I couldn\u2019t do push notifications and AirPlay support or something). Which is totally fine, because our plan was to do the notifications, app theming, and a cool custom icon as part of the pack.\nThe situation is certainly a puzzling one, and one that doesn\u2019t necessarily reflect well on the App Store Review process. As for what the future holds remains unclear, but Selig says he\u2019ll continue to keep users of Apollo for Reddit updated.\nSubscribe to 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.21402374720200895
    },
    "article_5": {
        "title": "Apple Invites Media to October 30th Event in New York City",
        "body": "Apple today sent out media invites for a second major 2018 event set to be held in New York City on Tuesday, October 30 at the Brooklyn Academy of Music, Howard Gilman Opera House.\nApple's October event, which will focus on products not introduced at the iPhone-centric event in September, will kick off at 10:00 a.m. Eastern Time.\nImage via Neil Cybart\nApple sent out multiple different Apple logo designs on the invitations that it sent out to members of the media, all of which feature the tagline \"There's more in the making.\"\nImage via Todd Haselton\nWe're still awaiting multiple product refreshes before the end of 2018, including updates to several Mac models and the iPad Pro, which are likely to see a debut at the event.\nImage via Lance Ulanoff\n2018 iPad Pro models are expected to adopt an iPhone X-style design with no Home button, slimmer bezels, and a TrueDepth camera system that will enable Face ID for biometric authentication.\nImage via Joanna Stern\nWhile the iPad Pro is expected to continue to use an LCD rather than OLED due to the high price of OLED displays, rumors suggest it could adopt a USB-C port instead of a Lightning port and that the headphone jack could potentially be eliminated.\n2018 iPad Pro mockup by \u00c1lvaro Pabesio\nAlong with new iPad Pro models, we're also expecting refreshes to the MacBook line. Refreshed 12-inch MacBooks are said to be in the works, and based on rumors, Apple has developed a low-cost notebook with a Retina display that could be positioned as an updated MacBook Air.\nThe Mac mini, which has not been updated for more than 1,400 days, is expected to be refreshed for the first time since 2014. We don't know a lot about what to expect for the Mac mini update, but upgraded internals and faster processors are a sure thing.\nIt's possible that at this iPad and Mac-focused event, we'll also hear more about the modular Mac Pro that Apple is working on for a 2019 debut.\nApple's 2018 keynote event will begin at 10:00 a.m. Eastern Time. Apple typically streams the event live on its website and on Apple TV, but for those who are unable to watch, MacRumors will be providing full event coverage both on MacRumors.com and through our MacRumorsLive Twitter account.",
        "sentiment": 0.12522854913885778
    },
    "article_6": {
        "title": "Apple\u2019s Tim Cook: \u2018Don\u2019t believe\u2019 tech companies that say they need your data (x-post r/privacy)",
        "body": "KEY POINTS Apple's Tim Cook hit out at tech companies that insist more customer data leads to superior products, saying in an interview with Vice that such claims are a \"bunch of bunk.\"\nThe tech CEO did not name names, but appeared to point the finger at the likes of Facebook and Google.\nCook insisted that Apple has not compromised user privacy in China.\nApple CEO Tim Cook speaks during the 2018 Apple Worldwide Developer Conference (WWDC) at the San Jose Convention Center on June 4, 2018 in San Jose, California. Getty Images\nApple CEO Tim Cook hit out at tech companies that claim more customer data leads to superior products, saying that's a \"bunch of bunk.\" In an exclusive interview with Vice News Tonight that aired Tuesday, Cook did not name any names but appeared to admonish the likes of advertising giants Facebook and Google, which rely on data sharing with third parties.\nSome argue that Apple's more conservative approach is damaging to the development of core products like Siri, especially in the face of fierce competition from Amazon's Alexa. But Cook reiterated to Vice the company's \"collect as little data as possible\" stance, saying he considers privacy \"one of the most important issues of the 21st century.\" The tech CEO added that he is not typically a \"pro-regulation kind of person,\" but he would be willing to work with lawmakers to educate them and ensure that tech companies create products that are \"great for society.\" \"I think some level of government regulation is important to come out on that,\" Cook told Vice.\nApple itself came under fire earlier this year about its commitment to user privacy following its decision to begin hosting Chinese users' iCloud accounts in a new data center within China's borders. Critics argued that the move would give Chinese authorities easier access to text messages, email and other data stored in the cloud, thereby potentially quashing users' freedom of speech. Cook insisted to Vice, however, that Apple's encryption policies are \"the same in every country\" and that the company continues to have ultimate control. \"I wouldn't get caught up in where's the location of it,\" Cook told the news organization. \"We have servers located in many different countries in the world. They're not easier to get data from being in one country versus the next.\"",
        "sentiment": -0.5806748023023829
    },
    "article_7": {
        "title": "Apple says its T2 chip can prevent hackers from eavesdropping through your MacBook mic - 12 Closing the lid shuts down the microphone, without fail",
        "body": "One of Apple\u2019s latest custom chips, the security-focused T2, has made its way to the newly updated MacBook Air, as well as the updated Mac mini. In a new security document published today, and reported on first by TechCrunch, Apple is giving us a glimpse at some of the chip\u2019s capabilities, specifically its ability to prevent hackers from eavesdropping through your laptop microphone.\nThe T2 chip was first introduced in late 2017 with the iMac Pro, and it provides Apple with a secure way to store sensitive biometric data like fingerprints and the ability to encrypt and decrypt the built-in solid state drive. It will also give users a way to lock the boot process of the device, among other complex security tasks. The T2 chip has since been built into the mid-2018 MacBook Pro, which Apple says allows that machine to respond to verbal \u201cHey Siri\u201d requests without requiring you to press a button.\nThe T2 chip disables the MacBook Pro and Air microphone when the lid is closed\nOne feature of the chip, detailed for the first time in the security guide, is a \u201chardware disconnect\u201d that disables the microphone whenever the MacBook Pro or Air\u2019s lid is closed. \u201cThis disconnect is implemented in hardware alone, and therefore prevents any software, even with root or kernel privileges in macOS, and even the software on the T2 chip, from engaging the microphone when the lid is closed,\u201d the document reads. Apple says the camera does not contain a hardware disconnect option via the T2 chip because having the lid closed will obscure its field of view naturally.\nAs TechCrunch notes, the idea that Macs are largely hack-proof has been thoroughly debunked in recent years, as a number of high-profile and well-documented security threats have proven how vulnerabilities have gone largely undetected in Macs by the broader cybersecurity industry. The Fruitfly malware, which went undetected for years and only came to light last year, allowed hackers to gain complete control of a Mac, including the ability to remotely turn on its webcam, control the mouse and keyboard, take screenshots of the display, and even alert a third party to when the user was actively using the device.",
        "sentiment": 0.14044412206858398
    },
    "article_8": {
        "title": "Apple CEO Tim Cook is calling for Bloomberg to retract its Chinese spy chip story.",
        "body": "Apple CEO Tim Cook, in an interview with BuzzFeed News, went on the record for the first time to deny allegations that his company was the victim of a hardware-based attack carried out by the Chinese government. And, in an unprecedented move for the company, he called for a retraction of the story that made this claim.\nEarlier this month Bloomberg Businessweek published an investigation alleging Chinese spies had compromised some 30 US companies by implanting malicious chips into Silicon Valley\u2013bound servers during their manufacture in China. The chips, Bloomberg reported, allowed the attackers to create \u201ca stealth doorway\u201d into any network running on a server in which they were embedded. Apple was alleged to be among the companies attacked, and a focal point of the story. According to Bloomberg, the company discovered some sabotaged hardware in 2015, promptly cut ties with the vendor, Supermicro, that supplied it, and reported the incident to the FBI.\nApple, however, has maintained that none of this is true \u2014 in a comment to Bloomberg, in a vociferous and detailed company statement, and in a letter to Congress signed by Apple\u2019s vice president of information security, George Stathakopoulos. Meanwhile, Bloomberg has stood steadfastly by its story and even published a follow-up account that furthered the original\u2019s claims.\nThe result has been an impasse between some of the world\u2019s most powerful corporations and a highly respected news organization, even in the face of questions from Congress. On Thursday evening, an indignant Cook further ratcheted up the tension in response to an inquiry from BuzzFeed News.\n\u201cThere is no truth in their story about Apple,\u201d Cook told BuzzFeed News in a phone interview. \"They need to do the right thing and retract it.\"\nThis is an extraordinary statement from Cook and Apple. The company has never previously publicly (though it may have done so privately) called for the retraction of a news story \u2014 even in cases where the stories have had major errors or were demonstratively false, such as a This American Life episode that was shown to be fabricated.\nReached for comment, Bloomberg reiterated its previous defense of the story. \u201cBloomberg Businessweek's investigation is the result of more than a year of reporting, during which we conducted more than 100 interviews,\u201d a spokesperson told BuzzFeed News in response to a series of questions. \u201cSeventeen individual sources, including government officials and insiders at the companies, confirmed the manipulation of hardware and other elements of the attacks. We also published three companies\u2019 full statements, as well as a statement from China\u2019s Ministry of Foreign Affairs. We stand by our story and are confident in our reporting and sources.\u201d\nBloomberg did not answer questions about evidence supporting its allegations or the public remarks of its named sources.",
        "sentiment": -0.48072139322757723
    },
    "article_9": {
        "title": "Adobe announces full Photoshop CC for iPad shipping 2019, syncs with desktop",
        "body": "A full, desktop-class version of Photoshop on iOS has been one of the most hotly anticipated creative apps for designers and artists since the original iPad\u2019s introduction in 2010. In the years since, competitors have released their own products hoping to fill the void, but can\u2019t offer true integration with Creative Cloud that existing Photoshop customer have come to expect. Today at 2018\u2019s Adobe MAX conference in Los Angeles, Adobe is answering the requests of the creative community by previewing what it calls real Photoshop CC for iPad.\nIn earlier Creative Cloud mobile products, Adobe focused on bringing specific features of Photoshop to iOS users. Photoshop Express, Sketch, Mix, and Fix each offered limited functionality bound by previous performance ceilings of mobile hardware. Adobe says the capabilities of modern tablets and phones are what enabled them to untether Photoshop\u2019s full power from the desktop.\nIn an early preview of the software, Adobe demoed manipulating a heavy, multi-layer PSD file on an iPad Pro with ease, effortlessly applying effects and making edits in real time. Integration between the app and an Apple Pencil allowed for efficient retouching, erasing, and duplicating of objects.\nJust like on the desktop, a full toolbar and layer management interface will be available with the same tools, filters, adjustments, and masking options you\u2019ve come to know. These features have been implemented in a new user interface optimized for the iPad\u2019s size and multi-touch display, but appear to respect much of the muscle memory built by longtime Photoshop users.\nEdits made to PSD files on an iPad will seamlessly sync via Creative Cloud to the desktop version of Photoshop, where the same file can be opened and adjusted. Since both applications share a code base, no exporting, converting, or adjusting is required when moving between devices. This seamless syncing is made possible by Cloud Documents, another new piece of the Creative Cloud puzzle.\nIn order to ship a 1.0 quickly, Photoshop CC for iPad will debut with a smaller set of core features initially and the rest will be added back over time. Adobe hasn\u2019t established a timeline for when we can expect this migration to be complete.\nIn addition to the iOS version, Photoshop CC on the desktop received significant improvements today, including an improved Content-Aware Fill experience and a new Frame Tool for easy masking and resizing of images. The introduction of a feature called Symmetry Painting allows for the creation of mirrored brush strokes along a symmetry axis.\nFinally, Photoshop\u2019s undo shortcut has been modernized. No longer will users need to remember the unusual Command-Option-Z shortcut to step back beyond the most recent change. These changes, along with many more features ship today, and cloud syncing of PSDs will follow in the future. A full list of improvements is available on Adobe\u2019s website.\nThose wishing to learn more about Photoshop CC for iPad can sign up for updates. iPad users may also be interested in Project Gemini, an upcoming dedicated drawing and painting app that Adobe says will \u201cenable you to create digital art that was impossible before.\u201d We\u2019ve detailed the app in a separate post here along with all of Adobe\u2019s other Creative Cloud announcements this morning.\nFinally, be sure to check out our exclusive first look at Adobe\u2019s new Creative Cloud illustration style, and stay tuned to 9to5Mac this week for more on-location coverage of Adobe MAX.\nCheck out 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": 0.17098536863233205
    },
    "article_10": {
        "title": "Sources say Apple did not invite Bloomberg to its fall product event as retribution for its story about an alleged China hack",
        "body": "Amazon pulled its fourth-quarter advertisements from Bloomberg\u2019s website, a move some within the media giant think is retribution for Bloomberg\u2019s controversial story alleging that Chinese spies hacked into the online retailer\u2019s servers.\nAccording to a source in a position to know, Amazon\u2019s digital media buyer, Initiative, informed Bloomberg\u2019s sales staff on Oct. 16 that it would cancel its ad buys for the fourth quarter due to budget cuts. Internally, the source said, the staff received that decision, made only eight days after a previous communication with Initiative confirming that the ads would run, as a direct response to Amazon\u2019s displeasure over the Oct. 4 story. (Amazon announced Thursday that its marketing expenses for Q3 2018 were $3.3 billion, up more than $800 million from the year before.)\nGrant Milne, an Amazon spokesperson, told BuzzFeed News on Wednesday evening that the ads had been canceled due to a missed \u201ccreative deadline,\u201d contradicting Amazon\u2019s media buyer. Yet according to the same source, Wednesday night \u2014 after BuzzFeed News inquired about the pulled ads \u2014 Amazon placed a new, significantly smaller order for Q4 ads on Bloomberg TV.\nNeither Initiative nor Amazon told Bloomberg that they pulled the ads because of the report.\nThe canceled (and rebought) ads come amid intense criticism of Bloomberg\u2019s story about Amazon and Apple, the other major firm named in the report as a victim of the Chinese hack. In an interview with BuzzFeed News last week, Apple CEO Tim Cook called on Bloomberg to retract the story. On Monday, Amazon Web Services CEO Andy Jassy seconded Cook. And Jay Carney, a former White House press secretary and Amazon\u2019s senior vice president of global corporate affairs, tweeted Tuesday, \u201cWhen a story is this wrong, journalistic integrity demands a retraction.\u201d\nNo other outlet has been able to match Bloomberg\u2019s story, which alleged the insertion of spy chips in motherboards assembled in China that were then distributed through the server manufacturer Super Micro to dozens of American companies.\nAmazon isn\u2019t alone in apparently retaliating against Bloomberg over a report it says is untrue and riddled with inaccuracies. According to multiple sources, Bloomberg was not invited to Apple\u2019s fall product event next week in Brooklyn. Apple declined to comment.\nJohn Paczkowski contributed reporting.",
        "sentiment": -0.5877540847286582
    },
    "article_11": {
        "title": "Happy 17th Birthday to Apple's Original iPod",
        "body": "17 years ago today, then Apple CEO Steve Jobs stood on stage at the Apple Town Hall in Cupertino, California and showed off the very first iPod, a palm-sized device that held an entire music library and helped bring Apple back to profitability.\nAs Jobs introduced the first iPod, he called it a quantum leap forward in technology and outlined its three major breakthrough features: ultra-portability, Apple's legendary ease of use, and auto-sync with iTunes.\niPod is an MP3 music player, has CD quality music, and it plays all of the popular formats of open music. The biggest thing about iPod is that it holds a thousand songs. Now this is a quantum leap because for most people, it's their entire music library. This is huge. How many times have you gone on the road with a CD player and said 'Oh God, I didn't bring the CD I wanted to listen to.' To have your whole music library with you at all times is a quantum leap in listening to music. The coolest thing about iPod is that your whole music library fits right in your pocket. Never before possible.\nThe first-generation iPod was advertised as the device that put \"1,000 songs in your pocket,\" with the music stored on its 5GB 1.8-inch hard drive. Priced starting at $399, the first iPod lasted for 10 hours before needing to be recharged.\nThe commercial that was shown during the keynote event where Jobs introduced the iPod\nApple's original iPod featured a black and white LCD and Apple's first scroll wheel, a simple, intuitive mechanical scrolling interface designed to let users quickly scroll through long lists of music.\nThe scroll wheel led to the click wheel, which became an iconic feature of Apple's iPod lineup, included easy access buttons for playing, pausing, rewinding, and fast forwarding through music content.\nA later version of the iPod classic\nFollowing the launch of the first iPod, new versions followed on a yearly basis.\nOther notable updates included the iPod Photo in 2004, which had the first color display; the smaller iPod mini, which also came in 2004; the smaller and more portable iPod nano, introduced in 2005; the tiny iPod shuffle, even smaller than the nano, which came in 2005; the third-generation iPod nano \"fatty\" with a wider, squatter body, introduced in 2007; and the first iPod touch, also released in 2007.\nToday, the iPod has been largely replaced with the iPhone, which was first introduced in 2007. Today's iPhone XS, XS Max, and XR feature music storage and Apple Music, along with messaging and calling capabilities, a full App Store, and so much more.\nThe iPhone XR comes in multiple colors, much like Apple's iPods did\nApple retired the iPod nano and the iPod shuffle in July of 2017, and has slimmed down the iPod lineup to a single iPod -- the iPod touch. Apple has not updated the iPod touch since 2015, and it's likely that at some point, the entire iPod lineup may be put to rest for good.\nIt's not clear when that might happen, and for now, the iPod touch remains available for sale. It can be purchased from Apple for $199 for 32GB of storage or $299 for 128GB of storage.\nUnfortunately, there have been no rumors of an update, so Apple's future plans for the iPod remain up in the air.",
        "sentiment": 0.20087626560226732
    },
    "article_12": {
        "title": "iFixit confirms you can still repair your own iMac Pro or MacBook Pro",
        "body": "iFixit confirms you can still repair your own iMac Pro or MacBook Pro Despite documents saying otherwise, systems with replacement parts still work.\nYesterday MacRumors and Motherboard reported on Apple service documents that indicated anyone replacing key parts on computers equipped with its custom T2 chip would require special diagnostic software to finish the job. While Apple has not commented on the leaks itself, the DIY repair folks at iFixit tested out the possibility by buying a brand-new 2018 MacBook Pro, pulling it apart and replacing the display. Shocker: it still worked, even without the software.\nAs they put it, any \"secret repair kill switch hasn't been activated -- yet.\" So far, it has limited approaches that limit repairs based on security to the TouchID and FaceID sensors that require specialized software, as I noted yesterday, even though people have reported trouble with the ambient light sensor after replacing iPhone displays.\nWhile it's possible that a future software update could change things and make it require specialized software that only official Apple Stores and authorized service centers have access to, we're not there yet. Passing \"right to repair\" laws currently under consideration could be a big step to guaranteeing things stay that way.",
        "sentiment": 0.007306967512704432
    },
    "article_13": {
        "title": "Stardew Valley coming to iOS October 24th!",
        "body": "Hello everyone,\nI hope you\u2019ve all had a great summer and are enjoying this transition to the cooler weather of fall.\nToday, I\u2019ve got some exciting news to share with everyone \u2013 Stardew Valley is coming to mobile, and it\u2019s actually releasing on the iOS App Store in just over 2 weeks time, on October 24th!\nThe mobile port is being developed by London-based mobile development studio, The Secret Police, who have been working with me for about a year. Chucklefish has been handling the business and marketing side of things. While the game will launch first on the iOS App Store, The Secret Police are currently working on finishing up the Android version, and I hope to give you more news about an exact launch date soon.\nFeatures:\nSupports iPhone and iPad gaming, just make sure to check on the App Store page whether the game is compatible with your iOS version\nIt\u2019s the full game, not a cut down version, and plays almost identically to all other versions. The main difference is that it has been rebuilt for touch-screen gameplay on iOS (new UI, menu systems and controls)\n\u00a37.99 / $7.99 / 8,99 \u20ac with no in-app purchases\nIncludes 1.3 single player content, such as the Night Market, but not Multiplayer\nPC players can transfer their save data to the iOS version via iTunes \u2013 but make sure to always back up your save data first! Mods are not supported, and any save data transferred containing mods may cause compatibility issues.\nPre-orders are available today\nI want to be sure it is clear that the development of the mobile port has had no impact on the development of the console Multiplayer ports, as they are being worked on by two entirely separate teams/companies. Console multiplayer is making great progress and I have every intention to release it as quickly as possible. For a quick update on where it\u2019s at, the Switch Multiplayer update is coming along well, and has now gone into QA (quality assurance testing). Xbox One and PS4 Multiplayer updates will follow (all being done by Sickhead Games). I promise to keep you all updated on any news I get regarding release time frames for all the consoles. I really appreciate everyone\u2019s patience.\nThat\u2019s all for now!\n-ConcernedApe",
        "sentiment": 0.2673394507418076
    },
    "article_14": {
        "title": "Apple fixes its new bagel emoji with cream cheese and a doughier consistency",
        "body": "Apple\u2019s most egregious crime in recent memory \u2014 a subpar bagel emoji \u2014 has been rectified, as first spotted by Jeremy Burge of Emojipedia. In the fourth beta release of iOS 12.1, it appears that the bagel has been replaced with a new icon that features both cream cheese and a doughier consistency more reminiscent of a fresh, hand-rolled bagel and not the frozen and machine-cut grocery store variety it was accused of emulating in its original form.\nIf you haven\u2019t been following the bagel emoji controversy, it\u2019s likely because you have better ways to spend your time. But to catch you up quickly: the iOS bagel emoji \u2014 like Android\u2019s goofy cheeseburger one that generated negative headlines last year \u2014 was widely decried for its unappetizing look and its lack of cream cheese. Many thought it looked like the kind of bagel you bought in a six-pack from your local grocery store:\nOf course, cream cheese brand Philadelphia had to squeeze some prime social media engagement out of the whole thing, resulting in a Twitter poll and then a facetious Change.org petition to get the emoji changed to one with cream cheese:\nYet like most conversations around food that has distinct ethnic and regional histories (think pizza), it then devolved into semi-ironic, faux-outrage over what\u2019s the most authentic conception of a bagel. As Vox put it, conversations around emoji are really about representation and cultural identity, especially in America where the concept of emoji is somewhat divorced from its Japanese origins as an abstract visual representation of a word.\n\u201cThe outcry over the bagel emoji suggests that there are people who really do feel \u2014 on some level, even if it is tongue-in-cheek \u2014 that the bagel does represent them in some way,\u201d Rachel Sugar wrote at Vox, \u201cand that this anemic version (\u2018the most gentile bagel ever baked\u2019) does a disservice not only to carbohydrates but to the rich diversity of American identity.\u201d You see this similarly play out when we as an online society collectively freak out over, say, former New York Governor candidate Cynthia Nixon\u2019s bagel choice.",
        "sentiment": -0.5937455520033836
    },
    "article_15": {
        "title": "PSA: The iOS App Store is No Longer Listing All In-App Purchases",
        "body": "PSA: The iOS App Store is No Longer Listing All In-App Purchases\nA concerning change was quietly pushed to the iOS App Store recently. Users this week noticed that Apple is no longer listing all in-app purchases and their prices for apps and games that offer them.\nTo clarify, developers have the ability to highlight featured in-app purchases in their app listing page. These appear as a horizontally scrollable list with a large \u201cIn-App Purchases\u201d heading between the \u201cWhat\u2019s New\u201d and \u201cPreview\u201d sections. An example of this is the game Galaxy on Fire 3.\nMissing In-App Purchase Information\nBut these featured in-app purchases were optional, and regardless of whether the developer chose to use them Apple included a complete list of all in-app purchases and their prices as an expanding item in the \u201cInformation\u201d section at the bottom of the app listing. We unfortunately don\u2019t have a screenshot of this at the moment because it\u2019s now gone and we never thought Apple would be crazy enough to remove it.\nUpdate: This change also affects the Mac App Store.\nFor example, take a look at the game Marvel Battle Lines. Apple thankfully still tells you that the game has in-app purchases (via the arguably too small text next to the Get/Buy button), but the developer of this app didn\u2019t elect to include any featured in-app purchases and the complete in-app purchase list in the Information section is nowhere to be found.\nThe problem with this new policy is that the nature and prices of in-app purchases vary wildly depending on the developer and type of app or game. Users could previously check out the description and prices of the in-app purchases to determine if they were reasonable before downloading or buying an app. Now, it seems, users must download and launch the app to see the same information.\nThis results in not only an inconvenience for the user, but it also inflates download statistics for app developers and potentially exposes user information as well. Users who are more technically savvy and aware of the risks might simply avoid apps that don\u2019t list their in-app purchases on the store page, but millions of others will be effectively tricked into downloading, launching, and potentially providing information to app developers before they even know if the in-app purchase model for a particular app is acceptable.\nIn-app purchase information was already slightly hidden, but still available for those who wanted to see it. Apple\u2019s decision to remove it and rely instead on the developers to optionally provide such data is completely anti-consumer and frankly baffling. We can only hope that user feedback forces the company to restore this feature quickly.\nUpdate: 2018-11-01\nThe full in-app purchase list has been restored to the iOS 12 App Store. Some initial explanations claim its removal was due to a bug, although its return is welcomed regardless.",
        "sentiment": -0.3817934743128717
    },
    "article_16": {
        "title": "Apple overhauls its privacy pages, &amp; now lets U.S. customers download their own data.",
        "body": "Apple has refreshed and expanded its privacy website, a month after its most recent iPhone and Mac launches.\nYou\u2019re not going to see much change from previous years \u2014 the privacy pages still state the same commitments that Apple\u2019s long held, like that privacy is a \u201cfundamental human right\u201d and that your information is largely on your iPhones, iPads and Macs. And, now with a bevy of new security and privacy features in iOS 12 and macOS Mojave, the pages are updated to include new information about end-to-end encrypted group FaceTime video calls and improvements to intelligence tracking protections \u2014 and, how it uses differential privacy to understand which are the most popular features so it can improve, without being able to identify individual users.\nOne key addition this time around: Apple is expanding its data portal to allow U.S. customers to get a copy of the data that the company stores on them.\nIt\u2019s the same portal that EU customers have been able to use since May, when the new EU-wide data protection rules \u2014 known as General Data Protection Regulation, or GDPR \u2014 went into effect. That mandated companies operating in Europe to allow customers to obtain a copy of their own data.\nApple\u2019s making good on its promise earlier this year that it would expand the feature to U.S. customers. Customers in Canada, Australia and New Zealand can also request their data.\nBut because the company doesn\u2019t store that much data on you in the first place \u2014 don\u2019t expect too much back. When I asked Apple for my own data, the company turned over only a few megabytes of spreadsheets, including my order and purchase histories, and marketing information. Any other data that Apple stores is either encrypted \u2014 so it can\u2019t turn over \u2014 or was only held for a short amount of time and was deleted.\nThat\u2019s a drop in the ocean compared to data hungry services like Facebook and Google, which compiled an archive of my data ranging from a few hundred megabytes to over a couple of gigabytes of data.\nApple refreshes its privacy pages once a year, usually a month or so after its product launches. It first launched its dedicated privacy pages in 2014, but aggressively began pushing back against claims revealed after the NSA surveillance scandal. A year later, the company blew up the traditional privacy policy in 2015 by going more full-disclosure than any other tech giant at the time.\nSince then, its pages have expanded and continued to transparently lay out how the company encrypts user data on its devices, so not even the company can read it \u2014 and, when data is uploaded, how it\u2019s securely processed and stored.",
        "sentiment": -0.030413132393732667
    },
    "article_17": {
        "title": "Apple\u2019s thought process in designing gesture based UI/UX",
        "body": "Download\nHey, guys. Thank you.\nThanks for coming by, guys.\nWelcome to Designing Fluid Interfaces. My name is Chan. And, I work on the human interface team here at Apple. And, most recently, I worked on this, the fluid gestural interface for iPhone 10.\nSo, me, Marcos, and Nathan, we want to share a little bit about what we learned working on this, and other projects like this in the past.\nSo, the question we ask ourselves a lot is, what actually makes an interface feel fluid? And, we've noticed that a lot of people actually describe it differently. You know, sometimes, when people actually try this stuff, when we show them a demo, and they try it, and they hold it in their hands, they sometimes say it feels fast. Or, other people sometimes say it feels smooth. And, when it's feeling really good, sometimes people even say it feels natural, or magical.\nBut, when it comes down to it, it really feels like, it's one of those things where you just know it when you feel it. It just feels right.\nAnd, you can have a gestural UI, and we've seen lots of gestural UI's out there, but if it's not done right, something just feels off about it. And, it's oftentimes hard to put your finger on why.\nAnd, it's more than just about frame rates, you know. You can have something chugging along at a nice 60 frames per second, but it just feels off. So, what gives us this feeling? Well, we think it boils down to when the tool feels like an extension of your mind. An extension of your mind.\nSo, why is this important? Well, if you look at it, the iPhone is a tool, right? It's a hand tool for information and communication. And, it works by marrying our tactile senses with our sense of vision.\nBut, if you think about it, it's actually part of a long line of hand tools extending back thousands of years.\nThe tool on the left here was used to extract bone marrow 150,000 years ago, extending the sharpness of what our fingers could do. So, we've been making hand tools for some time now. And, the most amazing thing is that our hands have actually evolved and adapted alongside our tools. We've evolved a huge concentration of muscles, nerves, blood vessels that can perform the most delicate gestures, and sense the lightest of touches.\nSo, we're extremely adapted to this tactile world we all live in.\nBut, if you look at the history of computers, we started in a place where there was a lot of layers of extraction between you and the interface. There was so much you had to know just to operate it. And, that made it out of reach for a lot of people.\nBut, over the last few decades or so, you've sort of been stripping those layers back you know, starting with indirect manipulation, where things were a little bit more one-to-one. A little bit more direct all the way to now, where we're finally stripping away all those layers back, to where you're directly interacting with the content. This to us is the magical element. It's when it stops feeling like a computer, and starts feeling more like an extension of the natural world. This means the interface is now communicating with us at a much more ancient level than interfaces have ever done. And, we have really high standards for it. You know, if the slightest thing feels wrong, boom, the illusion is just shattered. But, when it feels right, it feels like an extension of yourself, an extension of your physical body. It's a tool that's in sync with your thought. It feels delightful to use, and it feels really low-friction, and even playful.\nSo, what gives us this feeling? And, when it feels off, how do we make it feel right? That's what this presentation's all about.\nWe're going to talk about four things today. And, we're going to start with designing some principles, talking about how we build interfaces that feel like an extension of us.\nHow to design motion that feels in tune with the motion of our own bodies, and the world around us.\nAnd, also designing gestures that feel elegant and intelligent. We're also going to talk about that, now that we've built this kind of stuff, how do we build interactions on top of it that feel native to the medium of touch, as a medium? So, let's get started. How do we design an interface that actually extends our mind? How do we do this? Well, we think the way to do it, is to align the interface to the way we think and the way we move.\nSo, the most important part of that is that our minds are constantly responding to changes and stimulus and thought, you know? Our minds and bodies are constantly in a state of dynamic change. So, it's not that our interfaces should be fluid, it's that we're fluid, and our interfaces need to be able to respond to that.\nSo, that begins with response.\nYou know, our tools depend on the latency. Think about how hard it would be to use any tool, or play an instrument, or do anything in the physical world, if there was a delay to using it? And, we found that people are really, really sensitive to latency. You know? If you introduce any amount of lag, things all of a sudden just kind of fall off a cliff in terms of how they respond to you. There's all this additional mental burden. It feels super disconnected. It doesn't feel like an extension of you anymore.\nSo, we work so hard to reduce latency. Where, we actually engineered the latest iPhone to respond quicker to your finger, so we can detect all the nuances of your gestures as instantly as possible.\nSo, we really care about this stuff, and we think you should too. And, that means look for delays everywhere. It's not just swipes. It's taps, it's presses, it's every interaction with the object. Everything needs to respond instantly.\nAnd, during the process of designing this stuff, you know, oftentimes the delays kind of tend to seep in a little bit. You know? So, it's really important to keep an eye out for delays. Be vigilant and mindful of all the latencies or timers that we could introduce into the interface so that it always feels responsive.\nSo, that's the topic of response. It's really simple, but it makes an interface feel lively and dynamic. Next, we want to allow for constant redirection and interruption. This one's big.\nSo, our bodies and minds are constantly in a state of redirecting in response to change in thought, like we talked about.\nSo, if I was walking to the end of this stage here, and I realize I forgot something back there, I could just turn back immediately. And, I wouldn't have to wait for my body to reach the end before I did that, right? So, it's important for our interfaces to be able to reflect that ability of constantly redirecting. And, it makes it feel connected to you.\nThat's why for iPhone 10, we built a fully redirectable interface.\nSo, what's that? So, the iPhone 10's an actually-- it's pretty simple two-axis gesture. You go horizontally between apps. And, you go vertically to go home. But you can also mix the two axes, so you can be on your way home, and peek at multitasking and decide whether or not to go there. Or, you can go to multitasking and decide, actually, no, I want to go home.\nSo, this might not seem that important, but what if we didn't do this? What if it wasn't redirectable? So, what if the only gestures you could do was this horizontal gesture between apps, and then a vertical gesture to go home, and that's it. You couldn't do any of that in-between stuff I just mentioned.\nWell, what would happen is that you would have to think before what you did, before you performed the gesture, you'd have to think what you want to do.\nAnd so, the series of events would be very linear, right? So, you'd have to think, do I want to go home? Do I want to go to multitasking? Then you make your decision, then you perform the gesture, and then you release.\nBut, the cool thing is when it's redirectable, the thought and gesture happen in parallel. And, you sort of think it with the gesture, and it turns out this is way faster than thinking before doing. You know? Because it's a multi-axis gestural space. It's not separate gestures. It's one gesture that does all this stuff. Home, multitasking, quick app switching, so you don't have to think about it as a separate gesture.\nAnd, helps with discovery. Because you can discover a new gesture along the path of an existing gesture.\nAnd, it allows you to layer gestures at the speed of thought. So, what does that last one mean? So, let me show you some examples. And, we've slowed down the physics on the simulation, so you can actually see a little bit what I'm talking about.\nSo, I can swipe to go home, and then swipe to the next page, or springboard while I'm going home. I can layer these two gestures once I've internalized them.\nAnother example is that I can launch an app and realize, oh, actually I need to go to multitasking, and I can interrupt the app and go straight to multitasking, while the app is launching.\nOr, I can launch an app and realize, oh, that was the wrong app. And, I can shoot it back home, while I'm launching it. Now, there's one other one where I can actually just launch an app, and if I'm in a hurry, I can start interacting with the app as it's launching.\nSo, this stuff might not seem really important, but we've found it's super important for the interface to be always responding, always understanding you. It always feels alive. And, that's really important for your expectation and understanding of the interface, to be comfortable with it. To realize that it's always going to respond to you when you need it. And, that applies as well to changes in motion, not just to the start of an interaction, but when you're in the middle of an interaction, and you're changing. It's important for us to be responsive to interruption as well. So, a good example is multitasking on iPhone 10.\nSo, we have this pause gesture where you slide your finger up halfway up the screen, and pause, and so we need to figure out how to detect this change in motion. And so, how do we do this? How do we detect this change in motion? Should we use a timer? Should we wait until your finger has come below a certain velocity for a few amount of time, and then bring in the multitasking cards? Well, it turns out that's too slow. People expect to be able to get to multitasking instantly. And, we need a way that can respond as fast as them. So, instead we look at your finger's acceleration.\nIt turns out there's a huge spike in the acceleration of your finger when you pause. And, actually the faster you stop, the faster we can detect it. So, it's actually responding to the change in motion, as fast as we know how, instead of waiting for some timer.\nSo, this is a good example of responding to redirection as fast as possible. So, this is the concept of interruption and redirection. This stuff makes the interface feel really, really connected to you.\nNext, we want to talk a little bit about the architecture of the interface. How you lay it out, conceptually.\nAnd, we think when you're doing that, it's important to maintain spatial consistency throughout movement.\nWhat does that mean? This kind of mimics the way our object persistence memory works in the real world. So, things smoothly leave and enter our perception in symmetric paths.\nSo, if something disappears one way, we expect it to emerge from where it came? Right? So, if I walked off this stage this way, and then emerged that way, you'd be pretty amazed, right? Because that's impossible. So, we wanted to play into this consistent sense of space that we all have in the world. And so, what that means is, if something is going out of view in your interface, and coming back into view, it should do so in symmetric paths. It should have a consistent offscreen path as it enters and leaves. A good example of this is actually iOS navigation. When I tap on an element in this list here, it slides in from the right.\nWhen I tap the back button, it goes back to the right. It's a symmetric path. Each element has a consistent place where it lives at both states. This also reinforces the gesture. If I choose to slide it myself to the right, because I know that's where it lives, I can do that. It's expected.\nSo, what if we didn't do this. Here's an example, where when I tap on something, it slides in, and then when I hit back it goes down. And, it feels disconnected and confusing, right? It feels like I'm sending it somewhere. In fact, if I wanted to communicate that I was sending it somewhere, this is how I could do it, right? So, that's the topic of spatial consistency.\nIt helps the gesture feel aligned with our spatial understanding of the world. Now, the next one is to hint in the direction of the gesture.\nYou know, we humans are always, kind of, predicting the next few steps of our experience. We're always using the, kind of, trajectories of everything that's happening in the world to predict the next few steps of emotion.\nSo, we think it's great when an interface plays into that prediction. So, if you have two states here, initial state and a final state. The object-- and you have an intermediate transition. The object should transition smoothly between these two states in a way that it grows from the initial state to the final state, whether it's through a gesture or an animation.\nSo, good example is Control Center actually. We have these modules here in Control Center, where as you press they grow up and out towards your finger in the direction of the final state, where it actually finally just pops open. So, that's hinting. It makes the gestures feel expected, and predictable. Now, the next important principle is to keep touch interactions lightweight.\nYou know the lightness of multitouch is one of the most underrated aspects of it, I think. It enables the airy swipes and scrolls, and all the taps and stuff that we're all used to. It's all super lightweight. But, we also want to amplify their motion. You want to take a small input and make a big output, to give that satisfying feeling of moving or throwing something and having a magnified result. So, how does this apply to our interfaces? Well, it starts with a short interaction.\nA short, lightweight interaction.\nAnd, we use all our sensors, all our technology, to understand as much about it. To, sort of, generate a profile of energy and momentum contained within the gesture.\nUsing everything we know, including position, velocity, speed, force, everything we know about it to generate a kind of, inertial profile of this gesture. And then we take that, and generate an amplified extension of your movement. It still feels like an extension of you. So, you get that satisfying result with a light interaction.\nSo, a good example of this is scrolling, actually. Your finger's only onscreen for a brief amount of time, but the system is preserving all your energy and momentum, and gracefully transferring it into the interface.\nSo, what if it didn't have this? Those same swipes, well, they wouldn't get you very far.\nAnd, in order to scroll, you'd have to do these long, laborious swipes that would require a lot more manual input. It would be a huge pain to use.\nAnother good example of this is swipe to go home.\nThe amount of time that your finger's onscreen is very light. And, it's-- ends up making it a much more liquid and lightweight gesture that still feels native to the medium of multitouch.\nWhile still being able to reuse a lot of your muscle memory from a button, because you move your finger down on the screen, and back up to the springboard. And, it's not just swipes, it's taps too. It's important for an interface to respond satisfyingly to every interaction. The interface is signaling to you that it understood you. It's so important for the interface to feel alive and connected to you.\nSo, that's the topic of lightweightness and amplification.\nThe next one is called rubberbanding.\nIt means we're softly indicating boundaries of the interface. So, in this example, the interface is gradually and softly letting you know that there's nothing there. And, it's tracking you throughout. It's always letting you know that it's understanding you.\nWhat happens if you didn't do that? Well, it would feel like this. It would feel super harsh and disconcerting. You kind of hit a wall there. It would feel broken, right? And, you actually wouldn't know the difference between a frozen phone, and phone that's just at the top of the edge of the screen, right? So, it's really important that it's always telling you that you've reached the edge. And, this applies to transitions, too. It's not just about when you hit the edge, it's also when you hand off from one thing to another thing. Tracking. So, a good example of this is when you transition from sliding up the dock to sliding up the app. It doesn't just hit a wall, and one thing stops tracking, and then the other thing takes over. They both smoothly hand off in smooth curves, so that you don't feel like there's this harsh moment where you hand off from one thing to another. Next one is to design smooth frames of motion.\nSo, imagine I have a little object here moving up and down. It's very simple. But, we all know this object is not really moving, right? We're all just having the perception of it moving. Because we're seeing a bunch of frames on the screen all at once, and it's giving us the illusion of it moving. So, if we took all of those frames of motion, and kind of, spread them out here. And we see the ball's in motion over time, the thing that we're concerned about is right around here, where there's too much visual change between the adjacent frames. This is when the perception of the interface becomes a little choppy. You get this visual strobing.\nAnd, this is because the difference between the two frames is too much.\nAnd, it strobes against your vision, so. Here's an example of where you have two things both moving at 30 frames per second. But the one on the left looks a bit smoother than the one on the right, because the one on the right is moving so fast, that it's strobing. My perception of vision is, kind of, breaking down. I don't believe that it's moving smoothly any more.\nSo, the important thing to take away is that it's not just about framerate. It's what's in the frames.\nSo, we're kind of limited by the framerate, and how fast we can move and still preserve a smooth motion.\nSo, this one's in 30 frames per second. If we move it up to 60 frames per second, you can see that we can actually go a little bit faster, and still preserve smooth motion. We can do faster movement without strobing.\nAnd, there's addition tricks we can do too, we can do things like motion blur. Motion blur basically bakes in more information in each frame about the movement, like the way your eyes work, and the way a camera works.\nAnd you can also do-- take a page from 2D animation and video games by stretching, this-- this technique called motion stretching stretches the content in each frame to provide this elastic look as it moves with velocity. And so, in motion, it kind of looks like this. So, each of the different techniques, kind of, tries to encode more information visually about what's going on in the motion. And, I want to focus a little bit on this last one here, motion stretching, because we do this on iPhone 10, actually. You know, when you launch an app, the icon elastically stretches down to become the app as it opens.\nAnd, it stretches up in the opposite direction as you close the app. To give you that little bit extra information between each frame of motion to make it a little bit smoother-looking. Lastly, we want to work with behavior rather than animation. You know, things in the real world are always in a state of dynamic motion, and they're always being influenced by you. They don't really work like animations in the animation sense, right? There's no animation curve prescribed by real life. So, we want to think about animation and behavior more as a conversation between you and the object. Not prescribed by the interface. So, to move away from static things transitioning into animated things, instead think about behavior. So, Nathan's about to dive deep into this one. But, here's a quick example. So, in Photos, there's less mass on the photos, because it's conceptually lighter. But, then when you swipe apps, there's more mass on the apps. It's conceptually heavier, so we give more mass to the system.\nSo, that's a little bit about how to design interfaces that think and work like us.\nIn-- it starts with response. To make things feel connected to you, and to accommodate the way our minds are constantly in motion.\nTo maintain spatial consistency, to reinforce a consistent sense of space, and symmetric transitions within that space.\nAnd, to hint in the direction of the gesture. To play into our prediction of the future. And, to maintain lightweight interactions, but amplify their output.\nTo get that satisfying response, while still keeping the interaction airy and lightweight. And, to have soft boundaries and edges to the interface. That interface is always gracefully responding to you, even when you hit an edge, or transition from tracking one thing to tracking the other. And, to design smooth dynamic behavior that works in concert with you. So, that's some principles for how to approach building interfaces that feel like an extension of our minds.\nSo, let's dive in a little deeper. I'm going to turn it over to Nathan de Vries, my colleague, to design motion-- to talk about designing motion in a way that feels connected to motion, to the motion of both you and the natural world. Thanks, Chan.\nHi everyone. My name's Nathan, and I'm super excited to be here today to talk to you about designing with dynamic motion.\nSo, as Chan mentioned, our minds and our bodies are constantly in a state of change. The world around us is in a state of change. And, this introduces this expectation that our interfaces behave the same way, as they become more tactile, it shifts our expectations to be much higher fidelity.\nNow, one way we've used motion in interfaces is through timed animations. A button is tapped on the screen, and the reins are, kind of, handed over to the designer.\nAnd, their job is to craft these perfect frames of animation through time. And, once that animation is complete, the controls are handed back to the person using the interface, for them to continue interacting.\nSo, you can kind of think of animation and interaction as being-- as moving linearly through time in this, kind of, call and response pattern. In a fluid interface, the dynamic nature of the person using the interface kind of shifts control over time away from us as designers.\nAnd, instead, our role is to design how the motion behaves in concert with an interaction. And, we do this through these continuous dynamic behaviors that are always running, that are always active. So, it's these dynamic behaviors that I'm going to, really focus on today. First of all, we're going to talk about seamless motion. And, it's this element of motion that makes it feel like the dynamic motion is an extension of yourself.\nThen, we're going to take a look at character. How, even without timing curves, and timed animations, we can introduce the concept of playfulness, or character, or texture to motion in your interfaces.\nAnd finally, we'll look at how motion itself gives us some clues about what people intend to do with your interface. How we can resolve some uncertainty about what a gesture is trying to do by really looking at the motion of the gesture. So, to kick things off, let's look at seamless motion. What do I mean by seamless motion? So, let's look at an example that I think we can all familiarize with.\nSo, here we have a car, and it's cruising along at a constant speed. And then, the brakes are applied, slowing it down to a complete stop. Let's look at it again, but this time we'll plot out the position of the car over time. So, at the very start of this curve it's, kind of, straight, and pointing up to the right. And, this shows that the car's position is moving at a constant rate, it's kind of unchanging.\nBut then, you'll notice the curve starts to bend, to smoothly curve away from this straight line. And, this is the brakes being applied. The car is decelerating from friction being introduced.\nAnd, by the end of the curve, the curve is completely flat, horizontal, showing that the position is now unchanging. That the car is stopped. So, this position curve is visualizing essentially what we call seamless motion. The line is completely unbroken, and there are no sudden changes in direction.\nSo, it's smooth and it's seamless. Even when, actually, new dynamic behaviors are being introduced to the motion of the car, like a brake, which is applying friction to the car.\nAnd, even when the car comes to a complete stop, you'll notice that the curve is completely smooth. There's this indiscernible quality to it. You can't tell when the car precisely stopped. So, why am I talking about cars? This is a talk about fluid interfaces, right? So, we feel like the characteristics of the physical world make for great behaviors.\nEveryone in this room finds the car example so simple because we have a shared understanding, or a shared intuition for how an object like a car moves through the world.\nAnd, this makes us a great reference point. Now, I don't mean that we need to build perfect physical simulations of cars that literally drive our interface. But, we can draw on the motion of a car, of objects that we throw or move around in the physical world around us and use them in our own dynamic behaviors to make their motion feel familiar, or relatable, or even believable, which is the most important thing. Now, this idea of referencing the physical world in dynamic behaviors has been in the iPhone since the very beginning with scrolling.\nA child can pick up an iPhone and scroll to their favorite app on the Home screen, just as easily as they can push a toy car across the floor. So, what are some key, kind of, characteristics of this scrolling, dynamic behavior that we have? Well, firstly it's tapping into that intuition, that shared understanding that we all have for objects moving around in the world. And, our influence on those objects.\nThe motion of the content is perfectly seamless, so while I'm interacting with it, while I'm dragging the content around, my body is providing the fluidity of the movement, because my body is fluid.\nBut, as soon as I let go of the content, it seamlessly coasts to a stop. So, we're kind of maintaining the momentum of the effort being put into the interface.\nThe amount of friction that's being used for scrolling is consistent, which makes it predictable, and very easy to master.\nAnd finally, the content comes to an imperceptible stop, kind of like the car, not really knowing precisely when it came to a stop.\nAnd, we feel that this distinct lack of an ending kind of reinforces this idea that the content is always moving, and always able to move, so while content is scrolling, it makes it feel like you can just put your finger down again, and continue scrolling. You don't have to wait for anything to finish. So, there are innumerable characteristics of the physical world that would make for great behaviors.\nWe don't have time to talk about them all, but I'd like to focus on this next one, because we personally find it incredibly indispensable in our own design work.\nSo, materials like this beautiful flower here, the natural fibers of this flower have this organic characteristic called elasticity.\nAnd, elasticity is this tendency for a material to gracefully return into a resting state once stress or strain is removed.\nOur own bodies are incredibly elastic.\nNow, we're capable of running incredibly long distances, not because of the strength of our muscles, but because of their ability to relax.\nIt's their elasticity that's doing this.\nSo, our muscles contract and relax once stress and strain is removed. And, this is how we conserve energy. Makes us feel natural and organic. The same elasticity is used in iPhone 10.\nTap an icon on the Home screen, and an elastic behavior is pulling the app towards you.\nBring it exactly where you want it to be. And, when you swipe from the bottom, the app is placed back on the Home screen in its perfect position. We also use elasticity in scrolling. So, if I scroll too far and rubberband, like Chan was talking about, when you let go, the content uses elasticity to pull back within the boundaries, helping you get into this resting position, ready for the next time you want to scroll. So, let's dig in a little deeper on how this elasticity works behind the scenes. You can think of the scrolling content as a ball attached to a spring. On one end of the spring is the current value. This is where the content is on the display.\nAnd, the other end of the spring is where the content wants to go because of its elasticity. So, you've got this spring that's pulling the current value towards the target. Its behavior is influencing the position of the content.\nNow, the spring is essentially pulling that current value towards the target.\nAnd, what's interesting about a spring is, it does this seamlessly. This seamlessness is, kind of, built in to the behavior.\nAnd, this is what makes them such versatile tools for doing fluid interface design.\nIs that you, kind of, get this stuff for free. It's baked in to the behavior itself. So, we love this behavior of a value moving towards a target. We can just tell the ball where to go, and we'll get this seamless motion of the ball moving towards the target. But, we want a little bit more control over how fast it moves. And, whether it overshoots. So, how do we do that? Well, we could give the ball a little more mass, like make it bigger, or make it heavier.\nAnd, if we do that, then it changes the inertia of the ball, or its willingness to want to start moving. Or, maybe its unwillingness to want to stop moving. And, you end up with this little overshoot that happens. Another property that we could change is the stiffness of the spring, or the tensile strength of the spring. And, what this does, is it affects the force that's being applied to the ball, changing how quickly it moves towards the target. And, finally, much like the car, and the braking of a car, we can change the damping, or the friction, of the surface that the ball is sitting on. And, this will act as, kind of, a brake that slows the ball down over time, also affecting our ability to overshoot. So, the physical properties of a ball and a spring are, kind of, physics class material, right? It's super useful in a scientific context, but we've found that in our own design work they can be a little bit overwhelming or unwieldy for controlling the behavior of objects on the screen.\nSo, we think our design tool should have a bit of a human interface to them. That they need to reflect the needs of the designer that's using the tool.\nAnd so, how do we go about that? How do we simplify these properties down to make it more design friendly? So, mass stiffness and damping will remain behind the scenes, they're the fundamental properties of the spring system that we're using. But, we can simplify our interface down to two simple properties.\nThe first is damping, which controls how much or little overshoot there is from 100% damping, where there will be no overshoot to 0% damping where the spring would oscillate indefinitely.\nThe second property is response.\nAnd, this controls how quickly the value will try and get to the target.\nAnd, you might notice that I haven't used the word duration. We actually like to avoid using duration when we're describing elastic behaviors, because it reinforces this concept of constant dynamic change. The spring is always moving, and it's ready to move somewhere else.\nNow, the technical terms for these two properties are damping ratio and frequency response. So, if you'd like to use these for your own design work, you can look up those terms, and you'll find easy ways to convert them. So, we now have these two simple properties for controlling elastic behaviors. But, there's still an infinite number of possibilities that we can have with these curves. Like, there's just hundreds, thousands, millions of different ways we can configure those two simple properties and get very different behavior.\nHow do we use these to craft a character in our app? To control the feel of our app? Well, first and foremost, we need to remember that our devices are tools.\nAnd, we need to respect that tools, when they're used with purpose, require us to not be in the way, not get in the way with introducing unnecessary motion.\nSo, we think that you should start simple.\nA spring doesn't need to overshoot. You don't need to use springy springs.\nSo, we recommend starting with 100% damping, or no overshoot when you're tuning elastic behaviors.\nThat way you'll get smooth, graceful, and seamless motion that doesn't distract from the task at hand. Like, just quickly shooting off an email.\nSo, when is it appropriate to use bounciness? There's got to be a time when that's appropriate, right? Well, we feel if the gesture that's driving the motion itself has momentum, then you should reward that momentum with a little bit of overshoot.\nPut another way, if a gesture has momentum, and there isn't any overshoot, it can often feel broken or unsatisfying to have the motion follow that gesture. An example of where we use this is in the Music app.\nSo, the Music app has a small minibar representing Now Playing at the bottom of the screen, and you can tap the bar to show Now Playing. Because the tap doesn't have any momentum in the direction of the presentation of Now Playing, we use 100% damping to make sure it doesn't overshoot. But, if you swipe to dismiss Now Playing, there is momentum in the direction of the dismissal, and so we use 80% damping to have a little bit of bounce and squish, making the gesture a lot more satisfying.\nBounciness can also be used as a utility, as a functional means.\nIt can serve as a helpful hint that there's something more below the surface. With iPhone 10, we introduced two buttons to the cover sheet for turning on the Flashlight, and for launching the Camera.\nTo avoid accidentally turning on the flashlight by mistake, we require a more intentional gesture to activate the Flashlight.\nBut, if you don't know that there's a more intentional gesture needed to activate it, when you tap on the button, it responds with bounciness. Has this kind of playful feel to it.\nAnd, that hint is teaching you not only that the button is working, but that it's responding to you. But, it's kind of teaching you that if you press just a little bit more firmly, it'll activate. It's like teaching you. It's hinting in the direction of the motion. So, bounciness can be used to indicate this kind of thing. Now, so far we've been talking about using motion to move things around, or to change their scale, change their visual representation on the screen.\nBut, we perceive motion in many different ways.\nThrough changes in light and color, or texture and feel.\nOr even sound. Many other sensations that we-- our senses can detect. We feel this is an opportunity to go even further, go beyond motion, when you're tuning the character of your app.\nBy combining dynamic behaviors for motion with dynamic behaviors for sound and haptics, you can really fundamentally change the way an interface feels.\nSo, when you see, and you hear, and you feel the result of the gesture, it can transform what was otherwise just a scrolling behavior into something that feels like a very tactile interface. Now, there's one final note I want you thinking about when you're crafting the character of your app.\nAnd, that's that it feels cohesive, that you're staying in character. Now, what does this mean? So, even within your app, or across the whole system, it's important that you treat behaviors as a family of behaviors.\nSo, in scrolling for example, when I scroll down a page, using a scrolling behavior, and then I tap the status bar to scroll to the stop of the page, using an elastic behavior.\nIn both cases, the page itself feels like it's moving in the same way, that it has the same behavior, even though two different types of behaviors are driving its motion, are influencing its motion.\nNow, this extends beyond a single interaction like scrolling.\nIt applies to your whole app. If you have a playful app, then you should embrace that character, and make your whole app feel the same way. So, that people-- once they learn one behavior of your app, they can pick up another behavior really easily, because we learn through repetition. And, what we learn bleeds over into other behaviors.\nSo, next up, I'd like to talk a little bit about aligning motion, or dynamic motion, with intent.\nSo, for a discrete interaction like a button, it's pretty clear what the intent of the gesture is. Right? You've got three distinct visual representations on screen here.\nAnd, when I tap one of them, the outcome is clear.\nBut, with a gesture like a swipe, the intent is less immediately clear. You could say that the intent is almost encoded in the motion of the gesture, and so it's our job, our role, to interpret what the motion means to decide what we should do with it. Let's look at an example.\nSo, let's say I made a FaceTime call, a one-on-one FaceTime call, and in FaceTime, we have a small video representation of yourself in the corner of the screen. And, this is so I can see what the person on the other end sees.\nWe call this floating video the PIP, short for picture in picture.\nNow, we give the PIP a floating appearance to make it clear that it can be moved.\nAnd, it can be moved to any corner of the screen, with just a really lightweight flick.\nSo, if we compare that to the Play, Pause, and Skip buttons, like, what's the difference here? So, in this case, there's actually four invisible regions that we're dealing with. No longer do we have these three distinct visual representations on screen that are being tapped. We kind of have to look at the motion that's happening through the gesture, and intuit what was meant. Which corner did we intend to go to? Now, we call these regions of the screen endpoints of the gesture.\nAnd, when the PIP is thrown, our goal is to find the correct endpoint, the one that was intended. And, we call this aligning the endpoint with the intent of the gesture. So, one approach for this is to keep track of the closest endpoint as I'm dragging the PIP. Now, this kind of works. I can move the PIP to the other corner of the screen, but it starts to break down as soon as I move the PIP a little bit further. Now, I actually need to drag the PIP quite far, like past halfway over the screen. Pretty close to the other corner. So, it's not really magnifying my input. It's not really working for me. And, if I try and flick the PIP, it kind of goes back to the nearest corner, which isn't necessarily what I expected. So, the issue here is that we're only looking at position. We're completely ignoring the momentum of the PIP, and its velocity when it's thrown.\nSo, how can we incorporate momentum into deciding which endpoint we go to? So, to think about this, I think we can set aside endpoints for a moment, and take a step back.\nAnd, just really simplify the problem. Ultimately, what I'm trying to do here is move content around on the screen.\nAnd, I actually already have a lot of muscle memory for doing exactly that with scrolling. So, why don't we use that here? We use scrolling behaviors all the time, so we have this natural intuition for how far content goes when I scroll.\nSo, here you can see that when I scroll the PIP instead, it coasts along, and it slows down, using this familiar deceleration that we're familiar with from scrolling.\nAnd, basically by taking advantage of that here, we're reinforcing things that people have learned elsewhere. That the behavior is just doing what was expected of the system. Now this new, hypothetical, imaginary PIP position is not real. We're not going to show the PIP go here in the interface. This is what we call a projection.\nSo, we've taken the velocity of the PIP, when it was thrown. We've, kind of, mixed in the deceleration rate, and we end up with this projected position where it could go if we scrolled it there. And so, now instead of finding the nearest endpoint to the PIP when we throw, we can calculate its projected position and move there instead. So now, when I swipe from one corner of the screen to another with just a lightweight flick, it goes to the endpoint that I expected.\nSo, this idea of projecting momentum is incredibly useful. And, we think its super important. I'd like to share some code for doing this with you, so that you can do this in your own apps. So, this function will take a velocity like the PIP's position velocity, and deceleration rate, and it'll give you the value that you could use as an endpoint for dynamic behavior.\nIt's pretty simple. If we look at my FaceTime example of the pan gesture ending code, you can see that I'm just using the UIScrollView.DecelerationRate. So, we're leaning on that familiarity people have with scrolling and how far content will go when scrolled.\nAnd, I'm using that with my projection. So, I take the velocity of the PIP and the deceleration rate, and I create that imaginary PIP position. And, it's this imaginary, projected position that I then use as the nearest corner position.\nAnd, I send my PIP there, by retargeting it.\nSo, this idea of using projection to find out the endpoint of a position, is incredibly useful for things being dragged or swiped, where you really need to respect the momentum of the gesture.\nBut, this projection function isn't just useful for positions, you can also use it for scales, or even for rotations.\nOr, even combinations of the two.\nIt's a really versatile tool that you should really be using to make sure that you're respecting the momentum of a gesture, and making it feel like the dynamic motion in your app is an extension of yourself.\nSo, that's designing with motion. Dynamic motion.\nBehaviors should continuously and seamlessly work in concert with interactions.\nWe should be leaning on that shared intuition that we have for the physical world around us. The things that we learn as children about how objects behave and move in the physical world, apply just as readily to our dynamic interfaces.\nYou should remember that bounciness needs to be purposeful. Think about why you're using it, and whether it's appropriate. And, make sure that as you add character, and texture, that you're balancing it with utility.\nAnd finally, remember to project momentum. Don't just use position, use all of the information that's at your disposal to ensure that motion is aligned with the intent of where people actually want to go. And then, take them there. So, to talk a little bit more about how to fluidly respond to gestures and interactions, I'd like to introduce my colleague, Marcos, to the stage. Thanks for having me, everyone. That was great.\nThanks, Nathan.\nHi everyone.\nMy name is Marcos.\nSo far, we've seen how important fluidity is when designing interfaces.\nAnd, a lot of that comes from your interaction with a device.\nSo, in this section, we're going to show you how touches on the screen become gestures in your apps. And, how to design these gestures to capture all the expression and intent into your interfaces.\nSo, we're going to start by looking at the design of some core gestures like taps and swipes.\nThen, we'll look at some interaction principles, that you should follow when designing gestures for your interface.\nAnd then, we'll see how to deal with multiple gestures, and how to combine them into your apps.\nWe're going to start by looking at a gesture that is apparently very simple, a tap.\nYou would think that something-- you would think that a tap is something that doesn't have to be designed, but you'll see how its behavior has more nuances than it seems.\nIn our example, we're going to look at tapping on a button, in this case, on the Calculator app.\nThe first thing to remember is that the button should highlight immediately when I touch down on it.\nThis shows me the button is working, and that the system is reacting to my gesture.\nBut, we shouldn't confirm the tap until my touch goes up.\nThe next thing to remember is to create an extra margin around the tap area. This extra margin will make our taps more comfortable, and avoid accidental cancellations if a touch moves during interaction. And, like my colleague Chan was saying, I should be able to change my mind after I've touched down on the button. So, if I drag my finger outside the tap area, and lift it, I can cancel the tap. The same way, if I swipe it back on the button, the button should highlight again, and let me confirm the tap.\nThe next gesture we're going to talk about is swipe.\nSwipes are one of the core gestures of iOS, and they're used for multiple actions like scrolling, dragging, and paging.\nBut, no matter what you use it for, or how you call it, the core principles of a gesture are always the same. In this example, we're going to use a swipe to drag this image to the right.\nSo, the interaction starts the moment I touch down on the image with intention to drag it.\nBut, before we can be sure it's a swipe, the touch has to move a certain distance. We learn to differentiate swipes from other gestures. This distance is called hysteresis, and is usually 10 points in iOS.\nSo, once the touch reaches this distance, the swipe begins.\nThis is also a good moment to decide the direction of the swipe. If it's horizontal, or vertical for instance. We don't really need it for example, but it's very useful in some situations. So, now that the swipe has been detected, this is the initial position of a gesture.\nAfter this moment, the touch and the image should stay together and move as one thing. We should respect the relative position, and never use the center of the image as the dragging point.\nDuring the drag, we should also keep track of the position and speed up the touch, so when the drag is over, we don't use the last position. We use the history of the touch, to ensure that all the motion is transferred fluidly into the image.\nSo, as we've seen, touch and content should move together. One-to-one tracking is extremely important. When swiping or dragging, the contents should stay attached to the gesture.\nThis is one of the principles of iOS. You enable scrolling, and makes the device feel natural and intuitive. It's so recognizable and expected that the moment the touch and content stop tracking one-to-one, we immediately notice it. And, in the case of scrolling, it shows us that we've reached the end of the content. But, one-to-one tracking is not limited to touch screens. For instance, manipulating UI on the Apple TV was designed around this concept.\nSo, even if the touch is not manipulating the content directly, having a direct connection between the gesture and the interface puts you in control of the action, and makes the interaction intuitive.\nAnother core principle when designing gestures, is to provide continuous feedback during the interaction. And, this is not just limited to swipes or drags. It applies to all interactions. So, if you look again at the Flashlight button on the iPhone 10, the size of button changes based on the pressure of my touch. And, this gives me a confirmation of my action. It shows me the system is responding to my gesture, but it also teaches me that pressing harder will eventually turn on the flashlight. Another good example of continuous feedback, is the focus engine on the Apple TV.\nSo, the movements on the Siri remote are continuously represented on the screen. And, they show me the item that is currently selected, the moment the selection is going to change, and the direction the selection is going to go. So, having our UI respond during the gesture is critical to create a fluid experience. For that reason, when implementing your gestures, you should avoid methods that are only detected at the end of the gesture, like UISwipeGestureRecognizer. And, use ones like the actual touches, or other gestureRecognizers that provide all possible information about the gesture.\nSo, not just the position, but also the velocity, the pressure, the size of the touch. In most situations though, your interfaces must respond to more than one gesture.\nAs you keep adding features to your apps, the complexity and number of gestures increases, too. For instance, almost all UIs that use a scroll view will have other gestures like taps and swipes competing with each other. Like in this example, I can scroll the list of Contacts, or freely touch on one of them to preview it.\nSo, if we had to wait for the final gesture, before we show any feedback, we would have to introduce a delay. And, during that wait, the interface wouldn't feel responsive. For that reason, we should detect all possible gestures from the beginning of the action. And, once we are confident of the intention, cancel all the other gestures.\nSo, if we go back to our example, I start pressing that contact, but I decide to scroll instead. And, it's at that moment that we cancel the 3D touch action, and transition into the right gesture.\nSometimes, though, it's inevitable to introduce delay.\nFor instance, every time we use the double-tap in our UIs, all normal taps will be delayed.\nThe system has to wait after the tap, to see if it's a tap or a double-tap. In this example, since I can double-tap to zoom in and out of a photo, tapping to show the app menu is delayed by about half a second.\nSo, when designing gestures for your applications, you should be aware of these situations, and try to avoid delays whenever possible.\nSo, to summarize, we've seen how to design some core gestures, like taps and swipes. We've seen that content and touch should move one-to-one, and that is one of the core concepts of iOS.\nYou should also provide continuous feedback during all interactions, and when having multiple gestures, detect them in parallel from the beginning. And now, I'd like to hand it back to Chan, who will talk about working with fluid interfaces. Thanks, everyone. Nice job.\nAlright, I'm back.\nSo, we just learned about how to approach building interfaces that feel as fluid, as responsive, and as lively as we are.\nSo, lets talk about some considerations now that we're feeling a little bit more comfortable with this, for working within the medium of fluid interfaces. And that begins with teaching.\nSo, one downside to a gestural interface is that it's not immediately obvious what the gestures are. So, we have to be friendly and clever about how we bring users along with us in a way that's friendly and inviting.\nAnd so, one way we can do that is with visual cues. So, the world is filled with these things, right? You can learn them once, and you can use them everywhere. They're portable. And so, when you see this, you know how to use it.\nSo, we've tried to establish similar conventions in iOS. Here's a couple examples.\nSo, if you have a scrolling list of content, you can clip the content off the bottom there, to indicate that there's more to see, that invites me to try and reveal what's under there. And, if we're dealing with pages of content, you can use a paging indicator to indicate that there's multiple pages of content.\nAnd, for sliding panes of content, you can use an affordance, or a grabber handle like this, to indicate that it's grabbable and slidable.\nAnother technique you can use is to elevate interactive elements to a separate plane. So, if you have an interactive element, lifting it up to a separate plane can help distinguish it from the content.\nSo, a good example of this is our on/off switch. We want to indicate that the knob of the switch is grabbable, so we elevate it to another plane. This helps visually separate it, and indicate its draggable nature.\nSo, floating elements, interactive elements like this, above the interface can help indicate that they're grabbable. Next, we can use behavior, you know, to show rather than tell to use-- how to use an interface. So, we can reinforce a dynamic behavior with a static animation.\nSo, an example of this is Safari. In Safari, we have this x icon at the top left to close the tab, and when you hit that button, we slide the tab left to indicate it's deleted.\nThis hints to me that I can slide it myself to the left. And, accomplish the same action of deleting the tab through a gesture.\nSo, by keeping the discrete animation and the gesture aligned, we can use one to teach the other.\nAnd, there's another technique we can use, which is explanations. This is when you explicitly tell users how to use a gesture.\nSo, this is best when used sparingly, but it's best when you have one gesture that's used repeatedly in a bunch of places, and you explain it once up front, and then you just keep using it, and keep reinforcing it.\nDon't use it for a gesture that's used only intermittently. People won't remember that.\nNow, I want to talk a little bit about fun and playfulness. Because this is one of the most important aspects of a fluid interface. And, it only happens when you nail everything.\nIt's a natural consequence of a fluid interface. It's when the interface is responding instantly and satisfyingly. When it's redirectable and forgiving. When the motion and gestures are smooth. And, everything we just talked about. The interface starts to feel in sync with you.\nAnd, something magical happens where you don't feel like you need to learn the interface, you feel like you're discovering the interface.\nAnd so, we think it's great when we allow people to discover the interface through play. And, it doesn't even feel like they're learning it, it feels fun.\nSo, people love playing with stuff. So, we think it's great to play into our natural fiddle factor.\nYou know, play is our mind's internalizing the feel of an interface. So, it's great when we're building this stuff, when we're prototyping it, just to build it. You know, play with it yourself. See how you fiddle with it. Hand it to others see how they play with it. And, think about how you can reinforce that with something like an animation, or behavior, an explanation.\nAnd, it's surprising how far play can go, and having interface teach itself to people. Let's talk a little bit about fluidity as a medium. How we actually go about building this stuff. You know, we think interfaces like this are a unique medium, and it's important that we approach it right.\nSo, the first thing is to design the interactions to be inseparable from the visuals, not an afterthought. The interaction design should be done in concert with the visuals. You shouldn't be able to even tell when one ends and another begins.\nAnd, it's really important that we build demos of this stuff. The interactive demo we think is really worth a million static designs. Not just to show other people, but to also understand the true nature of the interface yourself.\nAnd, when you prototype this stuff, it's so valuable for you because you get to almost discover the interface as you're building it.\nYou know, this technique is actually how we built the iPhone 10 interface.\nAnd, it's really important because it also sets a goal for the implementation. We're so lucky here at Apple that we have this amazing engineering team to build this stuff, because it's really hard to build. And, it's so important also to have that kind of magical example that reminds yourself and the engineering teams, and yourselves that what it can feel like, you know? And, it's really important to, kind of, remember, remind yourself of that.\nAnd, it makes-- when you actually build it, it makes something that's hard to copy, and it gives your app a unique character.\nSo, you know, multitouch is such an amazing medium we all get to play in.\nWe get to use technology to interface with people at an ancient, tactile level. It's actually really cool.\nYou know, all those principles we talked about today, they're at the core of the design of the iPhone 10 gestural interface, you know, responsive, redirectable, interruptible gestures, dynamic motion, elegant gesture handling.\nIn a lot of ways, it's kind of the embodiment of what we think a fluid interface could be.\nWhen we align the interface to the way we think and move, something kind of magical happens.\nIt really stops feeling like a computer, and starts feeling like a seamless extension of us.\nYou know, as we design the future of interfaces, we think it's really important to try and capture our humanity in the technology like this.\nSo, that one of the most important tools of humankind is not a burden, but a pleasure and a delight to use.\nThank you very much. [ Applause ]",
        "sentiment": 0.18091274441086821
    },
    "article_18": {
        "title": "Genius Lyrics Are Now Available in Apple Music",
        "body": "Music encyclopedia and lyrics database Genius has announced a partnership with Apple Music, pairing the website\u2019s lyrics with songs. Starting today, Genius lyrics for \u201cthousands of songs\u201d are available on Apple Music, allowing users to read lyrics while they listen to a song.\nAdditionally, Apple Music is now the official music player for Genius. Apple Music subscribers who visit the Genius website and are logged into their account will be able to play a song directly on the song\u2019s Genius song page. So, if you\u2019re looking up the lyrics for Ariana Grande\u2019s \u201cGod is a Woman,\u201d there will be an Apple Music player present on the Genius page so you can listen along.\nApple Music users can read Genius lyrics and annotations while they listen to a song\nSnippets of Genius lyrics can also be found on hit songs throughout Spotify, a partnership that started back in 2016 with a series of playlists called Behind the Lyrics, which gave additional info for songs a la Pop-Up Video.",
        "sentiment": 0.5291763748973608
    },
    "article_19": {
        "title": "Fortnite on iOS hits $300 million revenue in 200 days",
        "body": "The iOS version of Fortnite has been a big hit, with the mobile battle royale game earning $300 million in 200 days, according to market analyst Sensor Tower since its launch on Apple smartphones and tablets March 15.\nFortnite is also available on the three major home gaming consoles (the PlayStation 4, the Xbox One, and the Nintendo Switch) and PC, and the free-to-play battle royale mode has turned the game into a global sensation. Its season pass model, which has players spend real money on a progression system that unlocks more cosmetic items as they play, has helped Fortnite become an earnings machine. Season 6 launched on September 27, and the iOS version grossed $20 million alone in the week after its launch.\nThe roadmap here is clear: Make a game that grabs players and put it on every device that can run it. But while that may sound simple, Fortnite\u2019s success stands out because it\u2019s really the first video game to have accomplished this, with a nod maybe to Tetris.\nAs a comparison, famous mobile hit Clash Royale earned $228 million in its first 200 days on iOS. And that figure excludes revenues from China (Fortnite iOS is not available in China).\nImage Credit: Sensor Tower\nAbout 65 percent of that $300 million comes from U.S. Meanwhile, the Android version of Fortnite, which has only been out since August 9, has earned over $60 million.\nFortnite iOS is also tracking ahead of PUBG Mobile. PUBG is the next most-popular battle royale game. Its iOS version has earned $47 million since launching on March 19.",
        "sentiment": 0.3704495821924259
    },
    "article_20": {
        "title": "Apple Releasing iOS 12.0.1 With Fixes for Wi-Fi 2.4GHz Bug, Lightning Charging Issue",
        "body": "Apple is today releasing iOS 12.0.1, the first official update to the iOS 12 operating system. iOS 12.0.1 comes three weeks after Apple released iOS 12.\nThe update will be available on all eligible devices over-the-air in the Settings app. To access the update, go to Settings --> General --> Software Update. iOS 12 users should be able to download iOS 12.0.1 when it is released at 10:00 a.m. Pacific Time.\nToday's update fixes several high profile bugs that have been plaguing iOS 12 users. It resolves an issue that could cause some iPhone XS devices not to charge when connected to a Lightning cable, an issue that was discovered shortly after iOS 12 was released.\nReports suggested multiple iOS 12 devices were affected rather than just the iPhone XS, and it's likely that if other devices are impacted, the new update solves the problem.\niOS 12.0.1 also fixes a major Wi-Fi bug that could cause some iPhone XS devices to prefer to join a 2.4GHz Wi-Fi network rather than a 5GHz Wi-Fi network, resulting in perceived slower Wi-Fi connection speeds. After this update, many users who were stuck with their phones connecting to a 2.4GHz network should see much faster Wi-Fi connection speeds as the devices once again prefer a 5GHz network.\nOther bug fixes in this update include a reorientation of the \"123\" number key on the iPad, which was moved in the iOS 12 update and swapped with the emoji/language key, a fix for a problem that could cause subtitles not to appear in some video apps, and an issue where Bluetooth could become unavailable.\nMany of these bugs have also been addressed in the upcoming iOS 12.1 update that is currently being beta tested by developers and public beta testers.\nApple's full release notes for the update are below:\niOS 12.0.1 includes bug fixes and improvements for your iPhone or iPad. This update:\n- Fixes an issue where some iPhone XS devices did not immediately charge when connected to a Lightning cable\n- Resolves an issue that could cause iPhone XS devices to rejoin a Wi-Fi network at 2.4GHz instead of 5GHz\n- Restores the original position of the \".?123\" key on the iPad keyboard\n- Fixes an issue where subtitles may not appear in some video apps\n- Addresses an issue where Bluetooth could become unavailable\n- For information on the security content of Apple software updates, please visit this website: https://support.apple.com/kb/HT201222\nThe new iOS 12 update introduces a slew of new features like Siri Shortcuts for creating voice-controlled automations, Screen Time for monitoring your iOS device usage, Memoji on devices with a TrueDepth camera, improvements to Do Not Disturb and notifications, ARKit 2.0, privacy enhancements, and more.\nFor additional details on everything new in iOS 12, make sure to check out our roundup",
        "sentiment": -0.2345223357392983
    },
    "article_21": {
        "title": "Apple has won a Patent for the MacBook Pro's Touch ID Feature",
        "body": "Apple's fourth generation MacBook Pro was announced on October 27, 2016. It replaced the function keys with an interactive, multi-touch \"Touch Bar\" and a Touch ID sensor integrated into the Power button as noted in our cover graphic. Apple was granted their first patent for the Touch Bar in November 2017 and a second granted patent in March 2018. Today Apple was granted their third patent for this invention with major patent claims added.\nApple's patent FIG. 1A noted above depicts a MacBook Pro incorporating a restricted-access button; FIG. 1B depicts a detail view of the region A-A depicted in FIG. 1A, showing an electronic device embodiment in which the restricted-access button extends proud of a surface of a housing of the electronic device; FIG. 1C depicts a detail view of the region A-A depicted in FIG. 1A, showing an electronic device embodiment in which the restricted-access button is flush with a surface of a housing of the electronic device; FIG. 2A depicts an exploded cross-section assembly view of the button assembly; and FIG. 6 depicts example operations of a method of operating a button that incorporates a biometric sensor.\nThe differences between one granted patent and another always resides in differing patent claims.\nWhile there are many fine patent claim points in this granted patent that differ from previous granted patents, below are the three major patent claims that Apple has added to this invention in order to better protect this invention.\nPatent Claim #1: \"A portable electronic device comprising: a housing; a set of keys extending at least partially through at least one aperture defined by the housing; a touch-sensitive display disposed at least partially within an opening defined by the housing and along a side of the set of keys; and a power button disposed at least partially within the opening and abutting the touch-sensitive display, wherein: the power button is configured to capture a biometric input when a user of the portable electronic device presses the power button; and the power button is configured to depress in a cantilevered manner such that the power button does not transfer a load to the touch-sensitive display.\"\nPatent Claim #12: \"A button assembly for a laptop computing device, the button assembly comprising: a button cap comprising: a frame; a cover attached to the frame and forming an external surface of the button cap; a bumper extending from an edge of the frame and configured to pivot against a housing of the laptop computing device in response to a button press; a spring plate coupled to an interior surface of the housing below a through-hole in the housing, the spring plate coupled to a standoff extending from the button cap through the through-hole; and a biometric sensor coupled to a bottom surface of the cover.\"\nPatent Claim #18: \"A method of operating an electronic device, the method comprising: detecting one or more inputs to a button operable in: an unrestricted mode in which a first function of the electronic device is executed in response to each detected input to the button; and a restricted mode in which a second function of the electronic device, different from the first function, is executed in response to the each detected input to the button only if the each detected input is provided by one of a limited set of users; in response to the each detected input when the button is in the unrestricted mode, executing the first function after the each detected input; and in response to the each detected input when the button is in the restricted mode: obtaining biometric data from a biometric sensor positioned below the button; upon determining that the obtained biometric data matches template data associated with at least one user of the limited set of users, executing the second function; and upon determining that the obtained biometric data does not match any template data associated with the limited set of users, rejecting the input.\"\nApple's granted patent 10,089,512 was originally filed in Q3 2016 and published today by the US Patent and Trademark Office.\nPatently Apple presents only a brief summary of granted patents with associated graphics for journalistic news purposes as each Granted Patent is revealed by the U.S. Patent & Trademark Office. Readers are cautioned that the full text of any Granted Patent should be read in its entirety for full details. About Making Comments on our Site: Patently Apple reserves the right to post, dismiss or edit any comments. Those using abusive language or negative behavior will result in being blacklisted on Disqus.",
        "sentiment": 0.015008856415409933
    },
    "article_22": {
        "title": "Apple\u2019s new iPad Pro has Face ID, USB-C, and slimmer bezels than ever before",
        "body": "After months of rumors, Apple has today announced a completely redesigned iPad Pro with slimmed-down bezels, Face ID, a USB-C port, and far more powerful specs than its predecessor. Just like prior years, the new iPad Pro comes in two screen sizes: 11 inches and 12.9 inches. The 11-inch model has essentially the same proportions as the prior 10.5-inch model. And the 12.9-inch model puts the same-sized display into a much smaller form factor. The new iPad Pro starts at $799 for the 11-inch and $999 for the 12.9-inch. Preorders begin today, and it ships on November 7th.\nThe new Pro is the company\u2019s first iPad not to include a home button, which allowed Apple to extend the screen vertically for a much more immersive experience. The bezels have been downsized on all four sides. Like the iPhone XR, Apple is referring to this screen as a Liquid Retina Display. Both sizes offer Apple\u2019s ProMotion technology for smooth, fluid display performance. With the removal of the home button, the new iPads will fully adopt the gesture navigation that debuted on the iPhone X.\nBut something else has been removed, too: the headphone jack. There\u2019s no 3.5mm port visible on any of the device\u2019s sides, meaning that buyers will need a USB-C to headphone dongle to listen to music through wired headphones. Apple will be releasing its own $9 headphone adapter \u2014 no, there\u2019s not one in the box \u2014 along with other USB-C accessories.\nBoth iPad Pros feature Face ID and the TrueDepth camera setup that also began with the iPhone X. But unlike the iPhone X, XS, and XR, the new iPad Pro lacks a notch; its Face ID components are housed in the top, flat bezel. On the iPad, Face ID can work when the device is oriented horizontally, like when it\u2019s connected to a keyboard, for example. Adding Face ID means that Animoji and Memoji will be coming to the iPad for the first time.\nThe 2018 iPad Pros are powered by Apple\u2019s new A12X Bionic chip with an 8-core CPU and 7-core GPU. This leads to 35 percent faster single-core performance and a leap to 90 percent faster for multicore tasks. As for the GPU, Apple\u2019s Jon Ternus said, \u201cThe iPad Pro delivers Xbox One S class graphics performance in a package that is much smaller.\u201d It will offer up to 1TB of built-in flash storage.\nThe iPad\u2019s transition away from Apple\u2019s proprietary Lightning connector to USB-C is a move that the company hasn\u2019t yet deemed to be right for its iPhones. The recently released iPhones XS and XR continue to use Lightning. But putting USB-C in an iPad further cements the device\u2019s place as a computer alternative \u2014 if not yet a full replacement. The new port opens new capabilities for the iPad Pro such as charge out; it can provide power to a connected iPhone, for example. The iPad Pro can be connected to displays ranging up to 5K resolution.\nThe new iPad Pros have four speakers that deliver wider stereo sound, a 12-megapixel camera (capable of 4K/60 video recording), and offer gigabit LTE on cellular models. They\u2019ll ship with an 18-watt charger in the box, and Apple says battery life is good for up to 10 hours of continuous use.\nApple is also introducing a redesigned $129 Apple Pencil alongside its latest iPads. The annoying charging method that required plugging the Pencil directly into the iPad is gone. Now, it attaches to the tablet magnetically and charges wirelessly. Tapping the iPad Pro with the Pencil will immediately wake the device and launch Notes. And other touch gestures \u2014 open to third-party developers \u2014 can switch between tools in productivity apps and execute other actions.\nAdditionally, Apple is updating the old Smart Keyboard, now called the Smart Keyboard Folio, to allow the iPad to sit at two different typing angles. Unfortunately, it\u2019s gotten more expensive at $179.",
        "sentiment": 0.10088090330143185
    },
    "article_23": {
        "title": "Exclusive: Icon found in iOS shows new iPad Pro with no home button, rounded corners, more",
        "body": "We\u2019ve seen a similar icon before, back on the fifth beta of iOS 12. That icon showed an iPad with no Home button, rounded corners and no notch, but it was a small icon designed to be used in the battery widget. Therefore, it didn\u2019t give us a lot of detail on the form factor of the device. Now, we\u2019ve found a more detailed look at the 2018 iPad Pro redesign within iOS.\nToday, a new icon found within iOS offers a more detailed view of the 2018 iPad Pro, which we expect to be announced during the Tuesday event in Brooklyn. This icon shows an iPad with rounded corners, no Home button and no notch. An interesting aspect of this icon is how large the bezels look when compared to the recent iPhones.\nThis could be due to how the icon is constructed, sometimes thicker lines are required for it to be \u201creadable\u201d at smaller sizes. A similar effect occurred with the iPhone X glyph found last year.\nAnother interesting detail is that the sleep/wake button is still at the top, while new iPhone models feature a side button that\u2019s used to invoke Siri, take screenshots and shut down the device with button combinations. The presence of a sleep/wake button instead of a side button could be just a design decision for the new iPad. On the other hand, it is possible the 2018 iPad Pro ends up featuring a side button as this can be an unfinished asset.\nDetails aside, it\u2019s clear that we\u2019re going to see a big redesign of the iPad Pro this Tuesday. Stay tuned to 9to5Mac as we\u2019ll have full coverage of everything Apple announces.\nRelated stories:\nSubscribe to 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.06916712899692357
    },
    "article_24": {
        "title": "Apple Watch and fall detection credited with saving man in Sweden with acute back pain",
        "body": "Apple Watch Series 4 isn\u2019t just a nice looking watch. It also has invisible features that can prove life-saving. Series 4 only hit the market last month and the new Apple Watch fall detection feature is already being credited with coming to the rescue for at least one customer in a dire situation.\nAccording to Swedish publication Aftonbladet, Apple Watch Series 4 was key in notifying emergency services for Gustavo Rodriguez when crippling back pain struck while he was cooking over a stove. The unexpected discomfort caused him to double over and collapse to the floor while the stove was fired up.\nApple Watch fall detection noticed the sudden collapse and offered to call local emergency services. Translated to English:\nGustavo, 34, suffered a sudden back injury and fell paralyzed to the kitchen floor. \u201cIt felt like someone pushed a knife into my back,\u201d he says. Luckily, his watch responded. On Friday, Gustavo Rodriguez stood by the stove as usual and cooked food. Suddenly he felt strange tension in his back and it became increasingly difficult to move the body. Gustavo tried not to think about it. \u201cBut then I moved the frying pan and it just hit me. It felt like someone pushed a knife into my back, says Gustavo. He fell to the floor. The pain was so strong that everything went black. He could not move. Then the watch chimed and asked \u201cDo you want to call 112?\u201d. \u201cMy Apple Watch had sensed the fall and wondered if it should make an emergency call,\u201d said Gustavo.\nRodriguez says he managed to crawl across the floor to the living room and climb to the couch, but his phone was in another room and he hadn\u2019t recovered from the pain yet.\nHe managed to call his mother-in-law from the Apple Watch who lived nearby and could turn off the stove, and an ambulance brought him to a hospital where he had to have a morphine shot to reduce the acute back pain.\nWhile it doesn\u2019t sound like the watch ultimately needed to call emergency services for Rodriguez, Apple Watch Series 4 does have a feature that will automatically call emergency services if it detects a fall and you don\u2019t respond to the alert within a minute.\nApple Watch fall detection is automatically enabled on Series 4 for customers who are 65 or older, and you can turn it on manually in the Watch app on iPhone if you\u2019re under 65 (although Apple warns that some activity could be mistaken for falls with more active customers). The feature also notifies your emergency contact and shares your location when a fall is detected and the watch user is unresponsive.\nApple Watch can also easily call emergency services by holding down the side button below the Digital Crown for several seconds. The SOS feature requires a nearby iPhone on Apple Watches without LTE, and models with active cellular service can make and receive phone calls away from iPhone.\nRelated Stories:\nSubscribe to 9to5Mac on YouTube for more Apple news:\nFTC: We use income earning auto affiliate links. More.\nCheck out 9to5Mac on YouTube for more Apple news:",
        "sentiment": -0.284974649291614
    }
}